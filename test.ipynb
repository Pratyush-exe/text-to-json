{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is OS\n",
      " Volume Serial Number is 4A0B-27B2\n",
      "\n",
      " Directory of c:\\Users\\Pratyush\\Desktop\\text-to-json\\examples\\text2SQL-solutions\n",
      "\n",
      "05-05-2025  20:26    <DIR>          .\n",
      "05-05-2025  20:26    <DIR>          ..\n",
      "05-05-2025  18:49         2,161,792 chase-sql.pdf\n",
      "05-05-2025  20:21         1,493,944 DBCopilot.pdf\n",
      "05-05-2025  20:26                 0 input.txt\n",
      "05-05-2025  20:23           918,547 RatSQL.pdf\n",
      "05-05-2025  20:21           341,083 RyanSQL.pdf\n",
      "05-05-2025  20:26                 0 schema.json\n",
      "05-05-2025  20:18           454,000 seq2SQL.pdf\n",
      "05-05-2025  20:19           443,530 sqlNet.pdf\n",
      "05-05-2025  20:20           728,921 SyntaxSQLNet.pdf\n",
      "05-05-2025  20:23         2,970,020 t2s-intermediate-rep.pdf\n",
      "05-05-2025  20:25         1,095,621 t2s-LLMs.pdf\n",
      "05-05-2025  20:20           614,703 UniSAr.pdf\n",
      "05-05-2025  18:48         4,588,484 xiyang-sql.pdf\n",
      "              13 File(s)     15,810,645 bytes\n",
      "               2 Dir(s)  380,987,977,728 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir \"examples/text2SQL-solutions/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['examples/text2SQL-solutions/chase-sql.pdf',\n",
       " 'examples/text2SQL-solutions/DBCopilot.pdf',\n",
       " 'examples/text2SQL-solutions/RatSQL.pdf',\n",
       " 'examples/text2SQL-solutions/RyanSQL.pdf',\n",
       " 'examples/text2SQL-solutions/seq2SQL.pdf',\n",
       " 'examples/text2SQL-solutions/sqlNet.pdf',\n",
       " 'examples/text2SQL-solutions/SyntaxSQLNet.pdf',\n",
       " 'examples/text2SQL-solutions/t2s-intermediate-rep.pdf',\n",
       " 'examples/text2SQL-solutions/t2s-LLMs.pdf',\n",
       " 'examples/text2SQL-solutions/UniSAr.pdf',\n",
       " 'examples/text2SQL-solutions/xiyang-sql.pdf']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymupdf\n",
    "import os\n",
    "\n",
    "\n",
    "BASE_PATH = \"examples/text2SQL-solutions/\"\n",
    "pdf_paths = [os.path.join(BASE_PATH, x) for x in os.listdir(BASE_PATH) if x.endswith('pdf')]\n",
    "pdf_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "APIKEY = \"Bearer premKey_zB4JsYhqRtxz2qiPSukLt0IEBsZHJTkQ9AUh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://sid.premai.io/api/chat/completions\"\n",
    "headers = {\n",
    "    \"Authorization\": APIKEY,\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "payload = {\n",
    "    \"model\": \"gpt-4o\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"hi\"}],\n",
    "    \"stream\": False,\n",
    "    \"temperature\": 0.5\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=payload, stream=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-BTs2OWZcc37VqkCCipiHFtXTFjA3z',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1746458932,\n",
       " 'model': 'gpt-4o-2024-08-06',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': 'Hello! How can I assist you today?',\n",
       "    'refusal': None,\n",
       "    'annotations': []},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 8,\n",
       "  'completion_tokens': 10,\n",
       "  'total_tokens': 18,\n",
       "  'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0},\n",
       "  'completion_tokens_details': {'reasoning_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'accepted_prediction_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0}},\n",
       " 'service_tier': 'default',\n",
       " 'system_fingerprint': 'fp_f5bdcc3276'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 15/15 [00:05<00:00,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Headings & Subheadings:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pymupdf\n",
    "import concurrent.futures\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from threading import Lock\n",
    "\n",
    "N = 100  # Number of lines per chunk\n",
    "\n",
    "def read_and_chunk(file_path, lines_per_chunk=N):\n",
    "    doc = pymupdf.open(file_path)\n",
    "    full_text = \"\"\n",
    "    for page in doc:\n",
    "        full_text += page.get_text()\n",
    "    doc.close()\n",
    "\n",
    "    # Split text into lines and chunk them\n",
    "    lines = full_text.splitlines()\n",
    "    chunks = [\n",
    "        \"\".join(lines[i:i + lines_per_chunk])\n",
    "        for i in range(0, len(lines), lines_per_chunk)\n",
    "    ]\n",
    "    return chunks\n",
    "\n",
    "def make_prompt(text_chunk):\n",
    "    return f\"\"\"\n",
    "You are an assistant that extracts headings and subheadings from academic or technical papers.\n",
    "\n",
    "The following text is a chunk from a paper. Your task is:\n",
    "1. Identify and extract **section headings, subheadings, and other headings** from the text.\n",
    "2. Prefer numbered headings if present (e.g., \"1. Introduction\", \"2.1 Previous Work\").\n",
    "3. Only return **phrases that clearly represent headings/subheadings**.\n",
    "\n",
    "ONLY Return the output as a JSON list of phrases in the order they appear.\n",
    "\n",
    "Text:\n",
    "\\\"\\\"\\\"\n",
    "{text_chunk}\n",
    "\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "def call_openapi_gpt(text_chunk):\n",
    "    url = \"https://sid.premai.io/api/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": APIKEY,\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    prompt = make_prompt(text_chunk)\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"stream\": False,\n",
    "        \"temperature\": 0.3\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    try:\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "def extract_headings_from_document(file_path):\n",
    "    chunks = read_and_chunk(file_path)\n",
    "    all_headings = [None] * len(chunks)\n",
    "    lock = Lock()\n",
    "\n",
    "    with tqdm(total=len(chunks), desc=\"Processing chunks\") as pbar:\n",
    "        def process_chunk(i, chunk):\n",
    "            result = call_openapi_gpt(chunk)\n",
    "            headings = []\n",
    "            if result:\n",
    "                try:\n",
    "                    cleaned = result.replace('```json', \"\").replace(\"```\", \"\").strip()\n",
    "                    headings = json.loads(cleaned)\n",
    "                except Exception as parse_error:\n",
    "                    print(f\"Parse error in chunk {i+1}: {parse_error}\")\n",
    "            with lock:\n",
    "                all_headings[i] = {\n",
    "                    \"chunk_id\": i + 1,\n",
    "                    \"headings\": headings\n",
    "                }\n",
    "                pbar.update(1)\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = [\n",
    "                executor.submit(process_chunk, i, chunk)\n",
    "                for i, chunk in enumerate(chunks)\n",
    "            ]\n",
    "            concurrent.futures.wait(futures)\n",
    "\n",
    "    return all_headings\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    FILE_PATH = r\"C:\\Users\\Pratyush\\Desktop\\text-to-json\\examples\\text2SQL-solutions\\chase-sql.pdf\"\n",
    "    headings = extract_headings_from_document(FILE_PATH)\n",
    "    print(\"\\nExtracted Headings & Subheadings:\")\n",
    "    # for h in headings:\n",
    "    #     print(\"-\", h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|██████████| 14/14 [00:02<00:00,  4.92it/s]\n",
      "100%|██████████| 56/56 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'heading': 'abstract',\n",
       "  'content:': 'Most existing studies in text-to-SQL tasks do\\nnot require generating complex SQL queries\\nwith multiple clauses or sub-queries, and gen-\\neralizing to new, unseen databases.\\nIn this\\npaper we propose SyntaxSQLNet, a syntax\\ntree network to address the complex and cross-\\ndomain text-to-SQL generation task.\\nSyn-\\ntaxSQLNet employs a SQL specic syn-\\ntax tree-based decoder with SQL generation\\npath history and table-aware column atten-\\ntion encoders.\\nWe evaluate SyntaxSQLNet\\non the Spider text-to-SQL task, which con-\\ntains databases with multiple tables and com-\\nplex SQL queries with multiple SQL clauses\\nand nested queries. We use a database split\\nsetting where databases in the test set are un-\\nseen during training.\\nExperimental results\\nshow that SyntaxSQLNet can handle a signi-\\ncantly greater number of complex SQL exam-\\nples than prior work, outperforming the pre-\\nvious state-of-the-art model by 7.3% in ex-\\nact matching accuracy.\\nWe also show that\\nSyntaxSQLNet can further improve the perfor-\\nmance by an additional 7.5% using a cross-\\ndomain augmentation method, resulting in a\\n14.8% improvement in total. To our knowl-\\nedge, we are the rst to study this complex and\\ncross-domain text-to-SQL task.',\n",
       "  'chunk_id': None},\n",
       " {'heading': 'authors',\n",
       "  'content:': ['Tao Yu',\n",
       "   'Michihiro Yasunaga',\n",
       "   'Kai Yang',\n",
       "   'Rui Zhang',\n",
       "   'Dongxu Wang',\n",
       "   'Zifan Li',\n",
       "   'Dragomir R. Radev'],\n",
       "  'chunk_id': None},\n",
       " {'heading': 'date_written', 'content:': '25 Oct 2018', 'chunk_id': None},\n",
       " {'heading': 'title',\n",
       "  'content:': 'SyntaxSQLNet: Syntax Tree Networks for Complex and Cross-Domain Text-to-SQL Task',\n",
       "  'chunk_id': None},\n",
       " {'heading': 'Abstract',\n",
       "  'chunk_id': 0,\n",
       "  'content': ' Most existing studies in text-to-SQL tasks do not require generating complex SQL queries with multiple clauses or sub-queries, and gen- eralizing to new, unseen databases. In this paper we propose SyntaxSQLNet, a syntax tree network to address the complex and cross- domain text-to-SQL generation task. Syn- taxSQLNet employs a SQL specic syn- tax tree-based decoder with SQL generation path history and table-aware column atten- tion encoders. We evaluate SyntaxSQLNet on the Spider text-to-SQL task, which con- tains databases with multiple tables and com- plex SQL queries with multiple SQL clauses and nested queries. We use a database split setting where databases in the test set are un- seen during training. Experimental results show that SyntaxSQLNet can handle a signi- cantly greater number of complex SQL exam- ples than prior work, outperforming the pre- vious state-of-the-art model by 7.3% in ex- act matching accuracy. We also show that SyntaxSQLNet can further improve the perfor- mance by an additional 7.5% using a cross- domain augmentation method, resulting in a 14.8% improvement in total. To our knowl- edge, we are the rst to study this complex and cross-domain text-to-SQL task.1 '},\n",
       " {'heading': '1 Introduction',\n",
       "  'chunk_id': 0,\n",
       "  'content': ' Text-to-SQL task is one of the most important sub- task of semantic parsing in natural language pro- cessing (NLP). It maps natural language sentences to corresponding SQL queries. In recent years, some state-of-the-art methods with Seq2Seq encoder-decoder architectures are able to obtain more than 80% exact matching ac- curacy on some complex text-to-SQL benchmarks such as ATIS and GeoQuery. These models seem to have already solved most problems in this area. 1Code available at https://github.com/taoyds/syntaxsql avg salary dept_name NONE SELECT ROOT HAVING GROUP > dept_name What are the name and lowest instructor salary of the departments with average salary greater than the overall average? Complex input sentence: Database: instructor Table 1 department Table 2 ...... Table n Columns ID name department_name salary .... name building budget  ....... primarykey foreignkey Correct SQL  translation: SELECTmin(salary),department_name FROMinstructor GROUPBYdepartment_name HAVINGavg(T1.salary)> (SELECTavg(salary)FROMinstructor) Our tree-based SQL generation:  ROOT SELECT salary avg min none salary Figure 1: To address the complex text-to-SQL gener- ation task, SyntaxSQLNet employs a tree-based SQL generator. For example, our model can systematically generate a nested query as illustrated above. However, as (Finegan-Dollak et al., 2018) show, because of the problematic task denition in the traditional datasets, most of these models just learn to match semantic parsing results, rather than truly learn to understand the meanings of inputs and generalize to new programs and databases. More specically, most existing complex text-to- SQL datasets have less than 500 SQL labels. They are expanded by paraphrasing 4-10 questions for each SQL query. Under the standard train and test split (Zettlemoyer and Collins, 2005), most queries in the test set also appear in the train set. The WikiSQL dataset recently developed arXiv:1810.05237v2  [cs.CL]  25 Oct 2018 by (Zhong et al., 2017) is much larger and does use different databases for training and testing, but it only contains very simple SQL queries and database schemas. To address those issues in the current semantic parsing datasets, Yu et al. (2018b) have developed a large-scale human labeled text-to-SQL dataset consisting of 10,181 questions, 5,693 unique com- plex SQL queries, and 200 databases with multi- ple tables. They split the dataset into train/dev/test by databases, dening a new complex and cross- domain text-to-SQL task that requires models to generalize well to both new SQL queries and databases. The task cannot be solved easily with- out truly understanding the semantic meanings of the input questions. In this paper, we propose SyntaxSQLNet, a SQL specic syntax tree network to address the Spider task. Specically, to generate complex SQL queries with multiple clauses, selections and sub-queries, we develop a syntax tree-based de- coder with SQL generation path history. To make our model learn to generalize to new databases with new tables and columns, we also develop a table-aware column encoder. Our contributions are as follows:  We propose SQL specic syntax tree networks for the complex and cross-domain text-to-SQL task, which is able to solve nested queries on unseen databases. We are the rst to develop a methodology for this challenging semantic parsing task.  We introduce a SQL specic syntax tree-based decoder with SQL path history and table-aware column attention encoders. Even with no hyper- parameter tuning, our model can signicantly outperform the previous best models, with an 7.3% boost in exact matching accuracy. Error analysis shows that our model is able to general- ize, and solve much more complex (e.g., nested) queries in new databases than prior work.  We also develop a cross-domain data augmen- tation method to generate more diverse training examples across databases, which further im- proves the exact matching accuracy by 7.5%. As a result, our model achieves 27.2% accuracy, a 14.8% total improvement compared with the previous best model. '},\n",
       " {'heading': '2 Related Work',\n",
       "  'chunk_id': 1,\n",
       "  'content': ' Semantic parsing maps natural language to formal meaning representations. There are a range of rep- resentations, such as logic forms and executable programs (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Pasupat and Liang, 2015; Herzig and Berant, 2018). As a sub-task of semantic parsing, the text-to- SQL problem has been studied for decades (War- ren and Pereira, 1982; Popescu et al., 2003a, 2004; Li et al., 2006; Giordani and Moschitti, 2012; Wang et al., 2017b). The methods proposed in the database community (Li and Jagadish, 2014; Yaghmazadeh et al., 2017) tend to involve hand feature engineering and user interactions with the systems. In this work, we focus on recent neu- ral network-based approaches (Yin et al., 2016; Zhong et al., 2017; Xu et al., 2017; Wang et al., 2017a; Iyer et al., 2017; Gur et al., 2018; Suhr et al., 2018). Dong and Lapata (2016) intro- duce a sequence-to-sequence (seq2seq) approach to converting texts to logical forms. Most pre- vious work focuses on a specic table schema. Zhong et al. (2017) publish the WikiSQL dataset and propose a seq2seq model with reinforcement learning to generate SQL queries. Xu et al. (2017) further improve the results on the WikiSQL task by using a SQL-sketch based approach employ- ing a sequence-to-set model. Dong and Lap- ata (2018) propose a coarse-to-ne model which achieves the new state-of-the-art performances on several datasets including WikiSQL. Their model rst generate a sketch of the target program. Then the model lls in missing details in the sketch. Our syntax tree-based decoder is related to re- cent work that exploits syntax information for code generation tasks (Yin and Neubig, 2017; Ra- binovich et al., 2017). Yin and Neubig (2017) introduce a neural model that transduces a nat- ural language statement into an abstract syntax tree (AST). While they format the generation pro- cess as a seq2seq decoding of rules and tokens, our model uses a sequence-to-set module for each grammar component, and calls them recursively to generate a SQL syntax tree. Similarly, Rabinovich et al. (2017) propose abstract syntax networks that use a collection of recursive modules for decod- ing. Our model differs from theirs in the following points. First, we exploit a SQL specic grammar instead of AST. AST-based models have to pre- dict many non-terminal rules before predicting the terminal tokens, involving more steps. Whereas, our SQL-specic grammar enables direct predic- tion of SQL tokens. Second, our model uses dif- ferent sequence-to-set modules to avoid the or- dering issue (Xu et al., 2017) in many code gen- eration tasks. Third, different from (Rabinovich et al., 2017), we pass a pre-order traverse of SQL decoding history to each module. This provides each module with important dependence informa- tion: e.g., if a SQL query has GROUP BY, it is very likely that the grouped column has appeared in SELECT too. Finally, instead of sharing param- eters across different modules, we train each mod- ule separately, because the parameters of different modules could have different converge times. In addition to the distinction in model design, our work differs from theirs in the data and task denition. They aim to develop general syn- tax model for code generation via abstract syn- tax trees. Instead, we are interested in solving the complex and cross-domain SQL query generation problem; this motivates us to take advantage of SQL specic syntax for decoding, which guides systematic generation of complex SQL queries. '},\n",
       " {'heading': '3 Problem Formulation',\n",
       "  'chunk_id': 2,\n",
       "  'content': ' This work aims to tackle the complex text-to-SQL task that involves multiple tables, SQL clauses and nested queries. Further, we use separate databases for training and testing, aiming to develop models that generalize to new databases. '},\n",
       " {'heading': 'Dataset',\n",
       "  'chunk_id': 2,\n",
       "  'content': '. We use Spider (Yu et al., 2018b) 2 as the main dataset, which contains 10,181 ques- tions, 5,693 unique complex SQL queries, and 200 databases with multiple tables. '},\n",
       " {'heading': 'Task and Challenges',\n",
       "  'chunk_id': 2,\n",
       "  'content': '.  The dataset contains a large number of complex SQL labels, which involve more tables, SQL clauses, and nested queries than prior datasets such as WikiSQL. Existing models developed for the WikiSQL task cannot handle those com- plex SQL queries in the Spider dataset.  The dataset contains 200 databases (138 do- mains), and different databases are used for 2The Spider task website is at https://yale-lily. github.io/spider AGG max,min,avg,sum,count,none : OP =,<,>,>=,<=,!=, LIKE,NOTIN,BETWEEN : IUEN KW :INTERSECT,UNION,EXCEPT,NONE :SELECT,WHERE,GROUP,ORDER Pop ROOT IUEN NONE KW IUE 2 sub-SQLs KW COL \"GROUP\" + COL HAVING \"ORDER\" \"SELECT\" / + COL \"HAVING\"/ AGG \"WHERE\" \"HAVING\" / + COL OP OP ROOT/TERMINAL \"WHERE\" + COL AND/OR History Current Token Module to call \"ORDER\" + COL DESC/ASC/LIMIT \"GROUP\" Stack IUEN IUEN \"SELECT\" \"HAVING\"/ Push predicted token COL a table column : Token Instances Figure 2: Our modules and SQL grammar used in de- coding process. A round symbol represents a SQL to- kens, a table column, etc. A square symbol indicates a module that predicts the next SQL token from its cor- responding token instances with the same color. training and testing. Unlike most previous se- mantic parsing tasks (e.g., ATIS), this task re- quires models to generalize to new, unseen databases. In sum, we train and test models on different com- plex SQL queries from different databases in this task. This aims to ensure that models can make the correct prediction only when they truly under- stand the meaning of the questions under the given database, rather than by mere memorization. '},\n",
       " {'heading': '4 Methodology',\n",
       "  'chunk_id': 3,\n",
       "  'content': ' Similar to (Rabinovich et al., 2017), our model structures the decoder as a collection of recursive modules. However, as we discussed in the re- lated work section, we make use of a SQL specic grammar to guide the decoding process, which al- lows us to take advantage of SQL queries well- dened structure. Also, modules do not share any parameters so that we train each of them indepen- dently. '},\n",
       " {'heading': '4.1 Module Overview',\n",
       "  'chunk_id': 3,\n",
       "  'content': ' Our model decomposes the SQL decoding process into 9 modules to handle the prediction of differ- ent SQL components such as keywords, operators, and columns. We provide the overview in this sec- tion and more details in later sections. Figure 2 illustrates our modules and SQL gram- mar used in decoding process. A round symbol represents a SQL token, such as SELECT, WHERE, a table column, etc. A square symbol indicates a module that predicts the next SQL token from its corresponding token instances with the same color. Specically, we have the following mod- ules.  IUEN Module, predicting INTERSECT, UNION, EXCEPT, and NONE, which deter- mines if we need to call itself again to gen- erate nested queries.  KW Module, predicting keywords from WHERE, GROUP BY, and ORDER BY. All queries in our dataset have SELECT.  COL Module, predicting table columns.  OP Module, for =, >, <, >=, <=, ! =, LIKE, NOT IN, IN, BETWEEN.  AGG Module, predicting aggregators from MAX, MIN, SUM, COUNT, AVG, and NONE.  Root/Terminal Module, predicting the ROOT of a new subquery or terminal value. It also enables our model to generate nested queries.  AND/OR Module, predicting the presence of AND or OR operator between two conditions.  DESC/ASC/LIMIT Module, predicting the keywords associated with ORDER BY. It is invoked only when ORDER BY is predicted before.  HAVING Module, predicting the presence of HAVING for GROUP BY clause. It is invoked only when GROUP BY is predicted earlier. '},\n",
       " {'heading': '4.2 SQL Grammar',\n",
       "  'chunk_id': 4,\n",
       "  'content': ' In order to structure our decoder to generate com- plex queries, we consider a SQL grammar. It determines which module to be invoked at each recursive decoding step. Figure 2 illustrates our SQL grammar. During decoding process, given the current SQL token and the SQL history (the tokens we have gone over to reach the current to- ken), we determine which module to invoke, and predict the next SQL token to generate. To invoke some modules such as HAVING and OP during decoding, we not only check the type of current token instance but also see whether the type of the previously decoded SQL token is GROUP for HAVING module, and WHERE or HAVING for OP module. In the grammar, IUEN and Root/Terminal mod- ules are able to generate ROOT, which can activate IUEN module again. In this way, our model can recursively generate nested subqueries, and can also predict two or more subqueries in queries that have EXCEPT, INTERSECT, and UNION. '},\n",
       " {'heading': '4.3 Input Encoder',\n",
       "  'chunk_id': 4,\n",
       "  'content': ' Our inputs of each module consist of three types of information: question, table schema, and current SQL decoding history path. We encode a question sentence by a bi-directional LSTM, BiLSTMQ. We encode table schema and history path in the manners described below. '},\n",
       " {'heading': '4.3.1 Table-Aware Column Representation',\n",
       "  'chunk_id': 4,\n",
       "  'content': ' In order to generalize to new databases in testing, it is important to make our model learn to obtain necessary information from a database schema. SQLNet (Xu et al., 2017) encodes this infor- mation by running different bi-directional LSTMs over words in each column name, whereas Type- SQL (Yu et al., 2018a) rst obtains embedding for each column name by taking the average embed- ding of the words constituting the column name, and then runs a single biLSTM on the embeddings of all columns in a table. Yu et al. (2018b) show that the column encoding method of SQLNet out- performs that of TypeSQL in the database split setting, and the result reverses under the example split setting. While SQLNet and TypeSQL only need the col- umn names as WikiSQL dataset only contains one table per question-SQL pair, Spiders databases contain multiple tables. To address this setting, we propose to use both table and column names to construct column embeddings. Specically, given a database, for each col- umn, we rst get the list for words in its table name, words in its column name, and the type in- formation of the column (string, or number, pri- mary/foreign key), as an initial input of the col- umn. Next, like SQLNet, the table-aware column representation of the given column is computed as the nal hidden state of a BiLSTM running on top of this sequence. This way, the encoding scheme can capture both the global (table names) and lo- cal (column names and types) information in the database schema to understand a natural language question in the context of the given database. We also experimented with a hierarchical table and column encoding, where we rst obtain em- bedding for each table name and then incorporate that information into column encoding. But this encoding method did not perform as well. '},\n",
       " {'heading': '4.3.2 SQL Decoding History',\n",
       "  'chunk_id': 4,\n",
       "  'content': ' In addition to question and column information, we also pass the SQL querys current decoding history as an input to each module. This enables us to use the information of previous decoding states to predict the next SQL token. For example, in Figure 1, the COL module would be more likely to predict salary in the subquery by consider- ing the path history which contains salary for HAVING, and SELECT in the main query. In contract, each module in SQLNet does not consider the previous decoded SQL history. Hence, if directly applied to our recursive SQL de- coding steps, each module would just predict the same output every time it is invoked. By passing the SQL history, each module is able to predict a different output according to the history every time it is called during the recursive SQL genera- tion process. Also, the SQL history can improve the performance of each module on long and com- plex queries because the history helps the model capture the relations between clauses. Predicted SQL history is used during test de- coding. For training, we rst traverse each node in the gold query tree in pre-order to generate gold SQL path history for each training example used in different modules. '},\n",
       " {'heading': '4.3.3 Attention for Input Encoding',\n",
       "  'chunk_id': 5,\n",
       "  'content': ' For each module, like SQLNet (Xu et al., 2017), we apply the attention mechanism to encode ques- tion representation. We also employs this tech- nique on SQL path history encoding. The specic formulas used are described in the next section. '},\n",
       " {'heading': '4.4 Module Details',\n",
       "  'chunk_id': 5,\n",
       "  'content': ' Similarly to SQLNet, we employ a sketch-based approach for each module. We apply a sequence- to-set prediction framework introduced by (Xu et al., 2017), to avoid the order issue that hap- pens in seq2seq based models for SQL gen- eration. For example, in Figure 1, SELECT salary, dept name is the same as SELECT dept name, salary. The traditional seq2seq decoder generates each of them one by one in or- der; hence the model could get penalized even if the prediction and gold label are the same as sets. To avoid this problem, SQLNet predicts them to- gether in one step so that their order does not affect the models training process. For instance, in Fig- ure 1, our model invokes the COL module to pre- dict salary and dept name, and push to stack at the same time. However, SQLNet only covers pre-dened SQL sketches, and its modules do not pass information to one another. To resolve these problems, Syn- taxSQLNet employs a syntax tree-based decod- ing method that recursively calls different modules based on a SQL grammar. Further, the history of generated SQL tokens is passed through modules, allowing SyntaxSQLNet to keep track of the re- cursive decoding steps. We rst describe how to compute the condi- tional embedding H1/2 of an embedding H1 given another embedding H2: H1/2 = softmax(H1WH 2 )H1. Here W is a trainable parameter. Moreover, we get a probability distribution from a given score matrix U by P(U) = softmax (Vtanh(U)) , where V is a trainable parameter. We denote the hidden states of LSTM on ques- tion embeddings, path history, and columns em- beddings as HQ, HHS, and HCOL respectively. In addition, we denote the hidden states of LSTM on multiple keywords embeddings and keywords embeddings as HMKW and HKW respectively. Fi- nally, we use W to denote trainable parameters that are not shared between modules. The output of each module is computed as follows: '},\n",
       " {'heading': 'IUEN Module',\n",
       "  'chunk_id': 5,\n",
       "  'content': ' In the IUEN module, since only one of the multiple keywords from {INTERSECT, UNION, EXCEPT, NONE} will be used, we compute the probabilities by PIUEN = P  W1H Q/MKW + W2H HS/MKW + W3H MKW  '},\n",
       " {'heading': 'KW Module',\n",
       "  'chunk_id': 5,\n",
       "  'content': ' In the KW module, we rst predict the number of keywords in the SQL query and then predict the keywords from {SELECT, WHERE, GROUP BY, ORDER BY}. P num KW = P  Wnum 1 Hnum Q/KW + Wnum 2 Hnum HS/KW  P val KW = P  Wval '},\n",
       " {'heading': '1 Hval', 'chunk_id': 6, 'content': ' Q/KW + Wval '},\n",
       " {'heading': '2 Hval', 'chunk_id': 6, 'content': ' HS/KW + Wval '},\n",
       " {'heading': '3 HKW', 'chunk_id': 6, 'content': ' '},\n",
       " {'heading': 'COL Module',\n",
       "  'chunk_id': 6,\n",
       "  'content': ' Similarly, in the COL module, we rst predict the number of columns in the SQL query and then predict which ones to use. P num COL = P  Wnum '},\n",
       " {'heading': '1 Hnum', 'chunk_id': 6, 'content': ' Q/COL + Wnum '},\n",
       " {'heading': '2 Hnum',\n",
       "  'chunk_id': 6,\n",
       "  'content': ' HS/COL  P val COL = P  Wval '},\n",
       " {'heading': '1 Hval', 'chunk_id': 6, 'content': ' Q/KW + Wval '},\n",
       " {'heading': '2 Hval',\n",
       "  'chunk_id': 6,\n",
       "  'content': ' HS/KW + Wval 3 HKW COL Module Similarly, in the COL module, we rst predict the number of columns in the SQL query and then predict which ones to use. P num COL = P  Wnum 1 Hnum Q/COL + Wnum 2 Hnum HS/COL  P val COL = P  Wval 1 Hval Q/COL + Wval 2 Hval HS/COL + Wval '},\n",
       " {'heading': '3 HCOL', 'chunk_id': 6, 'content': ' '},\n",
       " {'heading': 'OP Module',\n",
       "  'chunk_id': 6,\n",
       "  'content': ' In the OP module, for each pre- dicted column from the COL module that is in the WHERE clause, we rst predict the num- ber of operators on it then predict which op- erators to use from {=, >, <, >=, <=, != , LIKE, NOTIN, IN, BETWEEN}. We use HCS to denote the embedding of one of the predicted columns from the COL module. P num OP = P  Wnum '},\n",
       " {'heading': '1 Hnum', 'chunk_id': 6, 'content': ' Q/COL + Wnum '},\n",
       " {'heading': '2 Hnum',\n",
       "  'chunk_id': 6,\n",
       "  'content': ' HS/COL  P val COL = P  Wval '},\n",
       " {'heading': '1 Hval', 'chunk_id': 6, 'content': ' Q/KW + Wval '},\n",
       " {'heading': '2 Hval',\n",
       "  'chunk_id': 6,\n",
       "  'content': ' HS/KW + Wval 3 HKW COL Module Similarly, in the COL module, we rst predict the number of columns in the SQL query and then predict which ones to use. P num COL = P  Wnum 1 Hnum Q/COL + Wnum 2 Hnum HS/COL  P val COL = P  Wval 1 Hval Q/COL + Wval 2 Hval HS/COL + Wval 3 HCOL OP Module In the OP module, for each pre- dicted column from the COL module that is in the WHERE clause, we rst predict the num- ber of operators on it then predict which op- erators to use from {=, >, <, >=, <=, != , LIKE, NOTIN, IN, BETWEEN}. We use HCS to denote the embedding of one of the predicted columns from the COL module. P num OP = P  Wnum 1 Hnum Q/CS + Wnum 2 Hnum HS/CS + Wnum '},\n",
       " {'heading': '3 HCS',\n",
       "  'chunk_id': 6,\n",
       "  'content': ' P val OP = P  Wval 1 Hval Q/CS + Wval 2 Hval HS/CS + Wval 3 HCS '},\n",
       " {'heading': 'AGG Module',\n",
       "  'chunk_id': 6,\n",
       "  'content': ' In the AGG module, for each predicted column from the COL module, we rst predict the number of aggregators on it then predict which aggregators to use from {MAX, MIN, SUM, COUNT, AVG, NONE} P num AGG = P  Wnum '},\n",
       " {'heading': '1 Hnum', 'chunk_id': 6, 'content': ' Q/COL + Wnum '},\n",
       " {'heading': '2 Hnum',\n",
       "  'chunk_id': 6,\n",
       "  'content': ' HS/COL  P val COL = P  Wval '},\n",
       " {'heading': '1 Hval',\n",
       "  'chunk_id': 6,\n",
       "  'content': ' Q/KW + Wval 2 Hval HS/KW + Wval 3 HKW COL Module Similarly, in the COL module, we rst predict the number of columns in the SQL query and then predict which ones to use. P num COL = P  Wnum 1 Hnum Q/COL + Wnum 2 Hnum HS/COL  P val COL = P  Wval 1 Hval Q/COL + Wval 2 Hval HS/COL + Wval 3 HCOL OP Module In the OP module, for each pre- dicted column from the COL module that is in the WHERE clause, we rst predict the num- ber of operators on it then predict which op- erators to use from {=, >, <, >=, <=, != , LIKE, NOTIN, IN, BETWEEN}. We use HCS to denote the embedding of one of the predicted columns from the COL module. P num OP = P  Wnum 1 Hnum Q/CS + Wnum 2 Hnum HS/CS + Wnum 3 HCS P val OP = P  Wval 1 Hval Q/CS + Wval 2 Hval HS/CS + Wval 3 HCS AGG Module In the AGG module, for each predicted column from the COL module, we rst predict the number of aggregators on it then predict which aggregators to use from {MAX, MIN, SUM, COUNT, AVG, NONE} P num AGG = P  Wnum 1 Hnum Q/CS + Wnum 2 Hnum HS/CS + Wnum 3 HCS P val AGG = P  Wval 1 Hval Q/CS + Wval 2 Hval HS/CS + Wval 3 HCS '},\n",
       " {'heading': 'Root/Terminal Module',\n",
       "  'chunk_id': 7,\n",
       "  'content': ' To predict nested sub- queries, we add a module to predict if there is a new ROOT after an operator, which allows the model to decode queries recursively. For each pre- dicted column from the COL module that is in the WHERE clause, we rst call OP module, and then predict whether the next decoding step is a ROOT node or a value terminal node by PRT = P  W1H Q/CS + W2H HS/CS + W3H CS  '},\n",
       " {'heading': 'AND/OR Module',\n",
       "  'chunk_id': 7,\n",
       "  'content': ' For each condition column predicted from the COL module with number big- ger than 1, we predict from {AND, OR} by PAO = P  W1H Q + W2H HS  '},\n",
       " {'heading': 'DESC/ASC/LIMIT Module',\n",
       "  'chunk_id': 7,\n",
       "  'content': ' In this module, for each predicted column from the COL module that is in the ORDER BY clause, we predict from {DESC, ASC, DESC LIMIT, ASC LIMIT} by PDAL = P  W1H Q/CS + W2H HS/CS + W3H CS  '},\n",
       " {'heading': 'HAVING Module',\n",
       "  'chunk_id': 7,\n",
       "  'content': ' In the HAVING module, for each predicted column from the COL module that is in the GROUP BY clause, we predict whether it is in the HAVING clause by PHAVING = P  W1H Q/CS + W2H HS/CS + W3H CS  '},\n",
       " {'heading': '4.5 Recursive SQL Generation',\n",
       "  'chunk_id': 7,\n",
       "  'content': ' The SQL generation process is a process of ac- tivating different modules recursively. As illus- trated in Figure 2, we employ a stack to organize our decoding process. At each decoding step, we pop one SQL token instance from the stack, and invoke a module based on the grammar to predict the next token instance, and then push the pre- dicted instance into the stack. The decoding pro- cess continues until the stack is empty. More specically, we initialize a stack with only ROOT at the rst decoding step. At the next step, the stack pops ROOT. As illustrated in Figure 2, ROOT actives the IUEN module to predict if there is EXCEPT, INTERSECT or UNION. If so, there are two subqueries to be generated in the next step. If the model predicts NONE instead, it will be pushed into the stack. The stack pops NONE at next step. For example, in Figure 2, the current popped token is SELECT, which is a instance of keyword (KW) type. It calls the COL module to predict a column name, which will be pushed to the stack. '},\n",
       " {'heading': '4.6 Data Augmentation',\n",
       "  'chunk_id': 7,\n",
       "  'content': ' Even though Spider already has a signicantly larger number of complex queries than existing datasets, the number of training examples for some complex SQL components is still limited. A widely used way is to conduct data augmenta- tion to generate more training examples automati- cally. Many studies (Berant and Liang, 2014; Iyer et al., 2017; Su and Yan, 2017) have shown that data augmentation can bring signicant improve- ment in performance. In prior work, data augmentation was typically performed within a single domain dataset. We pro- pose a cross-domain data augmentation method to expand our training data for complex queries. Cross-domain data augmentation is more dif- cult than the in-domain setting because question- program pairs tend to have domain specic words and phrases. To tackle this issue, we rst create a list of universal patterns for question-SQL pairs, based on the human labeled pairs from all the differ- ent training databases in Spider. To do so, we use a script to remove (and later ll in) all the table / column names and value tokens in the la- beled question-SQL pairs, and then group together the same SQL query patterns. Consequently, each SQL query pattern has a list of about 5-20 corre- sponding questions. In our task, we want to gen- erate more complex training examples. Thus, we lter out simple SQL query patterns by measuring the length and the number of SQL keywords used. We obtain about 280 different complex SQL query patterns from over 4,000 SQL labels in the train set of our corpus. We then select the 50 most frequent complex SQL patterns that contain multiple SQL components and nested subqueries. After this, we manually edit the selected SQL patterns and their corresponding list of questions to make sure that the table/column/value slots in the questions have one-to-one correspondence to the slots in the corresponding SQL query. For each slot, we also add column type or table informa- tion. Thus, for example, columns with string type do not appear in the column slot with integer type during data augmentation (i.e., slot relling) pro- cess. In this way, our question-SQL patterns are generated based on existing human labeled exam- ples, which ensures that the generated training ex- amples are natural. Once we have the one-to-one slot mapping be- tween questions and SQL queries, we apply a script that takes a new database schema with type information and generates new question-SQL ex- amples by lling empty slots. Specically, for each table in WikiSQL, we rst randomly sample 10 question-SQL patterns. We randomly sample columns from the database schema based on its type: for example, if the slot type in the pattern is number, and then we only sample from columns with real type in the current table. We then rell the slots in both the question and SQL query with the selected column names. Similarly, we also re- ll table/value slots. By this data augmentation method, we nally obtain about 98,000 question and SQL pairs using some WikiSQL databases with one single table. '},\n",
       " {'heading': '5 Experiments', 'chunk_id': 8, 'content': ' '},\n",
       " {'heading': '5.1 Dataset',\n",
       "  'chunk_id': 8,\n",
       "  'content': ' In our experiments, we use Spider (Yu et al., 2018b), a new large-scale human annotated text- to-SQL dataset with complex SQL queries and cross-domain databases. In addition to their origi- nally annotated data, their training split includes 752 queries and 1659 questions from six ex- isting datasets: Restaurants (Tang and Mooney, 2001; Popescu et al., 2003b), GeoQuery (Zelle and Mooney, 1996), Scholar (Iyer et al., 2017), Aca- demic (Li and Jagadish, 2014), Yelp and IMDB (Yaghmazadeh et al., 2017). In total, this dataset consists of 11,840 questions, 6,445 unique com- plex SQL queries, and 206 databases with multi- ple tables. We follow (Yu et al., 2018b), and use 146, 20, 40 databases for train, development, test, respectively (randomly split). We also include the question-SQL pair examples generated by our data augmentation method in some experiments. '},\n",
       " {'heading': '5.2 Metrics',\n",
       "  'chunk_id': 8,\n",
       "  'content': ' We evaluate our model using SQL Component Matching and Exact Matching proposed by (Yu et al., 2018b). To compute the component matching scores, Yu et al. (2018b) rst decom- pose predicted queries on SQL clauses including SELECT, WHERE, GROUP BY, ORDER BY, and KEYWORDS separately. After that, they evaluate each predicted clause and the ground truth as bags of several sub-components, and check whether or not these two sets of components match exactly. Exact matching score is 1 if the model predicts all clauses correctly for a given example. To better understand model performance on different queries, (Yu et al., 2018b) divide SQL queries into 4 levels: easy, medium, hard, extra hard. The denition of difculty is based on the number of SQL components, selections, and con- ditions. '},\n",
       " {'heading': '5.3 Experimental Settings',\n",
       "  'chunk_id': 8,\n",
       "  'content': ' Our model is implemented in PyTorch (Paszke et al., 2017). We build each module based on the TypeSQL (Yu et al., 2018a) implementation. We use xed, pre-trained GloVe (Pennington et al., 2014) embeddings for question, SQL history, and schema tokens. For each experiment, the dimen- sion and dropout rate of all hidden layers is set to 120 and 0.3 respectively. We use Adam (Kingma and Ba, 2015) with the default hyperparameters for optimization, with a batch size of 64. The same loss functions in (Xu et al., 2017) are used for each module. The code is available on https: //github.com/taoyds/syntaxsql. '},\n",
       " {'heading': '6 Results and Discussion',\n",
       "  'chunk_id': 9,\n",
       "  'content': ' Table 1 presents SyntaxSQLNets dev and test re- sults compared to previous state-of-the-art mod- els on the Spider dataset with database splitting. Our model with SQL history and data augmenta- '},\n",
       " {'heading': 'Method',\n",
       "  'chunk_id': 9,\n",
       "  'content': ' Test Dev Easy Medium Hard Extra Hard All All Seq2Seq 11.9% 1.9% 1.3% 0.5% 3.7% 1.9% Seq2Seq+Attention 14.9% 2.5% 2.0% 1.1% 4.8% 1.8% Seq2Seq+Copying 15.4% 3.4% 2.0% 1.1% 5.3% 4.1% SQLNet 26.2% 12.6% 6.6% 1.3% 12.4% 10.9% TypeSQL 19.6% 7.6% 3.8% 0.8% 8.2% 8.0% SyntaxSQLNet 48.0% 27.0% 24.3% 4.6% 27.2% 24.8% -augment 38.6% 17.6% 16.3% 4.9% 19.7% 18.9% -table -augment 37.5% 13.5% 12.4% 1.3% 16.4% 15.9% -history -table -augment 18.1% 7.0% 0.2% 0.0% 6.8% 6.1% Table 1: Accuracy of Exact Matching on SQL queries with different hardness levels. Method SELECT WHERE GROUP BY ORDER BY KEYWORDS Seq2Seq 13.0% 1.5% 3.3% 5.3% 8.7% Seq2Seq+Attention 13.6% 3.1% 3.6% 9.9% 9.9% Seq2Seq+Copying 12.0% 3.1% 5.3% 5.8% 7.3% SQLNet 44.5% 19.8% 29.5% 48.8% 64.0% TypeSQL 36.4% 16.0% 17.2% 47.7% 66.2% SyntaxSQLNet 62.5% 34.8% 55.6% 60.9% 69.6% -augment 53.9% 24.5% 44.4% 49.5% 71.3% -table -augment 48.9% 20.1% 36.3% 46.8% 69.7% -history -table -augment 26.7% 14.6% 11.8% 34.9% 64.6% Table 2: F1 scores of Component Matching on all SQL queries on Test set. tion achieves 27.2% exact matching on all SQL queries, which is about 15% absolute increase compared to the previous best models, SQLNet and TypeSQL. '},\n",
       " {'heading': '6.1 Comparison to Existing Methods',\n",
       "  'chunk_id': 10,\n",
       "  'content': ' Even though our individual modules are similar to SQLNet and TypeSQL, our syntax-aware de- coder allows the modules to generate complex SQL queries in a recursive manner based on the SQL grammar. In addition, by incorporating the SQL decoding history into modules during the de- coding process, SyntaxSQL achieves a signicant gain in exact matching for queries of all hardness levels. Specically, even without our data aug- mentation technique, SyntaxSQLNet outperforms the previous best, SQLNet, by 7.3%. This result suggests that the syntax and history information is benecial for this complex text-to-SQL task. Moreover, the tree-based decoder enables Syn- taxSQLNet to systematically generate nested queries, boosting the performance for Hard/Extra Hard. As Table 1 shows, SyntaxSQLNet achieves particularly high scores 24.3% and 4.6% for Hard and Extra Hard, which contain nested queries. The Seq2Seq models suffer from generating ungram- matical queries, yielding very low exact matching accuracy on Hard and Extra Hard SQL queries. In contrast, our model generates valid SQL queries by enforcing the syntax. For the detailed component matching results in Table 2, our model consistently outperforms other previous work by signicant margins. Speci- cally, our model improve F1 score for most of the SQL components by more than 10%. '},\n",
       " {'heading': '6.2 Ablation Study',\n",
       "  'chunk_id': 10,\n",
       "  'content': ' In order to understand the techniques that are re- sponsible for the performance of our model, we perform an ablation study where we remove one of the proposed techniques from our model at a time. The exact match scores are shown in the same ta- bles as other previous models. '},\n",
       " {'heading': 'Data Augmentation',\n",
       "  'chunk_id': 10,\n",
       "  'content': ' Our models exact match- ing performance on all queries drops 7.5% by ex- cluding data augmentation technique. This drop is particularly large for GROUP BY and ORDER BY components (Table 2), for which the original Spider dataset has a relatively small number of training examples. Our cross-domain data aug- mentation technique provides signicantly more examples for column prediction (especially un- der GROUP BY and ORDER BY clauses), which greatly benets the overall model performance. '},\n",
       " {'heading': 'Column Encoding',\n",
       "  'chunk_id': 11,\n",
       "  'content': ' To see how our table-aware column encoding affects performance of our model, we also report the models result with- out using table information for our column encod- ing. After excluding the table embedding from column embeddings, the test performance further goes down by 3.3%. This drop is especially large for Medium/Hard SQL queries, where the correct column prediction is a key. Additionally, in Ta- ble 2, the models performance on GROUP BY component decreases dramatically because it is hard to predict group-by columns correctly with- out table information (e.g. multiple different ta- bles may have a column of the same name id in the database). This result shows that the table- aware encoding is important to predict the correct columns in unseen, complex databases (with many foreign keys). '},\n",
       " {'heading': 'SQL Decoding History',\n",
       "  'chunk_id': 11,\n",
       "  'content': ' In order to gain more insight into how our SQL decoding history ad- dresses complex SQL, we report our models per- formance without SQL path history. As shown in the Table 1, the models performance drops about 9.6% on exacting matching metric without con- sidering the previous decoding states in each de- coding state. More importantly, its performance on hard and extra hard SQL queries decreases to 0%. This indicates that our model is able to predict nested queries thanks to the SQL decoding history. '},\n",
       " {'heading': '6.3 Error Analysis and Future Work',\n",
       "  'chunk_id': 11,\n",
       "  'content': ' The most common errors are from column pre- diction. Future work may include developing a database schema encoder that can capture re- lationships among columns and foreign keys in the database more effectively. Other common er- rors include incorrect prediction of SQL skeleton structures, aggregators and operators. There are also a few limitations in our model. For example, SyntaxSQLNet rst predicts all the column names in the SQL query, and then chooses tables to generate the FROM clause based on the selected columns. Suppose the natural language input is return the stadium name and the number of concerts held in each stadium. The SQL query predicted by SyntaxSQLNet is SELECT count(*), name FROM stadium GROUP BY stadium id While the correct answer is SELECT T2.name, count(*) FROM concert AS T1 JOIN stadium AS T2 ON T1.stadium id = T2.stadium id GROUP BY T1.stadium id Even though SyntaxSQLNet predicts all column names and keywords correctly, its deterministic FROM clause generation method fails to join tables (concert and stadium in this case) together. One possible solution is to predict table names in the FROM clause by considering the relations among tables in the database. '},\n",
       " {'heading': '7 Conclusion',\n",
       "  'chunk_id': 11,\n",
       "  'content': ' In this paper, we presented a syntax tree-based model to address complex and cross-domain text- to-SQL task. Utilizing a SQL specic syntax de- coder, as well as SQL path history and table-aware column attention encoders, our model outperforms previous work by a signicant margin. The ab- lation study demonstrates that our proposed tech- niques are able to predict nested, complex SQL queries correctly even for unseen databases. '},\n",
       " {'heading': 'Acknowledgement',\n",
       "  'chunk_id': 11,\n",
       "  'content': ' We thank Graham Neubig, Tianze Shi, and three anonymous reviewers for their helpful feedback and discussion on this work. '},\n",
       " {'heading': 'References',\n",
       "  'chunk_id': 11,\n",
       "  'content': ' Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su- pervised learning of semantic parsers for mapping instructions to actions. Transactions of the Associa- tion forComputational Linguistics. Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griftt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking. In Proceedings of the 7th Linguis- tic Annotation Workshop and Interoperability with Discourse. Jonathan Berant and Percy Liang. 2014. Semantic parsing via paraphrasing.'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from t2j.prem_sdk import PremSDK  \n",
    "from t2j.prompts import Prompts\n",
    "from t2j.chunker import DocumentChunker\n",
    "from t2j.utils import *\n",
    "\n",
    "N = 100\n",
    "promptsClass = Prompts()\n",
    "model = PremSDK()\n",
    "chunker = DocumentChunker(prompts=promptsClass, model=model)\n",
    "\n",
    "FILE_PATH = r\"C:/Users/Pratyush/Desktop/text-to-json/examples/text2SQL-solutions/SyntaxSQLNet.pdf\"\n",
    "headings = chunker.smart_chunk(FILE_PATH)\n",
    "headings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[{'heading': 'head1', 'content': '...'}, {'heading': 'subhead1', 'content': '...'}, {'heading': 'subhead2', 'content': '...'}, {'heading': 'head1', 'content': '...'}'subhead1', 'subhead2', 'head2', 'subhead3', ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|██████████| 15/15 [00:03<00:00,  4.66it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 28008.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'heading': 'abstract', 'content': 'In tackling the challenges of large language model (LLM) performance for Text-to-SQL tasks, we introduce CHASE-SQL, a new framework that employs innovative strategies, using test-time compute in multi-agent modeling to improve candidate generation and selection. CHASE-SQL leverages LLMs intrinsic knowledge to generate diverse and high-quality SQL candidates using different LLM generators with: (1) a divide-and-conquer method that decomposes complex queries into manageable sub-queries in a single LLM call; (2) chain-of-thought reasoning based on query execution plans, reflecting the steps a database engine takes during execution; and (3) a unique instance-aware synthetic example generation technique, which offers specific few-shot demonstrations tailored to test questions. To identify the best candidate, a selection agent is employed to rank the candidates through pairwise comparisons with a fine-tuned binary-candidates selection LLM. This selection approach has been demonstrated to be more robust over alternatives. The proposed generators-selector framework not only enhances the quality and diversity of SQL queries but also outperforms previous methods. Overall, our proposed CHASE-SQL achieves the state-of-the-art execution accuracy of 73.0 % and 73.01% on the test set and development set of the notable BIRD Text-to-SQL dataset benchmark, rendering CHASE-SQL the top submission of the leaderboard (at the time of paper submission).'}, {'heading': 'authors', 'content': ['Mohammadreza Pourreza1, Hailong Li1, Ruoxi Sun1, Yeounoh Chung1, Shayan Talaei2, Gaurav Tarlok Kakkar1, Yu Gan1, Amin Saberi2, Fatma zcan1, Sercan . Ark1', '1Google Cloud, Sunnyvale, CA, USA', '2Stanford University, Stanford, CA, USA', '{pourreza, hailongli, ruoxis, yeounoh}@google.com', '{gkakkar, gany, fozcan, soarik}@google.com', '{stalaei, saberi}@stanford.edu', 'Equal contribution']}, {'heading': 'date_written', 'content': 'October 4, 2024'}, {'heading': 'title', 'content': 'CHASE-SQL: Multi-Path Reasoning and Preference Optimized Candidate Selection in Text-to-SQL'}, {'heading': 'Abstract', 'content': ' In tackling the challenges of large language model (LLM) performance for Text-to-SQL tasks, we introduce CHASE-SQL, a new framework that employs innovative strategies, using test-time compute in multi-agent modeling to improve candidate generation and selection. CHASE-SQL leverages LLMs intrinsic knowledge to generate diverse and high-quality SQL candidates using different LLM generators with: (1) a divide-and-conquer method that decomposes complex queries into manageable sub-queries in a single LLM call; (2) chain-of-thought reasoning based on query execution plans, reflecting the steps a database engine takes during execution; and (3) a unique instance-aware synthetic example generation technique, which offers specific few-shot demonstrations tailored to test questions. To identify the best candidate, a selection agent is employed to rank the candidates through pairwise comparisons with a fine-tuned binary-candidates selection LLM. This selection approach has been demonstrated to be more robust over alternatives. The proposed generators-selector framework not only enhances the quality and diversity of SQL queries but also outperforms previous methods. Overall, our proposed CHASE-SQL achieves the state-of-the-art execution accuracy of 73.0 % and 73.01% on the test set and development set of the notable BIRD Text-to-SQL dataset benchmark, rendering CHASE-SQL the top submission of the leaderboard (at the time of paper submission). '}, {'heading': '1 Introduction', 'content': ' Text-to-SQL, as a bridge between human language and machine-readable structured query languages, is crucial for many use cases, converting natural language questions into executable SQL commands (Androutsopoulos et al., 1995; Li & Jagadish, 2014; Li et al., 2024c; Yu et al., 2018; ?). By enabling users to interact with complex database systems without requiring SQL proficiency, Text-to-SQL empowers users to extract valuable insights, perform streamlined data exploration, make informed decisions, generate data-driven reports and mine better features for machine learning (Chen et al., 2023; Prez-Mercado et al., 2023; Pourreza & Rafiei, 2024a; Pourreza et al., 2024; Sun et al., 2023; Wang et al., 2019; Xie et al., 2023). Furthermore, Text-to-SQL systems play a pivotal role in automating data analytics with complex reasoning and powering conversational agents, expanding their applications beyond traditional data retrieval (Sun et al., 2023; Xie et al., 2023). As data continues to grow exponentially, the ability to query databases efficiently without extensive SQL knowledge becomes increasingly vital for a broad range of applications. Text-to-SQL can be considered a specialized form of code generation, with the contextual information potentially including the database schema, its metadata and along with the values. In the broader code generation domain, utilizing LLMs to generate a wide range of diverse candidates and select the best one has proven to be effective (Chen et al., 2021; Li et al., 2022; Ni et al., 2023). However, it is non-obvious what 1 arXiv:2410.01943v1  [cs.LG]  2 Oct 2024 leads to most effective candidate proposal and winner selector mechanisms. A straightforward yet effective approach involves generating candidates using zero-/few-shot or open-ended prompting, followed by selecting the best options utilizing self-consistency (Wang et al., 2022), which entails clustering candidates based on their execution outputs. This approach has demonstrated promising results in several studies (Lee et al., 2024; Maamari et al., 2024; Talaei et al., 2024; Wang et al., 2023). However, a single prompt design might not fully unleash the extensive Text-to-SQL knowledge of LLMs, and self-consistency methods might not be always effective. In fact, as illustrated in Table 1, the most consistent answers would not always be the correct ones, with an upper-bound performance 14% higher than that achieved through self-consistency. This substantial gap highlights the potential for significant improvement by implementing more effective selection methods to identify the best answer from the pool of candidate queries. Table 1: Evaluating single-query gen- eration vs. ensemble methods of self- consistency and the upper bound that can be achieved for Text-to-SQL with Gemini 1.5 Pro on the BIRD dev set. EX stands for execution accuracy. Method EX (%) Single query 63.01 Self-consistency 68.84 (+ 5.84) Upper-bound 82.79 (+ 19.78) Building on the challenges outlined in the previous section, we pro- pose novel approaches to improve LLM performance for Text-to-SQL by leveraging judiciously-designed test-time computations in an agentic framework. As indicated by the upper bound in Table 1, utilizing LLMs intrinsic knowledge offers significant potential for improvement. We propose methods that generate a diverse set of high-quality can- didate responses and apply a selection mechanism to identify the best answer. Achieving both high-quality and diverse candidate responses is critical for the success of scoring-based selection methods. Low diversity limits improvement potential and reduces the difference be- tween self-consistency and scoring-based approaches. While techniques like increasing temperature or reordering prompt contents can boost diversity, they often compromise the quality of the candidates. To address this, we introduce effective candidate generators designed to enhance diversity while maintaining high-quality outputs. Specifically, we propose three distinct candidate generation approaches, each capable of producing high-quality responses. The first is inspired by the divide-and-conquer algorithm, which breaks down complex problems into smaller, manageable parts to handle difficult queries. The second employs a query execution-plan-based chain-of-thought strategy, where the reasoning process mirrors the steps a database engine takes during query execution. Lastly, we introduce a novel online synthetic example generation method, which helps the model better understand the underlying data schema of the test database. These methods, when used independently, can produce highly-accurate SQL outputs. To effectively select the best answer among candidates, we introduce a selection agent, trained with a classification objective, that assigns scores based on pairwise comparisons between candidate queries. With this agent, we construct a comparison matrix for all candidates and select the final response based on the highest cumulative score. By combining these candidate generation methods with the proposed scoring model, we create an ensemble approach that leverages the strengths of each strategy to significantly improve overall performance. We present comprehensive evaluations on the efficacy of proposed methodologies of CHASE-SQL. Our innovative candidate generation approaches demonstrate superior performance compared to traditional generic CoT prompts, illustrating their capability in guiding LLMs through the decomposition of complex problems into manageable intermediate steps. Furthermore, the proposed selection agent significantly outperforms conventional consistency-based methods, contributing to the state-of-the-art results. Specifically, CHASE- SQL reaches an execution accuracy of 73.01% and 73.0% on the development set and test set of the challenging BIRD Text-to-SQL dataset which outperforms all of the published and undisclosed methods on this benchmark, by a large margin. '}, {'heading': '2 Related Work', 'content': ' Early Text-to-SQL methods predominantly utilized sequence-to-sequence architectures, encoding user queries and database schemas using models such as Graph Neural Networks (GNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and pre-trained transformer encoders (Cai et al., 2021; Cao et al., 2021; Hwang et al., 2019). On the decoding side, these systems employed either slot-filling or auto-regressive modelling approaches to construct the final SQL queries from the encoded inputs (Choi et al., 2021; Wang et al., 2019). Additionally, tabular language models like TaBERT (Yin et al., 2020), 2 TaPas (Herzig et al., 2020), and Grappa (Yu et al., 2020) have been developed to encode both tables and textual data effectively. However, the landscape has evolved with the widespread use of LLMs, which have largely replaced earlier methods with their superior performance (Katsogiannis-Meimarakis & Koutrika, 2023; Quamar et al., 2022). Initially, efforts concentrated on optimizing prompt designs for these LLMs (Dong et al., 2023; Gao et al., 2023; Pourreza & Rafiei, 2024a). Subsequent advancements have introduced more complex methodologies, including schema linking (Li et al., 2024b; Pourreza & Rafiei, 2024a,b; Talaei et al., 2024), self-correction or self-debugging (Chen et al., 2023; Talaei et al., 2024; Wang et al., 2023), and self-consistency techniques (Lee et al., 2024; Maamari et al., 2024; Sun et al., 2023; Talaei et al., 2024), further enhancing the performance by proposing complex LLM-based pipelines. '}, {'heading': '3 Methods', 'content': ' '}, {'heading': '3.1 Overall Framework', 'content': ' This section outlines the proposed CHASE-SQL framework, which consists of four primary components: 1) Value retrieval, 2) Candidate generator, 3) Query fixer, and 4) Selection agent. As illustrated in Fig. 1. The proposed framework begins by retrieving relevant database values. Subsequently, all contextual information, including retrieved values, database metadata, and schema, is provided to an LLM to generate candidate queries. These candidate queries then undergo a fixing loop, and finally, all candidates are compared in a pairwise way using the trained selection agent to pick the correct answer. The following sections delve into the details of each component. Figure 1: Overview of the proposed CHASE-SQL framework for Text-to-SQL, with value retrieval and using a selection agent for improve picking of the answers among the generated candidates along with a fixer to provide feedback for refinement of the outputs. '}, {'heading': '3.2 Value Retrieval', 'content': ' Databases might contain very high number of rows, with often only a few being relevant to a query. Retrieving relevant values is crucial as they can be used in various SQL clauses like WHERE and HAVING. Similar to the approach in (Talaei et al., 2024), we begin by extracting keywords from the given question using an LLM prompted with few-shot examples. For each keyword, we employ locality-sensitive hashing (LSH) (Datar et al., 2004) to retrieve the most syntactically-similar words, and re-rank them based on embedding-based similarity and edit distance. This approach is robust to typos in the question and considers keyword semantics during retrieval. '}, {'heading': '3.3 Multi-path Candidate Generation', 'content': ' As shown in Table 1, relying solely on consistency among responses can lead to sub-optimal performance. Therefore, we prioritize diversity in generation of multiple response candidates to increase the likelihood of generating at least one correct answer. Among the diverse responses generated by the candidate generators, we select one as the final response using a selection agent that compares candidates pairwise. To generate 3 diverse responses, we increase the next token sampling temperature, and also shuffle the order of columns and tables in the prompt. Chain-of-Thought (CoT) prompting (Wei et al., 2022) has been proposed to enhance LLMs reasoning abilities by conditioning their final responses on a step-by-step chain of reasoning. Most CoT prompting approaches rely on few-shot examples in the prompt to guide LLMs on thinking step-by-step, following the format M = (qi, ri, si), where qi is the example question, ri is the reasoning path, and si is the ground truth SQL query for qi. We employ two distinct reasoning methods and an online synthetic example generation approach. As shown in Fig. 3a, different generators can yield different outputs, indicating their effectiveness for specific questions and databases. '}, {'heading': 'Divide and Conquer CoT', 'content': ': Divide-and-conquer perspective brings breaking down complex problems into smaller sub-problems, solving each individually, and then combining the solutions to obtain the final answer. Along these lines, we propose a CoT prompting approach that first decomposes the given question into smaller sub-problems using pseudo-SQL queries. In the conquer step, the solutions to these sub-problems are aggregated to construct the final answer. Finally, an optimization step is applied to the constructed query to remove redundant clauses and conditions. This approach is particularly powerful handling complex scenarios that involve nested queries, e.g. intricate WHERE or HAVING conditions, and queries requiring advanced mathematical operations. In Appendix Fig. 17, we exemplify a question and its corresponding SQL query that was successfully solved using this generator, a scenario the other methods considered in this paper could not address due to the querys complex conditions and SQL clauses. For a more detailed view of the divide-and-conquer prompt, please see Appendix Fig. 16. Additionally, Alg. 1 outlines the step-by-step process of this strategy to generate the final SQL output using a single LLM call. Algorithm 1 Divide and Conquer Chain-of-Thought (CoT) Strategy for Text-to-SQL. Input: Set of human-annotated few-shot examples M, user question Qu, target database D associated with the question, and a large language model (LLM) . Divide: 1: Sq (M, D, Qu) // Decompose the original question Qu into a set of sub-questions Sq 2: Ssql // Initialize an empty set Ssql to store partial SQL queries for each sub-question Conquer: 3: for each sub-question qi in Sq do 4: // Generate a partial SQL query for each sub-question qi 5: Ssql Ssql {(M, D, Qu, q1, ..., qi, sql1, ..., sqli1)} 6: end for Assemble: 7: Sf (M, D, Qu, Sq, Ssql) // Assemble the final SQL query Sf from all sub-queries in Ssql 8: return Sf '}, {'heading': 'Query Plan CoT', 'content': ': A query (execution) plan is a sequence of steps that the database engine follows to access or modify the data described by a SQL command. When a SQL query is executed, the database management systems query optimizers translate the SQL text into a query plan that the database engine can execute. This plan outlines how tables are accessed, how they are joined, and the specific operations performed on the data (see Appendix Fig. 19 as an example). Inspired by the step-by-step process that database engines use to execute SQL queries, we propose a reasoning strategy to construct the final SQL output. Query plans for any given SQL query can be obtained using the EXPLAIN\" command, which provides a detailed breakdown of execution steps. However, this output is often presented in a format that is difficult to interpret by LLMs (e.g. in SQLite). To address this, we convert the output of EXPLAIN\" command into a human-readable text format that aligns more closely with the pretraining data of LLMs. The human-readable version of query plans consists of three key steps: (1) identifying and locating the relevant tables for the question, (2) performing operations such as counting, filtering, or matching between tables, and (3) delivering the final result by selecting the appropriate columns to return. This reasoning method complements the divide-and-conquer CoT strategy. While the divide-and-conquer approach is better suited for decomposing complex questions, the query plan approach excels when questions require more reasoning over the relationships between different parts of the question and the database schema. It systematically explains which tables to scan, how to match columns, and how to apply filters. Appendix Fig. 20 shows an example of a question that was answered correctly only by this method. Appendix Fig. 18 provides the prompt used for this reasoning strategy. '}, {'heading': '4 Online Synthetic Example Generation', 'content': ': Using M demonstrations for few-shot in-context learning has shown promising results on various related tasks (Pourreza & Rafiei, 2024a). Besides helping with specifying the task and illustrate the step-by-step process deriving the output, demonstrations constructed using relevant tables and columns can also help the model understand the underlying data schema. Based on this insight, we propose a synthetic demonstration generation strategy for Text-to-SQL  given the user question Qu, the target database D, and the selected columns ti (using a column selection approach similar to (Talaei et al., 2024)). Algorithm 2 Online Synthetic example generation strategy for Text-to-SQL. Input: User question Qu, additional user hint Hu, target database D and filtered relevant table columns t associated with the question, LLM , guidelines Rf for generating examples by SQL features, guidelines Rt for generating examples with filtered schema, and the numbers of examples to generate nf, nt respectively 1: P // {(qi, si) | qi, si }, where qi is input question , si is output SQL for the i-th example 2: P P {(D, Rf, nf)} // Generate n examples with entire database by common SQL features 3: P P {(t, Rt, nt)} // Generate examples with filtered columns to highlight correct schema usage 4: return P Algorithm 2 outlines the online synthetic example generation approach with two LLM generation steps. The first step focuses on generating illustrative examples with common SQL features described in the guideline Rf. The SQL features include equality and non-equality predicates, single table and multi-table JOIN, nested JOIN, ORDER BY and LIMIT, GROUP BY and HAVING, various aggregation functions. These are widely applicable SQL clauses and functions  the generated example SQL queries, incorporating these features, follow the BIRD SQL feature distribution (Appendix Fig 23a). The second step focuses on generating examples highlighting correct interpretation of the underlying data schema  the model  is asked to generate examples using ti and that are similar to the examples outlined in Rt. Appendix A.10 provides the prompts used for the example generation). While a relevant example (e.g. showing a nested JOIN query with multiple tables) can be helpful for questions that require complex JOIN queries, it might also mislead the LLM for overuse (e.g. when a simple single table query is sufficient). This and the inherent ambiguity of natural language query qi, for which we draw the examples by relevance, make the example selection challenging. Thus, we generate and inject the examples to the prompt online per qi. We ask the LLM to generate many input-output pairs for in-context learning. The final set of synthetic examples for qi contains examples generated with both Rf and Rt. This ensures that the example set is diverse both in SQL features/clauses and the choice of relevant tables/columns used. The diversity of the example set is desirable to avoid over-fitting the output to certain patterns (e.g., the model always writes a SQL with JOIN if shown mostly JOIN examples). Mixing various examples for various SQL features and database tables with and without column filtering is observed to result in better generation quality overall (please see Appendix Table 8). '}, {'heading': '3.4 Query Fixer', 'content': ' In some cases, LLMs might generate queries that are syntactically incorrect. These queries are clear candidates for correction, as they fail to provide the correct answers. To address this, we apply an LLM-based query fixer that leverages the self-reflection (Shinn et al., 2024) method. The fixer reflects on the previously generated query, using feedback such as syntax error details or empty result sets to guide the correction process. We continue this iterative fixing approach up to a specified number of attempts,  (set to three in this paper). Appendix Fig. 21 demonstrates the prompt used for this query fixing step. '}, {'heading': '3.5 Selection Agent', 'content': ' With three different methods for generating SQL queries, we can generate a set of candidate queries for any given question. The key challenge in this step is selecting the correct SQL query from this pool of candidates. A naive approach would be to measure consistency among the candidates by executing them, grouping them based on their execution results, and selecting a query from the largest group as the most likely correct answer. However, this would assume that the most consistent answer is always the best one, which is not always the case. Instead, we propose a more refined picking strategy, Algorithm 3, that relies on a selection agent. Given a set of candidates SQL queries C = {c1, c2, ..., cn}, the final responses are selected by finding 5 the candidate that has the highest score assigned by the selection model. This model p can take k candidates and rank them based on how accurately each of them answers the given question. Concretely, we formulate the selection of the final response as: cf = arg max cC    (n k) X i=1 p(ci1, . . . , cik | Qu, Hu, D)   , (1) where Qu refers to the users question, Hu is the provided hint, and D is the target database from which the question is being asked. In Eq. 1, we pass k candidates to the selection model to be ranked, with k being between 1 and n. In the extreme case of k = 1, the model is unable to make comparisons between candidates, which complicates the evaluation process for the model. As k increases, comparing more candidates makes the process more challenging for the model, as it needs to consider different aspects simultaneously. Consequently, we set k = 2 and train a model with a classification objective to compare only two candidates at a time. Having a set of high-quality and diverse candidates, the most straightforward solution is to employ off-the-shelf LLMs to make pairwise selections. However, experiments with Gemini-1.5-pro showed that using the LLM without fine-tuning resulted in only 58.01% binary classification accuracy. This is primarily due to the candidates being very similar to one another, requiring a fine-tuned model to learn the nuances and make more accurate decisions. To train the selection agent, we first generate candidate SQL queries on the training set (of Text-to-SQL benchmarks), and group them into clusters based on their execution results. For cases where at least one cluster contains correct queries and others contains incorrect ones, we create training examples in the form of tuples (Qu, Ci, Cj, Dij, yij), where Qu is the users question, Ci and Cj are the two candidate queries being compared, Dij is the database schema used by both candidates, and yij 0, 1 is the label indicating whether Ci or Cj is the correct query. To avoid order bias during training, we randomly shuffle the order of correct and incorrect queries in each pair. Since the number of cases with both correct and incorrect candidates is limited, for instances where no correct candidate exists, we include the ground truth SQL query in the prompt as a hint to guide the model in generating correct candidates. Algorithm 3 Picking the final SQL query from a pool of candidates. Input: Set of candidate SQL queries C = {c1, c2, ..., cn}, user question Qu, hint Hu, target database D, and a selection model p, er(ci, D) as the execution result of ci on D 1: ri 0 for all ci C // Initialize the score ri for each candidate query to zero 2: for each distinct pair (ci, cj) where i = j do 3: if er(ci, D) = er(cj, D) then 4: w i // ci is the winner if the execution results match 5: else 6: Si,j schema_union(ci, cj, D) // Construct union of schemas used in ci and cj 7: w p(Si,j, Qu, Hu, ci, cj)w {i, j} // Use binary classifier p to select the winner, w {i, j} 8: end if 9: rw rw + 1 // Increase the score of the winner cw by 1 10: end for 11: cf arg maxciC ri // Select the candidate with the highest score as the final SQL query cf 12: return cf In the pseudo-code for Algorithm 3, we begin by initializing a score of zero for each candidate query. Then, for every distinct pair of queries (ci, cj), we compare both (ci, cj) and (cj, ci) to mitigate any order bias, ensuring that both candidates in a pair are fairly evaluated. If both queries produce the same execution result on the database, we mark one as the winner and increment its score, as these results suggest consistency. If the execution results differ, we generate a union of the schema used by both queries and use the binary classifier to determine which query is more likely to be correct. The classifier takes into account the question, the two candidate queries, and the combined schema to make its decision. The winners score is then updated accordingly. After all comparisons, the candidate with the highest score is selected as the final query. In the rare case of a tie in the final scores, we break the tie by selecting one of the candidates arbitrarily. 6 '}, {'heading': '4 Experiments', 'content': ' '}, {'heading': '4.1 Datasets and Models', 'content': ' We evaluate the performance of the proposed CHASE-SQL framework on two widely-recognized cross-domain datasets: BIRD (Li et al., 2024c) and Spider (Yu et al., 2018). BIRD contains over 12,751 unique question-SQL pairs from 95 large databases, spanning more than 37 professional domains, with databases designed to resemble real-world scenarios, featuring messy data rows and complex schemas. Spider consists of 10,181 questions and 5,693 unique complex SQL queries across 200 databases, covering 138 domains. The Spider dataset is divided into non-overlapping training, development, and test sets similar to BIRD. For both, we use execution accuracy (EX), the official metric for their respective leaderboard, as the primary evaluation metric to compare methods. Details of the models and their hyperparameters are provided in Appendix section A.2. '}, {'heading': '4.2 BIRD results', 'content': ' We present the end-to-end Text-to-SQL performance of the proposed CHASE-SQL framework using Claude- 3.5-sonnet and Gemini 1.5 pro on the BIRD development set, and Gemini 1.5 pro on the BIRD test set. We compare with both published methods (either with an available codebase and/or paper) and undisclosed methods. For a fair comparison with Gemini 1.5 pro, all LLM calls in the Claude-3.5-sonnet setting, except for the selection model, are made using Claude-3.5-sonnet (previously-trained selection model is reused). As shown in Table 2, CHASE-SQL with Gemini 1.5 pro achieves 73.01% accuracy on the BIRD development set and 73.0% on the BIRD holdout test set, outperforming all previous works and setting a new state-of-the-art performance. Table 2: Performance Comparison of different Text-to-SQL methods on BIRD benchmark. Method EX (Dev) EX (Test) Published CHASE-SQL + Gemini 1.5 (Ours) 73.01 73.0 CHASE-SQL + Claude 3.5 Sonnet (Ours) 69.53  Distillery + GPT-4o (Maamari et al., 2024) 67.21 71.83 CHESS (Talaei et al., 2024) 65.00 66.69 MCS-SQL + GPT-4 (Lee et al., 2024) 63.36 65.45 SuperSQL (Li et al., 2024a) 58.5 62.66 Undisclosed Insights AI 72.16 70.26 AskData + GPT-4o 72.03 72.39 OpenSearch-v2 + GPT-4o 69.3 72.28 PURPLE-RED + GPT-4o 68.12 70.21 Arcwise + GPT-4o 67.99 66.21 ExSL + granite-34b-code 67.47 67.76 Table 3: Performance Comparison of different Text-to-SQL methods on Spider test set. Method EX Training with Spider MCS-SQL + GPT-4 (Lee et al., 2024) 89.6  CHASE-SQL + Gemini 1.5 (Ours) 87.6  CHESS (Talaei et al., 2024) 87.2  DAIL-SQL + GPT-4 (Gao et al., 2023) 86.6  DIN-SQL + GPT-4 (Pourreza & Rafiei, 2024a) 85.3  C3 + ChatGPT (Dong et al., 2023) 82.3  RESDSQL 3B (Li et al., 2023a) 79.9  DIN-SQL + CodeX (Pourreza & Rafiei, 2024a) 78.2  T5-3B+NatSQL (Rai et al., 2023) 78.0  Graphix-3B+PICARD (Li et al., 2023b) 77.6  '}, {'heading': '4.3 Spider results', 'content': ' We assess the generalizability of the proposed CHASE-SQL by evaluating it in an end-to-end way on the Spider test set without modifying the few-shot samples in the prompts or training a new selection model, i.e. without using and data from the target distribution. This approach allows us to test the performance of CHASE-SQL on different unseen query and database distributions compared to the data from training distributions. Table 3 demonstrates that CHASE-SQL achieves an execution accuracy of 87.6% on the Spider test set, placing it second among methods that have undergone specific training or prompt optimization for the Spider dataset. This highlights the strong generalizability of CHASE-SQL and its potential for generating high quality Text-to-SQL for unseen samples coming from very different distributions and unique challenges. 7 '}, {'heading': '4.4 Generator and selection performance', 'content': ' Table 4: Ablation studies on single candidate gen- eration performance of the candidate generators compared with original BIRD prompt + zero-shot CoT with Gemini 1.5 pro on the BIRD dev set. Selector is our final performance applying the se- lection agent to 21 candidates generated from all generators. Method Execution Accuracy (%) (%) Baseline 57.75 - QP CoT 63.62 +5.87 DC CoT 63.92 +6.17 OS CoT 67.09 +9.34 Baseline w Query Fixer 61.58 +3.83 QP CoT w Query Fixer 65.51 +7.76 DC CoT w Query Fixer 65.77 +8.02 OS CoT w Query Fixer 68.02 +10.27 Generator + Fixer: To reveal performance of genera- tors, we conducted an ablation study to evaluate the per- formance of each candidate generation method before and after applying the query fixer. We compare the performance of the proposed generators in producing a single candidate query against the original BIRD prompt (Li et al., 2024c), 67.47 67.76 Table 3: Performance Comparison of different Text-to-SQL methods on Spider test set. Method EX Training with Spider MCS-SQL + GPT-4 (Lee et al., 2024) 89.6  CHASE-SQL + Gemini 1.5 (Ours) 87.6  CHESS (Talaei et al., 2024) 87.2  DAIL-SQL + GPT-4 (Gao et al., 2023) 86.6  DIN-SQL + GPT-4 (Pourreza & Rafiei, 2024a) 85.3  C3 + ChatGPT (Dong et al., 2023) 82.3  RESDSQL 3B (Li et al., 2023a) 79.9  DIN-SQL + CodeX (Pourreza & Rafiei, 2024a) 78.2  T5-3B+NatSQL (Rai et al., 2023) 78.0  Graphix-3B+PICARD (Li et al., 2023b) 77.6  4.3 Spider results We assess the generalizability of the proposed CHASE-SQL by evaluating it in an end-to-end way on the Spider test set without modifying the few-shot samples in the prompts or training a new selection model, i.e. without using and data from the target distribution. This approach allows us to test the performance of CHASE-SQL on different unseen query and database distributions compared to the data from training distributions. Table 3 demonstrates that CHASE-SQL achieves an execution accuracy of 87.6% on the Spider test set, placing it second among methods that have undergone specific training or prompt optimization for the Spider dataset. This highlights the strong generalizability of CHASE-SQL and its potential for generating high quality Text-to-SQL for unseen samples coming from very different distributions and unique challenges. 7 4.4 Generator and selection performance Table 4: Ablation studies on single candidate gen- eration performance of the candidate generators compared with original BIRD prompt + zero-shot CoT with Gemini 1.5 pro on the BIRD dev set. Selector is our final performance applying the se- lection agent to 21 candidates generated from all generators. Method Execution Accuracy (%) (%) Baseline 57.75 - QP CoT 63.62 +5.87 DC CoT 63.92 +6.17 OS CoT 67.09 +9.34 Baseline w Query Fixer 61.58 +3.83 QP CoT w Query Fixer 65.51 +7.76 DC CoT w Query Fixer 65.77 +8.02 OS CoT w Query Fixer 68.02 +10.27 Generator + Fixer: To reveal performance of genera- tors, we conducted an ablation study to evaluate the per- formance of each candidate generation method before and after applying the query fixer. We compare the performance of the proposed generators in producing a single candidate query against the original BIRD prompt (Li et al., 2024c), augmented with zero-shot CoT reasoning (Kojima et al., 2022), which serves as the baseline for assessing the quality of prompts. The results, shown in Table 4, indicate that the proposed methods significantly improve SQL generation performance, compared to the naive baseline, towards the goal of producing high-quality candidates while maintain- ing diversity. Among the candidate generators, the online synthetic data generation approach produced an impressive performance of 68.02%, demonstrating its effectiveness in leveraging test-time compute to improve LLM performance by generating high-quality synthetic examples. Furthermore, the query fixer proved crucial, enhancing the quality of the candidate pool and increasing performance by nearly 2% across all candidate generators. Table 5: Evaluating the binary selection accuracy of the different selection models. '}, {'heading': 'Selection', 'content': ' Model Binary Acc. (%) Claude-3.5-sonnet 60.21 Gemini-1.5-pro 63.98 Tuned Gemma 2 9B 64.28 Tuned Gemini-1.5-flash 71.01 Selection: We conducted an analysis on the binary selection accu- racy of the selection agent for cases where, in a pairwise comparison, one candidate is correct and the other is incorrect. We exclude cases where both candidates are either correct or incorrect, as the selection would not affect the outcome since both candidates have the same label. We compare the performance of Claude-3.5-sonnet and Gemini-1.5-pro (both out-of-the-box without fine-tuning) with two fine-tuned models: 1) Gemma 2 9B and 2) Gemini-1.5-flash. As shown in Table 5, both fine-tuned models achieve higher accuracy than the untuned counterparts, demonstrating the importance of fine-tuning to teach the model about the specific preferences. '}, {'heading': 'Candidate Generation Analysis', 'content': ': We analyze the performance of each candidate generator method individually. To better understand the performance potential when effectively selecting the correct SQL query from the candidate pool, we generate seven candidate SQL queries from each generator method (21 candidates in total) for all samples in the BIRD development set. We determine this number of candidates based on the observation that increasing the candidate pool beyond 20 did not yield significant improvements, as illustrated in Fig. 2d. By assuming access to an oracle selection model that always selects the correct SQL query from the seven candidates, we calculate the upper-bound performance achievable for each generator. Conversely, by assuming an adversarial selection model that always selects the wrong SQL query, we determine the lower-bound performance. Fig. 2 illustrates the upper-bound and lower-bound performance for all three methods together with the performance of our selection agent. As shown, the upper-bound performance of the two different CoT methods is generally higher than that of the synthetic example generation method for different number of candidates. However, their lower-bound performance is also lower than the synthetic method. Lower-bound accuracy reflects cases where all candidates are correct, reducing the noise in the selection process since it doesnt matter which candidate is chosen, so a higher lower-bound is preferred. This is evident in the selection agents performance, where a drop in the lower bound leads to diminishing returns from increasing the upper bound, causing the selection agents performance to plateau. Additionally, the upper-bound performance of combining all three methods reaches 82.79%, highlighting the significant room for improvement through better candidate picking methods. This demonstrates that the LLMs parametric knowledge already contains the information needed to solve most questions, highlighting the need for ensemble approaches to effectively extract and utilize this knowledge. 8 (a) Upper-bound and lower-bound Accuracy for Divide and Conquer CoT (b) Upper-bound and lower-bound Accuracy for Online Synthetic Example (c) Upper-bound and lower-bound performance for Query Plan CoT. (d) Upper-bound performance of all three can- didate generators across different difficulty cat- egories. Figure 2: Comparison of the upper- and lower-bound performance of different candidate generators. Additionally, we evaluate the upper-bound performance by combining all candidates from three candidate generation methods across the simple, moderate, and challenging difficulty levels for the BIRD development set. These difficulty categories are assigned by human experts during the creation of the BIRD development set. Fig. 2d shows that, as expected, the upper-bound performance increases with the number of candidates across all difficulty levels. However, for the challenging and moderate classes, the improvement plateaus earlier than in the simple class, suggesting that generating more samples does not further improve the upper-bound performance for these two difficulty levels. Fig. 2 presents a Venn diagram showcasing the performance of three generation methods: Query Plan, Divide and Conquer, and with Synthetic Examples. The numbers within the intersecting regions represent the instances where multiple methods generated at least one correct candidate. This diagram visually highlights the unique contributions of each method, which indicates the necessity of using all three generators. Additionally, in Fig. 3b, we compare the number of correct queries generated by each SQL generation method that are not correct by the other generators. The divide-and-conquer approach outperforms the others on challenging questions, while the query plan method excels on moderately difficult queries. To further analyze the performance of the generators across different domains and varying numbers of columns and tables, we compare the number of correct queries generated for each database, as shown in Appendix Fig. 4. As illustrated, both CoT methods generally perform similarly across databases, while the online synthetic example generation method significantly increases diversity, resulting in more correct answers overall across different databases. '}, {'heading': 'Selection Agent Analysis', 'content': ': We evaluate the query-picking performance by comparing the Text-to-SQL execution accuracy of the selection agent with the self-consistency method (using majority voting) Wang et al. (2022), an oracle model (upper bound), and an adversarial model (lower bound). To conduct the evaluation, we generate 10 samples from each candidate generation method using two different sampling temperatures: 0.5 and 1.8. The results, shown in Table 6, demonstrate that the selection agent significantly outperforms the self-consistency method with a large margin, roughly 6%. As expected, increasing the sampling temperature 9 35 30 33 38 72 23 1045 '}, {'heading': 'Query Plan', 'content': ' '}, {'heading': 'Synthetic Example', 'content': ' '}, {'heading': 'Divide and Conquer', 'content': ' '}, {'heading': 'Unsolved Questions: 258', 'content': ' (a) Venn diagram illustrating the num- ber of instances for which each method: Query Plan, Synthetic Example, Divide and Conquer, produces at least one cor- rect candidate. The overlap regions repre- sent multiple methods generating correct candidates. (b) Number of correct queries across different complexity levels that were answered by each method. Figure 3: Comparison of SQL generation methods: Venn diagram showing unique and overlapping correct answers (left) and the performance across different complexity levels (right). raises the upper bound but also lowers the lower bound. This effect is more pronounced for the synthetic data generation method compared to the two CoT methods, mainly because LLMs generate reasoning steps before producing the final SQL query, which helps mitigate the randomness introduced by high-temperature sampling. The performance with self-consistency method generally decreases as temperature increases, since the majority cluster becomes smaller with more random queries. However, the proposed trained selection agent is less affected by temperature scaling and, in two cases, even improved its performance with a more diverse pool of samples. Table 6: Performance comparison of different picking methods on the candidates generated by the candidate generators on BIRD development set with two different temperatures. QP refers to query plan COT, DC refers to divide and conquer COT, and OS is the online synthetic example generation method. Picking Method QP (T=0.5) QP (T=1.8) DC (T=0.5) DC (T=1.8) OS (T=0.5) OS (T=1.8) Lower Bound 50.46 48.63 51.37 47.39 60.43 50.98 Upper Bound 78.55 80.44 78.42 79.34 74.77 79.66 Self-consistency 65.78 65.51 66.43 64.41 67.34 66.88 Our Selection Agent 71.7 71.73 71.31 70.53 70.4 71.38 '}, {'heading': '4.5 Ablation Studies', 'content': ' In the previous sections, we evaluate the importance of the selection agent and each candidate generation method. Next, we focus on the analysis of the remaining components of CHASE-SQL: LSH for value retrieval, the query fixer, and three reasoning strategies (QP, OS, and DC). Table 7 shows the performance of CHASE- SQL without each of these steps, highlighting their significance in achieving higher-quality performance. The results demonstrate the contribution of each component, where removing LSH, the query fixer, or any of the candidate generators leads to a reduction in execution accuracy, further validating the importance of these components of CHASE-SQL. Moreover, the table compares the performance of our binary selection agent with two other selection methods: self-consistency (Wang et al., 2022) and a ranker agent. The ranker agent receives all candidates generated by our three candidate generators in a single prompt, compares them, and produce a ranking for each. For the ranker agent, we select the query with the lowest rank as the best answer. The binary selection agent significantly outperforms both the self-consistency and ranker agents, demonstrating the effectiveness of the proposed method. 10 '}, {'heading': '5 Conclusion', 'content': ' Table 7: Ablation studies on the performance of CHASE- SQL after removing the query fixer, LSH for value re- trieval, and reasoning strategies, i.e., QP, OS, and DC. Method Execution Accuracy (%) (%) CHASE-SQL All 73.01 - CHASE-SQL w self-consistency 68.84 -4.17 CHASE-SQL w ranker agent 65.51 -7.5 CHASE-SQL w/o LSH 70.09 -2.92 CHASE-SQL w/o Query Fixer 69.23 -3.78 CHASE-SQL w/o QP 72.36 -0.65 CHASE-SQL w/o OS 72.16 -0.85 CHASE-SQL w/o DC 71.77 -1.24 We introduce a novel agentic framework, CHASE-SQL, to leverage test-time compute for generating diverse, high-quality SQL queries and accurately selecting the correct one. We propose multiple chain-of-thought prompting methods and an online synthetic example generation technique, along with a query selection mechanism that scores candidates based on pairwise comparisons. Our framework, CHASE-SQL, sets a new state-of-the-art in the notable public Text-to-SQL leaderboard (at the time of the submission), demon- strating the effectiveness of test-time computation for both generating diverse queries and selecting the most accurate response. CHASE-SQL addresses key issues like query diversity and selection optimization, paving the way for further improvements in complex reasoning tasks encountered at real-world Text-to-SQL challenges. '}, {'heading': 'Acknowledgments', 'content': ' We would like to thank Per Jacobsson, Raj Sinha, Zeke Miller, Reza Sherkat, James Su, Zhixian Yan, David Culler, and Xiance Si, for their valuable comments and feedbacks on our paper. We would also like to thank the BIRD team for their invaluable assistance with the evaluation of the BIRD test set. '}, {'heading': 'References', 'content': ' Ion Androutsopoulos, Graeme D Ritchie, and Peter Thanisch. Natural language interfaces to databasesan introduction. Natural language engineering, 1(1):2981, 1995. Ruichu Cai, Jinjie Yuan, Boyan Xu, and Zhifeng Hao. Sadga: Structure-aware dual graph aggregation network for text-to-sql. Advances in Neural Information Processing Systems, 34:76647676, 2021. Ruisheng Cao, Lu Chen, Zhi Chen, Yanbin Zhao, Su Zhu, and Kai Yu. Lgesql: line graph enhanced text-to-sql model with mixed local and non-local relations. arXiv preprint arXiv:2106.01093, 2021. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Xinyun Chen, Maxwell Lin, Nathanael Schrli, and Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023. DongHyun Choi, Myeong Cheol Shin, EungGyun Kim, and Dong Ryeol Shin. Ryansql: Recursively applying sketch-based slot fillings for complex text-to-sql in cross-domain databases. Computational Linguistics, 47 (2):309332, 2021. Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S Mirrokni. Locality-sensitive hashing scheme based on p-stable distributions. In Proceedings of the twentieth annual symposium on Computational geometry, pp. 253262, 2004. Xuemei Dong, Chao Zhang, Yuhang Ge, Yuren Mao, Yunjun Gao, Jinshu Lin, Dongfang Lou, et al. C3: Zero-shot text-to-sql with chatgpt. arXiv preprint arXiv:2307.07306, 2023. Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, and Jingren Zhou. Text-to-sql empowered by large language models: A benchmark evaluation. arXiv preprint arXiv:2308.15363, 2023. Jonathan Herzig, Pawe Krzysztof Nowak, Thomas Mller, Francesco Piccinno, and Julian Martin Eisenschlos. Tapas: Weakly supervised table parsing via pre-training. arXiv preprint arXiv:2004.02349, 2020. 11 Wonseok Hwang, Jinyeong Yim, Seunghyun Park, and Minjoon Seo. A comprehensive exploration on wikisql with table-aware word contextualization. arXiv preprint arXiv:1902.01069, 2019. George Katsogiannis-Meimarakis and Georgia Koutrika. A survey on deep learning approaches for text-to-sql. The VLDB Journal, 32(4):905936, 2023. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. Dongjun Lee, Choongwon Park, Jaehyuk Kim, and Heesoo Park. Mcs-sql: Leveraging multiple prompts and multiple-choice selection for text-to-sql generation. arXiv preprint arXiv:2405.07467, 2024. Boyan Li, Yuyu Luo, Chengliang Chai, Guoliang Li, and Nan Tang. The dawn of natural language to sql: Are we fully ready? arXiv preprint arXiv:2406.01265, 2024a. Fei Li and Hosagrahar V Jagadish. Constructing an interactive natural language interface for relational databases. Proceedings of the VLDB Endowment, 8(1):7384, 2014. Haoyang Li, Jing Zhang, Cuiping Li, and Hong Chen. Resdsql: Decoupling schema linking and skeleton parsing for text-to-sql. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 1306713075, 2023a. Haoyang Li, Jing Zhang, Hanbing Liu, Ju Fan, Xiaokang Zhang, Jun Zhu, Renjie Wei, Hongyan Pan, Cuiping Li, and Hong Chen. Codes: Towards building open-source language models for text-to-sql. Proceedings of the ACM on Management of Data, 2(3):128, 2024b. Jinyang Li, Binyuan Hui, Reynold Cheng, Bowen Qin, Chenhao Ma, Nan Huo, Fei Huang, Wenyu Du, Luo Si, and Yongbin Li. Graphix-t5: Mixing pre-trained transformers with graph-aware layers for text-to-sql parsing. arXiv preprint arXiv:2301.07507, 2023b. Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, et al. Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls. Advances in Neural Information Processing Systems, 36, 2024c. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rmi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):10921097, 2022. Karime Maamari, Fadhil Abubaker, Daniel Jaroslawicz, and Amine Mhedhbi. The death of schema linking? text-to-sql in the age of well-reasoned language models. arXiv preprint arXiv:2408.07702, 2024. Ansong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-tau Yih, Sida Wang, and Xi Victoria Lin. Lever: Learning to verify language-to-code generation with execution. In International Conference on Machine Learning, pp. 2610626128. PMLR, 2023. Rubn Prez-Mercado, Antonio Balderas, Andrs Muoz, Juan Francisco Cabrera, Manuel Palomo-Duarte, and Juan Manuel Dodero. Chatbotsql: Conversational agent to support relational database query language learning. SoftwareX, 22:101346, 2023. Mohammadreza Pourreza and Davood Rafiei. Din-sql: Decomposed in-context learning of text-to-sql with self-correction. Advances in Neural Information Processing Systems, 36, 2024a. Mohammadreza Pourreza and Davood Rafiei. Dts-sql: Decomposed text-to-sql with small large language models. arXiv preprint arXiv:2402.01117, 2024b. Mohammadreza Pourreza, Ruoxi Sun, Hailong Li, Lesly Miculicich, Tomas Pfister, and Sercan O Arik. Sql-gen: Bridging the dialect gap for text-to-sql via synthetic data and model merging. arXiv preprint arXiv:2408.12733, 2024. 12 Abdul Quamar, Vasilis Efthymiou, Chuan Lei, and Fatma zcan. Natural language interfaces to data. Found. Trends Databases, 11(4):319414, 2022. doi: 10.1561/1900000078. URL https://doi.org/10. 1561/1900000078.'}]\n",
      "{\n",
      "    \"abstract\": {\n",
      "        \"content\": \"In tackling the challenges of large language model (LLM) performance for Text-to-SQL tasks, we introduce CHASE-SQL, a new framework that employs innovative strategies, using test-time compute in multi-agent modeling to improve candidate generation and selection. CHASE-SQL leverages LLMs intrinsic knowledge to generate diverse and high-quality SQL candidates using different LLM generators with: (1) a divide-and-conquer method that decomposes complex queries into manageable sub-queries in a single LLM call; (2) chain-of-thought reasoning based on query execution plans, reflecting the steps a database engine takes during execution; and (3) a unique instance-aware synthetic example generation technique, which offers specific few-shot demonstrations tailored to test questions. To identify the best candidate, a selection agent is employed to rank the candidates through pairwise comparisons with a fine-tuned binary-candidates selection LLM. This selection approach has been demonstrated to be more robust over alternatives. The proposed generators-selector framework not only enhances the quality and diversity of SQL queries but also outperforms previous methods. Overall, our proposed CHASE-SQL achieves the state-of-the-art execution accuracy of 73.0 % and 73.01% on the test set and development set of the notable BIRD Text-to-SQL dataset benchmark, rendering CHASE-SQL the top submission of the leaderboard (at the time of paper submission).\",\n",
      "        \"sub-headings\": []\n",
      "    },\n",
      "    \"authors\": {\n",
      "        \"content\": [\n",
      "            \"Mohammadreza Pourreza1, Hailong Li1, Ruoxi Sun1, Yeounoh Chung1, Shayan Talaei2, Gaurav Tarlok Kakkar1, Yu Gan1, Amin Saberi2, Fatma zcan1, Sercan . Ark1\",\n",
      "            \"1Google Cloud, Sunnyvale, CA, USA\",\n",
      "            \"2Stanford University, Stanford, CA, USA\",\n",
      "            \"{pourreza, hailongli, ruoxis, yeounoh}@google.com\",\n",
      "            \"{gkakkar, gany, fozcan, soarik}@google.com\",\n",
      "            \"{stalaei, saberi}@stanford.edu\",\n",
      "            \"Equal contribution\"\n",
      "        ],\n",
      "        \"sub-headings\": []\n",
      "    },\n",
      "    \"date_written\": {\n",
      "        \"content\": \"October 4, 2024\",\n",
      "        \"sub-headings\": []\n",
      "    },\n",
      "    \"title\": {\n",
      "        \"content\": \"CHASE-SQL: Multi-Path Reasoning and Preference Optimized Candidate Selection in Text-to-SQL\",\n",
      "        \"sub-headings\": []\n",
      "    },\n",
      "    \"Abstract\": {\n",
      "        \"content\": \" In tackling the challenges of large language model (LLM) performance for Text-to-SQL tasks, we introduce CHASE-SQL, a new framework that employs innovative strategies, using test-time compute in multi-agent modeling to improve candidate generation and selection. CHASE-SQL leverages LLMs intrinsic knowledge to generate diverse and high-quality SQL candidates using different LLM generators with: (1) a divide-and-conquer method that decomposes complex queries into manageable sub-queries in a single LLM call; (2) chain-of-thought reasoning based on query execution plans, reflecting the steps a database engine takes during execution; and (3) a unique instance-aware synthetic example generation technique, which offers specific few-shot demonstrations tailored to test questions. To identify the best candidate, a selection agent is employed to rank the candidates through pairwise comparisons with a fine-tuned binary-candidates selection LLM. This selection approach has been demonstrated to be more robust over alternatives. The proposed generators-selector framework not only enhances the quality and diversity of SQL queries but also outperforms previous methods. Overall, our proposed CHASE-SQL achieves the state-of-the-art execution accuracy of 73.0 % and 73.01% on the test set and development set of the notable BIRD Text-to-SQL dataset benchmark, rendering CHASE-SQL the top submission of the leaderboard (at the time of paper submission). \",\n",
      "        \"sub-headings\": []\n",
      "    },\n",
      "    \"1 Introduction\": {\n",
      "        \"content\": \" Text-to-SQL, as a bridge between human language and machine-readable structured query languages, is crucial for many use cases, converting natural language questions into executable SQL commands (Androutsopoulos et al., 1995; Li & Jagadish, 2014; Li et al., 2024c; Yu et al., 2018; ?). By enabling users to interact with complex database systems without requiring SQL proficiency, Text-to-SQL empowers users to extract valuable insights, perform streamlined data exploration, make informed decisions, generate data-driven reports and mine better features for machine learning (Chen et al., 2023; Prez-Mercado et al., 2023; Pourreza & Rafiei, 2024a; Pourreza et al., 2024; Sun et al., 2023; Wang et al., 2019; Xie et al., 2023). Furthermore, Text-to-SQL systems play a pivotal role in automating data analytics with complex reasoning and powering conversational agents, expanding their applications beyond traditional data retrieval (Sun et al., 2023; Xie et al., 2023). As data continues to grow exponentially, the ability to query databases efficiently without extensive SQL knowledge becomes increasingly vital for a broad range of applications. Text-to-SQL can be considered a specialized form of code generation, with the contextual information potentially including the database schema, its metadata and along with the values. In the broader code generation domain, utilizing LLMs to generate a wide range of diverse candidates and select the best one has proven to be effective (Chen et al., 2021; Li et al., 2022; Ni et al., 2023). However, it is non-obvious what 1 arXiv:2410.01943v1  [cs.LG]  2 Oct 2024 leads to most effective candidate proposal and winner selector mechanisms. A straightforward yet effective approach involves generating candidates using zero-/few-shot or open-ended prompting, followed by selecting the best options utilizing self-consistency (Wang et al., 2022), which entails clustering candidates based on their execution outputs. This approach has demonstrated promising results in several studies (Lee et al., 2024; Maamari et al., 2024; Talaei et al., 2024; Wang et al., 2023). However, a single prompt design might not fully unleash the extensive Text-to-SQL knowledge of LLMs, and self-consistency methods might not be always effective. In fact, as illustrated in Table 1, the most consistent answers would not always be the correct ones, with an upper-bound performance 14% higher than that achieved through self-consistency. This substantial gap highlights the potential for significant improvement by implementing more effective selection methods to identify the best answer from the pool of candidate queries. Table 1: Evaluating single-query gen- eration vs. ensemble methods of self- consistency and the upper bound that can be achieved for Text-to-SQL with Gemini 1.5 Pro on the BIRD dev set. EX stands for execution accuracy. Method EX (%) Single query 63.01 Self-consistency 68.84 (+ 5.84) Upper-bound 82.79 (+ 19.78) Building on the challenges outlined in the previous section, we pro- pose novel approaches to improve LLM performance for Text-to-SQL by leveraging judiciously-designed test-time computations in an agentic framework. As indicated by the upper bound in Table 1, utilizing LLMs intrinsic knowledge offers significant potential for improvement. We propose methods that generate a diverse set of high-quality can- didate responses and apply a selection mechanism to identify the best answer. Achieving both high-quality and diverse candidate responses is critical for the success of scoring-based selection methods. Low diversity limits improvement potential and reduces the difference be- tween self-consistency and scoring-based approaches. While techniques like increasing temperature or reordering prompt contents can boost diversity, they often compromise the quality of the candidates. To address this, we introduce effective candidate generators designed to enhance diversity while maintaining high-quality outputs. Specifically, we propose three distinct candidate generation approaches, each capable of producing high-quality responses. The first is inspired by the divide-and-conquer algorithm, which breaks down complex problems into smaller, manageable parts to handle difficult queries. The second employs a query execution-plan-based chain-of-thought strategy, where the reasoning process mirrors the steps a database engine takes during query execution. Lastly, we introduce a novel online synthetic example generation method, which helps the model better understand the underlying data schema of the test database. These methods, when used independently, can produce highly-accurate SQL outputs. To effectively select the best answer among candidates, we introduce a selection agent, trained with a classification objective, that assigns scores based on pairwise comparisons between candidate queries. With this agent, we construct a comparison matrix for all candidates and select the final response based on the highest cumulative score. By combining these candidate generation methods with the proposed scoring model, we create an ensemble approach that leverages the strengths of each strategy to significantly improve overall performance. We present comprehensive evaluations on the efficacy of proposed methodologies of CHASE-SQL. Our innovative candidate generation approaches demonstrate superior performance compared to traditional generic CoT prompts, illustrating their capability in guiding LLMs through the decomposition of complex problems into manageable intermediate steps. Furthermore, the proposed selection agent significantly outperforms conventional consistency-based methods, contributing to the state-of-the-art results. Specifically, CHASE- SQL reaches an execution accuracy of 73.01% and 73.0% on the development set and test set of the challenging BIRD Text-to-SQL dataset which outperforms all of the published and undisclosed methods on this benchmark, by a large margin. \",\n",
      "        \"sub-headings\": []\n",
      "    },\n",
      "    \"2 Related Work\": {\n",
      "        \"content\": \" Early Text-to-SQL methods predominantly utilized sequence-to-sequence architectures, encoding user queries and database schemas using models such as Graph Neural Networks (GNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and pre-trained transformer encoders (Cai et al., 2021; Cao et al., 2021; Hwang et al., 2019). On the decoding side, these systems employed either slot-filling or auto-regressive modelling approaches to construct the final SQL queries from the encoded inputs (Choi et al., 2021; Wang et al., 2019). Additionally, tabular language models like TaBERT (Yin et al., 2020), 2 TaPas (Herzig et al., 2020), and Grappa (Yu et al., 2020) have been developed to encode both tables and textual data effectively. However, the landscape has evolved with the widespread use of LLMs, which have largely replaced earlier methods with their superior performance (Katsogiannis-Meimarakis & Koutrika, 2023; Quamar et al., 2022). Initially, efforts concentrated on optimizing prompt designs for these LLMs (Dong et al., 2023; Gao et al., 2023; Pourreza & Rafiei, 2024a). Subsequent advancements have introduced more complex methodologies, including schema linking (Li et al., 2024b; Pourreza & Rafiei, 2024a,b; Talaei et al., 2024), self-correction or self-debugging (Chen et al., 2023; Talaei et al., 2024; Wang et al., 2023), and self-consistency techniques (Lee et al., 2024; Maamari et al., 2024; Sun et al., 2023; Talaei et al., 2024), further enhancing the performance by proposing complex LLM-based pipelines. \",\n",
      "        \"sub-headings\": []\n",
      "    },\n",
      "    \"3 Methods\": {\n",
      "        \"content\": \" \",\n",
      "        \"sub-headings\": [\n",
      "            {\n",
      "                \"3.1 Overall Framework\": {\n",
      "                    \"content\": \" This section outlines the proposed CHASE-SQL framework, which consists of four primary components: 1) Value retrieval, 2) Candidate generator, 3) Query fixer, and 4) Selection agent. As illustrated in Fig. 1. The proposed framework begins by retrieving relevant database values. Subsequently, all contextual information, including retrieved values, database metadata, and schema, is provided to an LLM to generate candidate queries. These candidate queries then undergo a fixing loop, and finally, all candidates are compared in a pairwise way using the trained selection agent to pick the correct answer. The following sections delve into the details of each component. Figure 1: Overview of the proposed CHASE-SQL framework for Text-to-SQL, with value retrieval and using a selection agent for improve picking of the answers among the generated candidates along with a fixer to provide feedback for refinement of the outputs. \",\n",
      "                    \"sub-headings\": []\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"3.2 Value Retrieval\": {\n",
      "                    \"content\": \" Databases might contain very high number of rows, with often only a few being relevant to a query. Retrieving relevant values is crucial as they can be used in various SQL clauses like WHERE and HAVING. Similar to the approach in (Talaei et al., 2024), we begin by extracting keywords from the given question using an LLM prompted with few-shot examples. For each keyword, we employ locality-sensitive hashing (LSH) (Datar et al., 2004) to retrieve the most syntactically-similar words, and re-rank them based on embedding-based similarity and edit distance. This approach is robust to typos in the question and considers keyword semantics during retrieval. \",\n",
      "                    \"sub-headings\": []\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"3.3 Multi-path Candidate Generation\": {\n",
      "                    \"content\": \" As shown in Table 1, relying solely on consistency among responses can lead to sub-optimal performance. Therefore, we prioritize diversity in generation of multiple response candidates to increase the likelihood of generating at least one correct answer. Among the diverse responses generated by the candidate generators, we select one as the final response using a selection agent that compares candidates pairwise. To generate 3 diverse responses, we increase the next token sampling temperature, and also shuffle the order of columns and tables in the prompt. Chain-of-Thought (CoT) prompting (Wei et al., 2022) has been proposed to enhance LLMs reasoning abilities by conditioning their final responses on a step-by-step chain of reasoning. Most CoT prompting approaches rely on few-shot examples in the prompt to guide LLMs on thinking step-by-step, following the format M = (qi, ri, si), where qi is the example question, ri is the reasoning path, and si is the ground truth SQL query for qi. We employ two distinct reasoning methods and an online synthetic example generation approach. As shown in Fig. 3a, different generators can yield different outputs, indicating their effectiveness for specific questions and databases. \",\n",
      "                    \"sub-headings\": [\n",
      "                        {\n",
      "                            \"Divide and Conquer CoT\": {\n",
      "                                \"content\": \": Divide-and-conquer perspective brings breaking down complex problems into smaller sub-problems, solving each individually, and then combining the solutions to obtain the final answer. Along these lines, we propose a CoT prompting approach that first decomposes the given question into smaller sub-problems using pseudo-SQL queries. In the conquer step, the solutions to these sub-problems are aggregated to construct the final answer. Finally, an optimization step is applied to the constructed query to remove redundant clauses and conditions. This approach is particularly powerful handling complex scenarios that involve nested queries, e.g. intricate WHERE or HAVING conditions, and queries requiring advanced mathematical operations. In Appendix Fig. 17, we exemplify a question and its corresponding SQL query that was successfully solved using this generator, a scenario the other methods considered in this paper could not address due to the querys complex conditions and SQL clauses. For a more detailed view of the divide-and-conquer prompt, please see Appendix Fig. 16. Additionally, Alg. 1 outlines the step-by-step process of this strategy to generate the final SQL output using a single LLM call. Algorithm 1 Divide and Conquer Chain-of-Thought (CoT) Strategy for Text-to-SQL. Input: Set of human-annotated few-shot examples M, user question Qu, target database D associated with the question, and a large language model (LLM) . Divide: 1: Sq (M, D, Qu) // Decompose the original question Qu into a set of sub-questions Sq 2: Ssql // Initialize an empty set Ssql to store partial SQL queries for each sub-question Conquer: 3: for each sub-question qi in Sq do 4: // Generate a partial SQL query for each sub-question qi 5: Ssql Ssql {(M, D, Qu, q1, ..., qi, sql1, ..., sqli1)} 6: end for Assemble: 7: Sf (M, D, Qu, Sq, Ssql) // Assemble the final SQL query Sf from all sub-queries in Ssql 8: return Sf \",\n",
      "                                \"sub-headings\": []\n",
      "                            }\n",
      "                        },\n",
      "                        {\n",
      "                            \"Query Plan CoT\": {\n",
      "                                \"content\": \": A query (execution) plan is a sequence of steps that the database engine follows to access or modify the data described by a SQL command. When a SQL query is executed, the database management systems query optimizers translate the SQL text into a query plan that the database engine can execute. This plan outlines how tables are accessed, how they are joined, and the specific operations performed on the data (see Appendix Fig. 19 as an example). Inspired by the step-by-step process that database engines use to execute SQL queries, we propose a reasoning strategy to construct the final SQL output. Query plans for any given SQL query can be obtained using the EXPLAIN\\\" command, which provides a detailed breakdown of execution steps. However, this output is often presented in a format that is difficult to interpret by LLMs (e.g. in SQLite). To address this, we convert the output of EXPLAIN\\\" command into a human-readable text format that aligns more closely with the pretraining data of LLMs. The human-readable version of query plans consists of three key steps: (1) identifying and locating the relevant tables for the question, (2) performing operations such as counting, filtering, or matching between tables, and (3) delivering the final result by selecting the appropriate columns to return. This reasoning method complements the divide-and-conquer CoT strategy. While the divide-and-conquer approach is better suited for decomposing complex questions, the query plan approach excels when questions require more reasoning over the relationships between different parts of the question and the database schema. It systematically explains which tables to scan, how to match columns, and how to apply filters. Appendix Fig. 20 shows an example of a question that was answered correctly only by this method. Appendix Fig. 18 provides the prompt used for this reasoning strategy. \",\n",
      "                                \"sub-headings\": []\n",
      "                            }\n",
      "                        }\n",
      "                    ]\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"3.4 Query Fixer\": {\n",
      "                    \"content\": \" In some cases, LLMs might generate queries that are syntactically incorrect. These queries are clear candidates for correction, as they fail to provide the correct answers. To address this, we apply an LLM-based query fixer that leverages the self-reflection (Shinn et al., 2024) method. The fixer reflects on the previously generated query, using feedback such as syntax error details or empty result sets to guide the correction process. We continue this iterative fixing approach up to a specified number of attempts,  (set to three in this paper). Appendix Fig. 21 demonstrates the prompt used for this query fixing step. \",\n",
      "                    \"sub-headings\": []\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"3.5 Selection Agent\": {\n",
      "                    \"content\": \" With three different methods for generating SQL queries, we can generate a set of candidate queries for any given question. The key challenge in this step is selecting the correct SQL query from this pool of candidates. A naive approach would be to measure consistency among the candidates by executing them, grouping them based on their execution results, and selecting a query from the largest group as the most likely correct answer. However, this would assume that the most consistent answer is always the best one, which is not always the case. Instead, we propose a more refined picking strategy, Algorithm 3, that relies on a selection agent. Given a set of candidates SQL queries C = {c1, c2, ..., cn}, the final responses are selected by finding 5 the candidate that has the highest score assigned by the selection model. This model p can take k candidates and rank them based on how accurately each of them answers the given question. Concretely, we formulate the selection of the final response as: cf = arg max cC    (n k) X i=1 p(ci1, . . . , cik | Qu, Hu, D)   , (1) where Qu refers to the users question, Hu is the provided hint, and D is the target database from which the question is being asked. In Eq. 1, we pass k candidates to the selection model to be ranked, with k being between 1 and n. In the extreme case of k = 1, the model is unable to make comparisons between candidates, which complicates the evaluation process for the model. As k increases, comparing more candidates makes the process more challenging for the model, as it needs to consider different aspects simultaneously. Consequently, we set k = 2 and train a model with a classification objective to compare only two candidates at a time. Having a set of high-quality and diverse candidates, the most straightforward solution is to employ off-the-shelf LLMs to make pairwise selections. However, experiments with Gemini-1.5-pro showed that using the LLM without fine-tuning resulted in only 58.01% binary classification accuracy. This is primarily due to the candidates being very similar to one another, requiring a fine-tuned model to learn the nuances and make more accurate decisions. To train the selection agent, we first generate candidate SQL queries on the training set (of Text-to-SQL benchmarks), and group them into clusters based on their execution results. For cases where at least one cluster contains correct queries and others contains incorrect ones, we create training examples in the form of tuples (Qu, Ci, Cj, Dij, yij), where Qu is the users question, Ci and Cj are the two candidate queries being compared, Dij is the database schema used by both candidates, and yij 0, 1 is the label indicating whether Ci or Cj is the correct query. To avoid order bias during training, we randomly shuffle the order of correct and incorrect queries in each pair. Since the number of cases with both correct and incorrect candidates is limited, for instances where no correct candidate exists, we include the ground truth SQL query in the prompt as a hint to guide the model in generating correct candidates. Algorithm 3 Picking the final SQL query from a pool of candidates. Input: Set of candidate SQL queries C = {c1, c2, ..., cn}, user question Qu, hint Hu, target database D, and a selection model p, er(ci, D) as the execution result of ci on D 1: ri 0 for all ci C // Initialize the score ri for each candidate query to zero 2: for each distinct pair (ci, cj) where i = j do 3: if er(ci, D) = er(cj, D) then 4: w i // ci is the winner if the execution results match 5: else 6: Si,j schema_union(ci, cj, D) // Construct union of schemas used in ci and cj 7: w p(Si,j, Qu, Hu, ci, cj)w {i, j} // Use binary classifier p to select the winner, w {i, j} 8: end if 9: rw rw + 1 // Increase the score of the winner cw by 1 10: end for 11: cf arg maxciC ri // Select the candidate with the highest score as the final SQL query cf 12: return cf In the pseudo-code for Algorithm 3, we begin by initializing a score of zero for each candidate query. Then, for every distinct pair of queries (ci, cj), we compare both (ci, cj) and (cj, ci) to mitigate any order bias, ensuring that both candidates in a pair are fairly evaluated. If both queries produce the same execution result on the database, we mark one as the winner and increment its score, as these results suggest consistency. If the execution results differ, we generate a union of the schema used by both queries and use the binary classifier to determine which query is more likely to be correct. The classifier takes into account the question, the two candidate queries, and the combined schema to make its decision. The winners score is then updated accordingly. After all comparisons, the candidate with the highest score is selected as the final query. In the rare case of a tie in the final scores, we break the tie by selecting one of the candidates arbitrarily. 6 \",\n",
      "                    \"sub-headings\": []\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"4 Online Synthetic Example Generation\": {\n",
      "        \"content\": \": Using M demonstrations for few-shot in-context learning has shown promising results on various related tasks (Pourreza & Rafiei, 2024a). Besides helping with specifying the task and illustrate the step-by-step process deriving the output, demonstrations constructed using relevant tables and columns can also help the model understand the underlying data schema. Based on this insight, we propose a synthetic demonstration generation strategy for Text-to-SQL  given the user question Qu, the target database D, and the selected columns ti (using a column selection approach similar to (Talaei et al., 2024)). Algorithm 2 Online Synthetic example generation strategy for Text-to-SQL. Input: User question Qu, additional user hint Hu, target database D and filtered relevant table columns t associated with the question, LLM , guidelines Rf for generating examples by SQL features, guidelines Rt for generating examples with filtered schema, and the numbers of examples to generate nf, nt respectively 1: P // {(qi, si) | qi, si }, where qi is input question , si is output SQL for the i-th example 2: P P {(D, Rf, nf)} // Generate n examples with entire database by common SQL features 3: P P {(t, Rt, nt)} // Generate examples with filtered columns to highlight correct schema usage 4: return P Algorithm 2 outlines the online synthetic example generation approach with two LLM generation steps. The first step focuses on generating illustrative examples with common SQL features described in the guideline Rf. The SQL features include equality and non-equality predicates, single table and multi-table JOIN, nested JOIN, ORDER BY and LIMIT, GROUP BY and HAVING, various aggregation functions. These are widely applicable SQL clauses and functions  the generated example SQL queries, incorporating these features, follow the BIRD SQL feature distribution (Appendix Fig 23a). The second step focuses on generating examples highlighting correct interpretation of the underlying data schema  the model  is asked to generate examples using ti and that are similar to the examples outlined in Rt. Appendix A.10 provides the prompts used for the example generation). While a relevant example (e.g. showing a nested JOIN query with multiple tables) can be helpful for questions that require complex JOIN queries, it might also mislead the LLM for overuse (e.g. when a simple single table query is sufficient). This and the inherent ambiguity of natural language query qi, for which we draw the examples by relevance, make the example selection challenging. Thus, we generate and inject the examples to the prompt online per qi. We ask the LLM to generate many input-output pairs for in-context learning. The final set of synthetic examples for qi contains examples generated with both Rf and Rt. This ensures that the example set is diverse both in SQL features/clauses and the choice of relevant tables/columns used. The diversity of the example set is desirable to avoid over-fitting the output to certain patterns (e.g., the model always writes a SQL with JOIN if shown mostly JOIN examples). Mixing various examples for various SQL features and database tables with and without column filtering is observed to result in better generation quality overall (please see Appendix Table 8). \",\n",
      "        \"sub-headings\": []\n",
      "    },\n",
      "    \"4 Experiments\": {\n",
      "        \"content\": \" \",\n",
      "        \"sub-headings\": [\n",
      "            {\n",
      "                \"4.1 Datasets and Models\": {\n",
      "                    \"content\": \" We evaluate the performance of the proposed CHASE-SQL framework on two widely-recognized cross-domain datasets: BIRD (Li et al., 2024c) and Spider (Yu et al., 2018). BIRD contains over 12,751 unique question-SQL pairs from 95 large databases, spanning more than 37 professional domains, with databases designed to resemble real-world scenarios, featuring messy data rows and complex schemas. Spider consists of 10,181 questions and 5,693 unique complex SQL queries across 200 databases, covering 138 domains. The Spider dataset is divided into non-overlapping training, development, and test sets similar to BIRD. For both, we use execution accuracy (EX), the official metric for their respective leaderboard, as the primary evaluation metric to compare methods. Details of the models and their hyperparameters are provided in Appendix section A.2. \",\n",
      "                    \"sub-headings\": []\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"4.2 BIRD results\": {\n",
      "                    \"content\": \" We present the end-to-end Text-to-SQL performance of the proposed CHASE-SQL framework using Claude- 3.5-sonnet and Gemini 1.5 pro on the BIRD development set, and Gemini 1.5 pro on the BIRD test set. We compare with both published methods (either with an available codebase and/or paper) and undisclosed methods. For a fair comparison with Gemini 1.5 pro, all LLM calls in the Claude-3.5-sonnet setting, except for the selection model, are made using Claude-3.5-sonnet (previously-trained selection model is reused). As shown in Table 2, CHASE-SQL with Gemini 1.5 pro achieves 73.01% accuracy on the BIRD development set and 73.0% on the BIRD holdout test set, outperforming all previous works and setting a new state-of-the-art performance. Table 2: Performance Comparison of different Text-to-SQL methods on BIRD benchmark. Method EX (Dev) EX (Test) Published CHASE-SQL + Gemini 1.5 (Ours) 73.01 73.0 CHASE-SQL + Claude 3.5 Sonnet (Ours) 69.53  Distillery + GPT-4o (Maamari et al., 2024) 67.21 71.83 CHESS (Talaei et al., 2024) 65.00 66.69 MCS-SQL + GPT-4 (Lee et al., 2024) 63.36 65.45 SuperSQL (Li et al., 2024a) 58.5 62.66 Undisclosed Insights AI 72.16 70.26 AskData + GPT-4o 72.03 72.39 OpenSearch-v2 + GPT-4o 69.3 72.28 PURPLE-RED + GPT-4o 68.12 70.21 Arcwise + GPT-4o 67.99 66.21 ExSL + granite-34b-code 67.47 67.76 Table 3: Performance Comparison of different Text-to-SQL methods on Spider test set. Method EX Training with Spider MCS-SQL + GPT-4 (Lee et al., 2024) 89.6  CHASE-SQL + Gemini 1.5 (Ours) 87.6  CHESS (Talaei et al., 2024) 87.2  DAIL-SQL + GPT-4 (Gao et al., 2023) 86.6  DIN-SQL + GPT-4 (Pourreza & Rafiei, 2024a) 85.3  C3 + ChatGPT (Dong et al., 2023) 82.3  RESDSQL 3B (Li et al., 2023a) 79.9  DIN-SQL + CodeX (Pourreza & Rafiei, 2024a) 78.2  T5-3B+NatSQL (Rai et al., 2023) 78.0  Graphix-3B+PICARD (Li et al., 2023b) 77.6  \",\n",
      "                    \"sub-headings\": []\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"4.3 Spider results\": {\n",
      "                    \"content\": \" We assess the generalizability of the proposed CHASE-SQL by evaluating it in an end-to-end way on the Spider test set without modifying the few-shot samples in the prompts or training a new selection model, i.e. without using and data from the target distribution. This approach allows us to test the performance of CHASE-SQL on different unseen query and database distributions compared to the data from training distributions. Table 3 demonstrates that CHASE-SQL achieves an execution accuracy of 87.6% on the Spider test set, placing it second among methods that have undergone specific training or prompt optimization for the Spider dataset. This highlights the strong generalizability of CHASE-SQL and its potential for generating high quality Text-to-SQL for unseen samples coming from very different distributions and unique challenges. 7 \",\n",
      "                    \"sub-headings\": []\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"4.4 Generator and selection performance\": {\n",
      "                    \"content\": \" Table 4: Ablation studies on single candidate gen- eration performance of the candidate generators compared with original BIRD prompt + zero-shot CoT with Gemini 1.5 pro on the BIRD dev set. Selector is our final performance applying the se- lection agent to 21 candidates generated from all generators. Method Execution Accuracy (%) (%) Baseline 57.75 - QP CoT 63.62 +5.87 DC CoT 63.92 +6.17 OS CoT 67.09 +9.34 Baseline w Query Fixer 61.58 +3.83 QP CoT w Query Fixer 65.51 +7.76 DC CoT w Query Fixer 65.77 +8.02 OS CoT w Query Fixer 68.02 +10.27 Generator + Fixer: To reveal performance of genera- tors, we conducted an ablation study to evaluate the per- formance of each candidate generation method before and after applying the query fixer. We compare the performance of the proposed generators in producing a single candidate query against the original BIRD prompt (Li et al., 2024c), 67.47 67.76 Table 3: Performance Comparison of different Text-to-SQL methods on Spider test set. Method EX Training with Spider MCS-SQL + GPT-4 (Lee et al., 2024) 89.6  CHASE-SQL + Gemini 1.5 (Ours) 87.6  CHESS (Talaei et al., 2024) 87.2  DAIL-SQL + GPT-4 (Gao et al., 2023) 86.6  DIN-SQL + GPT-4 (Pourreza & Rafiei, 2024a) 85.3  C3 + ChatGPT (Dong et al., 2023) 82.3  RESDSQL 3B (Li et al., 2023a) 79.9  DIN-SQL + CodeX (Pourreza & Rafiei, 2024a) 78.2  T5-3B+NatSQL (Rai et al., 2023) 78.0  Graphix-3B+PICARD (Li et al., 2023b) 77.6  4.3 Spider results We assess the generalizability of the proposed CHASE-SQL by evaluating it in an end-to-end way on the Spider test set without modifying the few-shot samples in the prompts or training a new selection model, i.e. without using and data from the target distribution. This approach allows us to test the performance of CHASE-SQL on different unseen query and database distributions compared to the data from training distributions. Table 3 demonstrates that CHASE-SQL achieves an execution accuracy of 87.6% on the Spider test set, placing it second among methods that have undergone specific training or prompt optimization for the Spider dataset. This highlights the strong generalizability of CHASE-SQL and its potential for generating high quality Text-to-SQL for unseen samples coming from very different distributions and unique challenges. 7 4.4 Generator and selection performance Table 4: Ablation studies on single candidate gen- eration performance of the candidate generators compared with original BIRD prompt + zero-shot CoT with Gemini 1.5 pro on the BIRD dev set. Selector is our final performance applying the se- lection agent to 21 candidates generated from all generators. Method Execution Accuracy (%) (%) Baseline 57.75 - QP CoT 63.62 +5.87 DC CoT 63.92 +6.17 OS CoT 67.09 +9.34 Baseline w Query Fixer 61.58 +3.83 QP CoT w Query Fixer 65.51 +7.76 DC CoT w Query Fixer 65.77 +8.02 OS CoT w Query Fixer 68.02 +10.27 Generator + Fixer: To reveal performance of genera- tors, we conducted an ablation study to evaluate the per- formance of each candidate generation method before and after applying the query fixer. We compare the performance of the proposed generators in producing a single candidate query against the original BIRD prompt (Li et al., 2024c), augmented with zero-shot CoT reasoning (Kojima et al., 2022), which serves as the baseline for assessing the quality of prompts. The results, shown in Table 4, indicate that the proposed methods significantly improve SQL generation performance, compared to the naive baseline, towards the goal of producing high-quality candidates while maintain- ing diversity. Among the candidate generators, the online synthetic data generation approach produced an impressive performance of 68.02%, demonstrating its effectiveness in leveraging test-time compute to improve LLM performance by generating high-quality synthetic examples. Furthermore, the query fixer proved crucial, enhancing the quality of the candidate pool and increasing performance by nearly 2% across all candidate generators. Table 5: Evaluating the binary selection accuracy of the different selection models. \",\n",
      "                    \"sub-headings\": [\n",
      "                        {\n",
      "                            \"Selection\": {\n",
      "                                \"content\": \" Model Binary Acc. (%) Claude-3.5-sonnet 60.21 Gemini-1.5-pro 63.98 Tuned Gemma 2 9B 64.28 Tuned Gemini-1.5-flash 71.01 Selection: We conducted an analysis on the binary selection accu- racy of the selection agent for cases where, in a pairwise comparison, one candidate is correct and the other is incorrect. We exclude cases where both candidates are either correct or incorrect, as the selection would not affect the outcome since both candidates have the same label. We compare the performance of Claude-3.5-sonnet and Gemini-1.5-pro (both out-of-the-box without fine-tuning) with two fine-tuned models: 1) Gemma 2 9B and 2) Gemini-1.5-flash. As shown in Table 5, both fine-tuned models achieve higher accuracy than the untuned counterparts, demonstrating the importance of fine-tuning to teach the model about the specific preferences. \",\n",
      "                                \"sub-headings\": []\n",
      "                            }\n",
      "                        },\n",
      "                        {\n",
      "                            \"Candidate Generation Analysis\": {\n",
      "                                \"content\": \": We analyze the performance of each candidate generator method individually. To better understand the performance potential when effectively selecting the correct SQL query from the candidate pool, we generate seven candidate SQL queries from each generator method (21 candidates in total) for all samples in the BIRD development set. We determine this number of candidates based on the observation that increasing the candidate pool beyond 20 did not yield significant improvements, as illustrated in Fig. 2d. By assuming access to an oracle selection model that always selects the correct SQL query from the seven candidates, we calculate the upper-bound performance achievable for each generator. Conversely, by assuming an adversarial selection model that always selects the wrong SQL query, we determine the lower-bound performance. Fig. 2 illustrates the upper-bound and lower-bound performance for all three methods together with the performance of our selection agent. As shown, the upper-bound performance of the two different CoT methods is generally higher than that of the synthetic example generation method for different number of candidates. However, their lower-bound performance is also lower than the synthetic method. Lower-bound accuracy reflects cases where all candidates are correct, reducing the noise in the selection process since it doesnt matter which candidate is chosen, so a higher lower-bound is preferred. This is evident in the selection agents performance, where a drop in the lower bound leads to diminishing returns from increasing the upper bound, causing the selection agents performance to plateau. Additionally, the upper-bound performance of combining all three methods reaches 82.79%, highlighting the significant room for improvement through better candidate picking methods. This demonstrates that the LLMs parametric knowledge already contains the information needed to solve most questions, highlighting the need for ensemble approaches to effectively extract and utilize this knowledge. 8 (a) Upper-bound and lower-bound Accuracy for Divide and Conquer CoT (b) Upper-bound and lower-bound Accuracy for Online Synthetic Example (c) Upper-bound and lower-bound performance for Query Plan CoT. (d) Upper-bound performance of all three can- didate generators across different difficulty cat- egories. Figure 2: Comparison of the upper- and lower-bound performance of different candidate generators. Additionally, we evaluate the upper-bound performance by combining all candidates from three candidate generation methods across the simple, moderate, and challenging difficulty levels for the BIRD development set. These difficulty categories are assigned by human experts during the creation of the BIRD development set. Fig. 2d shows that, as expected, the upper-bound performance increases with the number of candidates across all difficulty levels. However, for the challenging and moderate classes, the improvement plateaus earlier than in the simple class, suggesting that generating more samples does not further improve the upper-bound performance for these two difficulty levels. Fig. 2 presents a Venn diagram showcasing the performance of three generation methods: Query Plan, Divide and Conquer, and with Synthetic Examples. The numbers within the intersecting regions represent the instances where multiple methods generated at least one correct candidate. This diagram visually highlights the unique contributions of each method, which indicates the necessity of using all three generators. Additionally, in Fig. 3b, we compare the number of correct queries generated by each SQL generation method that are not correct by the other generators. The divide-and-conquer approach outperforms the others on challenging questions, while the query plan method excels on moderately difficult queries. To further analyze the performance of the generators across different domains and varying numbers of columns and tables, we compare the number of correct queries generated for each database, as shown in Appendix Fig. 4. As illustrated, both CoT methods generally perform similarly across databases, while the online synthetic example generation method significantly increases diversity, resulting in more correct answers overall across different databases. \",\n",
      "                                \"sub-headings\": []\n",
      "                            }\n",
      "                        },\n",
      "                        {\n",
      "                            \"Selection Agent Analysis\": {\n",
      "                                \"content\": \": We evaluate the query-picking performance by comparing the Text-to-SQL execution accuracy of the selection agent with the self-consistency method (using majority voting) Wang et al. (2022), an oracle model (upper bound), and an adversarial model (lower bound). To conduct the evaluation, we generate 10 samples from each candidate generation method using two different sampling temperatures: 0.5 and 1.8. The results, shown in Table 6, demonstrate that the selection agent significantly outperforms the self-consistency method with a large margin, roughly 6%. As expected, increasing the sampling temperature 9 35 30 33 38 72 23 1045 \",\n",
      "                                \"sub-headings\": []\n",
      "                            }\n",
      "                        },\n",
      "                        {\n",
      "                            \"Query Plan\": {\n",
      "                                \"content\": \" \",\n",
      "                                \"sub-headings\": []\n",
      "                            }\n",
      "                        },\n",
      "                        {\n",
      "                            \"Synthetic Example\": {\n",
      "                                \"content\": \" \",\n",
      "                                \"sub-headings\": []\n",
      "                            }\n",
      "                        },\n",
      "                        {\n",
      "                            \"Divide and Conquer\": {\n",
      "                                \"content\": \" \",\n",
      "                                \"sub-headings\": []\n",
      "                            }\n",
      "                        }\n",
      "                    ]\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"4.5 Ablation Studies\": {\n",
      "                    \"content\": \" In the previous sections, we evaluate the importance of the selection agent and each candidate generation method. Next, we focus on the analysis of the remaining components of CHASE-SQL: LSH for value retrieval, the query fixer, and three reasoning strategies (QP, OS, and DC). Table 7 shows the performance of CHASE- SQL without each of these steps, highlighting their significance in achieving higher-quality performance. The results demonstrate the contribution of each component, where removing LSH, the query fixer, or any of the candidate generators leads to a reduction in execution accuracy, further validating the importance of these components of CHASE-SQL. Moreover, the table compares the performance of our binary selection agent with two other selection methods: self-consistency (Wang et al., 2022) and a ranker agent. The ranker agent receives all candidates generated by our three candidate generators in a single prompt, compares them, and produce a ranking for each. For the ranker agent, we select the query with the lowest rank as the best answer. The binary selection agent significantly outperforms both the self-consistency and ranker agents, demonstrating the effectiveness of the proposed method. 10 \",\n",
      "                    \"sub-headings\": []\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"Unsolved Questions: 258\": {\n",
      "        \"content\": \" (a) Venn diagram illustrating the num- ber of instances for which each method: Query Plan, Synthetic Example, Divide and Conquer, produces at least one cor- rect candidate. The overlap regions repre- sent multiple methods generating correct candidates. (b) Number of correct queries across different complexity levels that were answered by each method. Figure 3: Comparison of SQL generation methods: Venn diagram showing unique and overlapping correct answers (left) and the performance across different complexity levels (right). raises the upper bound but also lowers the lower bound. This effect is more pronounced for the synthetic data generation method compared to the two CoT methods, mainly because LLMs generate reasoning steps before producing the final SQL query, which helps mitigate the randomness introduced by high-temperature sampling. The performance with self-consistency method generally decreases as temperature increases, since the majority cluster becomes smaller with more random queries. However, the proposed trained selection agent is less affected by temperature scaling and, in two cases, even improved its performance with a more diverse pool of samples. Table 6: Performance comparison of different picking methods on the candidates generated by the candidate generators on BIRD development set with two different temperatures. QP refers to query plan COT, DC refers to divide and conquer COT, and OS is the online synthetic example generation method. Picking Method QP (T=0.5) QP (T=1.8) DC (T=0.5) DC (T=1.8) OS (T=0.5) OS (T=1.8) Lower Bound 50.46 48.63 51.37 47.39 60.43 50.98 Upper Bound 78.55 80.44 78.42 79.34 74.77 79.66 Self-consistency 65.78 65.51 66.43 64.41 67.34 66.88 Our Selection Agent 71.7 71.73 71.31 70.53 70.4 71.38 \",\n",
      "        \"sub-headings\": []\n",
      "    },\n",
      "    \"5 Conclusion\": {\n",
      "        \"content\": \" Table 7: Ablation studies on the performance of CHASE- SQL after removing the query fixer, LSH for value re- trieval, and reasoning strategies, i.e., QP, OS, and DC. Method Execution Accuracy (%) (%) CHASE-SQL All 73.01 - CHASE-SQL w self-consistency 68.84 -4.17 CHASE-SQL w ranker agent 65.51 -7.5 CHASE-SQL w/o LSH 70.09 -2.92 CHASE-SQL w/o Query Fixer 69.23 -3.78 CHASE-SQL w/o QP 72.36 -0.65 CHASE-SQL w/o OS 72.16 -0.85 CHASE-SQL w/o DC 71.77 -1.24 We introduce a novel agentic framework, CHASE-SQL, to leverage test-time compute for generating diverse, high-quality SQL queries and accurately selecting the correct one. We propose multiple chain-of-thought prompting methods and an online synthetic example generation technique, along with a query selection mechanism that scores candidates based on pairwise comparisons. Our framework, CHASE-SQL, sets a new state-of-the-art in the notable public Text-to-SQL leaderboard (at the time of the submission), demon- strating the effectiveness of test-time computation for both generating diverse queries and selecting the most accurate response. CHASE-SQL addresses key issues like query diversity and selection optimization, paving the way for further improvements in complex reasoning tasks encountered at real-world Text-to-SQL challenges. \",\n",
      "        \"sub-headings\": []\n",
      "    },\n",
      "    \"Acknowledgments\": {\n",
      "        \"content\": \" We would like to thank Per Jacobsson, Raj Sinha, Zeke Miller, Reza Sherkat, James Su, Zhixian Yan, David Culler, and Xiance Si, for their valuable comments and feedbacks on our paper. We would also like to thank the BIRD team for their invaluable assistance with the evaluation of the BIRD test set. \",\n",
      "        \"sub-headings\": []\n",
      "    },\n",
      "    \"References\": {\n",
      "        \"content\": \" Ion Androutsopoulos, Graeme D Ritchie, and Peter Thanisch. Natural language interfaces to databasesan introduction. Natural language engineering, 1(1):2981, 1995. Ruichu Cai, Jinjie Yuan, Boyan Xu, and Zhifeng Hao. Sadga: Structure-aware dual graph aggregation network for text-to-sql. Advances in Neural Information Processing Systems, 34:76647676, 2021. Ruisheng Cao, Lu Chen, Zhi Chen, Yanbin Zhao, Su Zhu, and Kai Yu. Lgesql: line graph enhanced text-to-sql model with mixed local and non-local relations. arXiv preprint arXiv:2106.01093, 2021. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Xinyun Chen, Maxwell Lin, Nathanael Schrli, and Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023. DongHyun Choi, Myeong Cheol Shin, EungGyun Kim, and Dong Ryeol Shin. Ryansql: Recursively applying sketch-based slot fillings for complex text-to-sql in cross-domain databases. Computational Linguistics, 47 (2):309332, 2021. Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S Mirrokni. Locality-sensitive hashing scheme based on p-stable distributions. In Proceedings of the twentieth annual symposium on Computational geometry, pp. 253262, 2004. Xuemei Dong, Chao Zhang, Yuhang Ge, Yuren Mao, Yunjun Gao, Jinshu Lin, Dongfang Lou, et al. C3: Zero-shot text-to-sql with chatgpt. arXiv preprint arXiv:2307.07306, 2023. Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, and Jingren Zhou. Text-to-sql empowered by large language models: A benchmark evaluation. arXiv preprint arXiv:2308.15363, 2023. Jonathan Herzig, Pawe Krzysztof Nowak, Thomas Mller, Francesco Piccinno, and Julian Martin Eisenschlos. Tapas: Weakly supervised table parsing via pre-training. arXiv preprint arXiv:2004.02349, 2020. 11 Wonseok Hwang, Jinyeong Yim, Seunghyun Park, and Minjoon Seo. A comprehensive exploration on wikisql with table-aware word contextualization. arXiv preprint arXiv:1902.01069, 2019. George Katsogiannis-Meimarakis and Georgia Koutrika. A survey on deep learning approaches for text-to-sql. The VLDB Journal, 32(4):905936, 2023. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. Dongjun Lee, Choongwon Park, Jaehyuk Kim, and Heesoo Park. Mcs-sql: Leveraging multiple prompts and multiple-choice selection for text-to-sql generation. arXiv preprint arXiv:2405.07467, 2024. Boyan Li, Yuyu Luo, Chengliang Chai, Guoliang Li, and Nan Tang. The dawn of natural language to sql: Are we fully ready? arXiv preprint arXiv:2406.01265, 2024a. Fei Li and Hosagrahar V Jagadish. Constructing an interactive natural language interface for relational databases. Proceedings of the VLDB Endowment, 8(1):7384, 2014. Haoyang Li, Jing Zhang, Cuiping Li, and Hong Chen. Resdsql: Decoupling schema linking and skeleton parsing for text-to-sql. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 1306713075, 2023a. Haoyang Li, Jing Zhang, Hanbing Liu, Ju Fan, Xiaokang Zhang, Jun Zhu, Renjie Wei, Hongyan Pan, Cuiping Li, and Hong Chen. Codes: Towards building open-source language models for text-to-sql. Proceedings of the ACM on Management of Data, 2(3):128, 2024b. Jinyang Li, Binyuan Hui, Reynold Cheng, Bowen Qin, Chenhao Ma, Nan Huo, Fei Huang, Wenyu Du, Luo Si, and Yongbin Li. Graphix-t5: Mixing pre-trained transformers with graph-aware layers for text-to-sql parsing. arXiv preprint arXiv:2301.07507, 2023b. Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, et al. Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls. Advances in Neural Information Processing Systems, 36, 2024c. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rmi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):10921097, 2022. Karime Maamari, Fadhil Abubaker, Daniel Jaroslawicz, and Amine Mhedhbi. The death of schema linking? text-to-sql in the age of well-reasoned language models. arXiv preprint arXiv:2408.07702, 2024. Ansong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-tau Yih, Sida Wang, and Xi Victoria Lin. Lever: Learning to verify language-to-code generation with execution. In International Conference on Machine Learning, pp. 2610626128. PMLR, 2023. Rubn Prez-Mercado, Antonio Balderas, Andrs Muoz, Juan Francisco Cabrera, Manuel Palomo-Duarte, and Juan Manuel Dodero. Chatbotsql: Conversational agent to support relational database query language learning. SoftwareX, 22:101346, 2023. Mohammadreza Pourreza and Davood Rafiei. Din-sql: Decomposed in-context learning of text-to-sql with self-correction. Advances in Neural Information Processing Systems, 36, 2024a. Mohammadreza Pourreza and Davood Rafiei. Dts-sql: Decomposed text-to-sql with small large language models. arXiv preprint arXiv:2402.01117, 2024b. Mohammadreza Pourreza, Ruoxi Sun, Hailong Li, Lesly Miculicich, Tomas Pfister, and Sercan O Arik. Sql-gen: Bridging the dialect gap for text-to-sql via synthetic data and model merging. arXiv preprint arXiv:2408.12733, 2024. 12 Abdul Quamar, Vasilis Efthymiou, Chuan Lei, and Fatma zcan. Natural language interfaces to data. Found. Trends Databases, 11(4):319414, 2022. doi: 10.1561/1900000078. URL https://doi.org/10. 1561/1900000078.\",\n",
      "        \"sub-headings\": []\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from t2j.prem_sdk import PremSDK  \n",
    "from t2j.prompts import Prompts\n",
    "from t2j.chunker import DocumentChunker\n",
    "from t2j.utils import *\n",
    "\n",
    "import json\n",
    "\n",
    "N = 100\n",
    "promptsClass = Prompts()\n",
    "model = PremSDK()\n",
    "chunker = DocumentChunker(prompts=promptsClass, model=model)\n",
    "\n",
    "FILE_PATH = r\"C:\\Users\\Pratyush\\Desktop\\text-to-json\\examples\\text2SQL-solutions\\chase-sql.pdf\"\n",
    "output = chunker.smart_chunk(FILE_PATH)\n",
    "print(json.dumps(output, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'heading': 'abstract',\n",
       " 'content:': 'In tackling the challenges of large language model (LLM) performance for Text-to-SQL tasks, we introduce CHASE-SQL, a new framework that employs innovative strategies, using test-time compute in multi-agent modeling to improve candidate generation and selection. CHASE-SQL leverages LLMs intrinsic knowledge to generate diverse and high-quality SQL candidates using different LLM generators with: (1) a divide-and-conquer method that decomposes complex queries into manageable sub-queries in a single LLM call; (2) chain-of-thought reasoning based on query execution plans, reflecting the steps a database engine takes during execution; and (3) a unique instance-aware synthetic example generation technique, which offers specific few-shot demonstrations tailored to test questions. To identify the best candidate, a selection agent is employed to rank the candidates through pairwise comparisons with a fine-tuned binary-candidates selection LLM. This selection approach has been demonstrated to be more robust over alternatives. The proposed generators-selector framework not only enhances the quality and diversity of SQL queries but also outperforms previous methods. Overall, our proposed CHASE-SQL achieves the state-of-the-art execution accuracy of 73.0 % and 73.01% on the test set and development set of the notable BIRD Text-to-SQL dataset benchmark, rendering CHASE-SQL the top submission of the leaderboard (at the time of paper submission).'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = extract_text(FILE_PATH)\n",
    "chunks = chunk(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q/CS\n",
      "⊤+ Wval\n",
      "2 Hval\n",
      "HS/CS\n",
      "⊤+ Wval\n",
      "3 HCS⊤\u0011\n",
      "Root/Terminal Module\n",
      "To predict nested sub-\n",
      "queries, we add a module to predict if there is a\n",
      "new “ROOT” after an operator, which allows the\n",
      "model to decode queries recursively. For each pre-\n",
      "dicted column from the COL module that is in\n",
      "the WHERE clause, we ﬁrst call OP module, and\n",
      "then predict whether the next decoding step is a\n",
      "“ROOT” node or a value terminal node by\n",
      "PRT = P\n",
      "\u0010\n",
      "W1H⊤\n",
      "Q/CS + W2H⊤\n",
      "HS/CS + W3H⊤\n",
      "CS\n",
      "\u0011\n",
      "AND/OR Module\n",
      "For each condition column\n",
      "predicted from the COL module with number big-\n",
      "ger than 1, we predict from {AND, OR} by\n",
      "PAO = P\n",
      "\u0010\n",
      "W1H⊤\n",
      "Q + W2H⊤\n",
      "HS\n",
      "\u0011\n",
      "DESC/ASC/LIMIT Module\n",
      "In this module,\n",
      "for each predicted column from the COL module\n",
      "that is in the ORDER BY clause, we predict\n",
      "from {DESC, ASC, DESC LIMIT, ASC LIMIT} by\n",
      "PDAL = P\n",
      "\u0010\n",
      "W1H⊤\n",
      "Q/CS + W2H⊤\n",
      "HS/CS + W3H⊤\n",
      "CS\n",
      "\u0011\n",
      "HAVING Module\n",
      "In the HAVING module, for\n",
      "each predicted column from the COL module that\n",
      "is in the GROUP BY clause, we predict whether it\n",
      "is in the HAVING clause by\n",
      "PHAVING = P\n",
      "\u0010\n",
      "W1H⊤\n",
      "Q/CS + W2H⊤\n",
      "HS/CS + W3H⊤\n",
      "CS\n",
      "\u0011\n",
      "4.5\n",
      "Recursive SQL Generation\n",
      "The SQL generation process is a process of ac-\n",
      "tivating different modules recursively. As illus-\n",
      "trated in Figure 2, we employ a stack to organize\n",
      "our decoding process. At each decoding step, we\n",
      "pop one SQL token instance from the stack, and\n",
      "invoke a module based on the grammar to predict\n",
      "the next token instance, and then push the pre-\n",
      "dicted instance into the stack. The decoding pro-\n",
      "cess continues until the stack is empty.\n",
      "More speciﬁcally, we initialize a stack with only\n",
      "ROOT at the ﬁrst decoding step. At the next step,\n",
      "the stack pops ROOT. As illustrated in Figure 2,\n",
      "ROOT actives the IUEN module to predict if there\n",
      "is EXCEPT, INTERSECT or UNION. If so, there\n",
      "are two subqueries to be generated in the next\n",
      "step. If the model predicts NONE instead, it will\n",
      "be pushed into the stack. The stack pops NONE at\n",
      "next step. For example, in Figure 2, the current\n",
      "popped token is SELECT, which is a instance of\n",
      "keyword (KW) type. It calls the COL module to\n",
      "predict a column name, which will be pushed to\n",
      "the stack.\n",
      "4.6\n",
      "Data Augmentation\n",
      "Even though Spider already has a signiﬁcantly\n",
      "larger number of complex queries than existing\n",
      "datasets, the number of training examples for\n",
      "some complex SQL components is still limited.\n",
      "A widely used way is to conduct data augmenta-\n",
      "tion to generate more training examples automati-\n",
      "cally. Many studies (Berant and Liang, 2014; Iyer\n",
      "et al., 2017; Su and Yan, 2017) have shown that\n",
      "data augmentation can bring signiﬁcant improve-\n",
      "ment in performance.\n",
      "In prior work, data augmentation was typically\n",
      "performed within a single domain dataset. We pro-\n",
      "pose a cross-domain data augmentation method\n",
      "to expand our training data for complex queries.\n",
      "Cross-domain data augmentation is more difﬁ-\n",
      "cult than the in-domain setting because question-\n",
      "program pairs tend to have domain speciﬁc words\n",
      "and phrases.\n"
     ]
    }
   ],
   "source": [
    "print(chunks[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "final_headings = []\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i, head in tqdm(enumerate(headings), total=len(headings)):\n",
    "    text_chunk = chunks[head['chunk_id']]\n",
    "    text_chunk = text_chunk.replace(\"\\n\", \" \")\n",
    "    if i+1 != len(headings):\n",
    "        if headings[i+1]['chunk_id'] != head['chunk_id']:\n",
    "            text_chunk += (\" \" + chunks[headings[i+1]['chunk_id']])\n",
    "        text_chunk = text_chunk.replace(\"\\n\", \" \")\n",
    "        content = text_chunk.split(head['heading'], maxsplit=1)[1].split(headings[i+1]['heading'], maxsplit=1)[0]\n",
    "        final_headings.append({\n",
    "            'heading': head['heading'],\n",
    "            'chunk_id': head['chunk_id'],\n",
    "            'content': content\n",
    "        })\n",
    "    else:\n",
    "        content = text_chunk.split(head['heading'], maxsplit=1)[1]\n",
    "        final_headings.append({\n",
    "            'heading': head['heading'],\n",
    "            'chunk_id': head['chunk_id'],\n",
    "            'content': content\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_headings = [{'heading': '1 Introduction',\n",
    "  'chunk_id': 0,\n",
    "  'content': ' Text-to-SQL, as a bridge between human language and machine-readable structured query languages, is crucial for many use cases, converting natural language questions into executable SQL commands (Androutsopoulos et al., 1995; Li & Jagadish, 2014; Li et al., 2024c; Yu et al., 2018; ?). By enabling users to interact with complex database systems without requiring SQL proficiency, Text-to-SQL empowers users to extract valuable insights, perform streamlined data exploration, make informed decisions, generate data-driven reports and mine better features for machine learning (Chen et al., 2023; Pérez-Mercado et al., 2023; Pourreza & Rafiei, 2024a; Pourreza et al., 2024; Sun et al., 2023; Wang et al., 2019; Xie et al., 2023). Furthermore, Text-to-SQL systems play a pivotal role in automating data analytics with complex reasoning and powering conversational agents, expanding their applications beyond traditional data retrieval (Sun et al., 2023; Xie et al., 2023). As data continues to grow exponentially, the ability to query databases efficiently without extensive SQL knowledge becomes increasingly vital for a broad range of applications. Text-to-SQL can be considered a specialized form of code generation, with the contextual information potentially including the database schema, its metadata and along with the values. In the broader code generation domain, utilizing LLMs to generate a wide range of diverse candidates and select the best one has proven to be effective (Chen et al., 2021; Li et al., 2022; Ni et al., 2023). However, it is non-obvious what 1 arXiv:2410.01943v1  [cs.LG]  2 Oct 2024 leads to most effective candidate proposal and winner selector mechanisms. A straightforward yet effective approach involves generating candidates using zero-/few-shot or open-ended prompting, followed by selecting the best options utilizing self-consistency (Wang et al., 2022), which entails clustering candidates based on their execution outputs. This approach has demonstrated promising results in several studies (Lee et al., 2024; Maamari et al., 2024; Talaei et al., 2024; Wang et al., 2023). However, a single prompt design might not fully unleash the extensive Text-to-SQL knowledge of LLMs, and self-consistency methods might not be always effective. In fact, as illustrated in Table 1, the most consistent answers would not always be the correct ones, with an upper-bound performance 14% higher than that achieved through self-consistency. This substantial gap highlights the potential for significant improvement by implementing more effective selection methods to identify the best answer from the pool of candidate queries. Table 1: Evaluating single-query gen- eration vs. ensemble methods of self- consistency and the upper bound that can be achieved for Text-to-SQL with Gemini 1.5 Pro on the BIRD dev set. EX stands for execution accuracy. Method EX (%) Single query 63.01 Self-consistency 68.84 (+ 5.84) Upper-bound 82.79 (+ 19.78) Building on the challenges outlined in the previous section, we pro- pose novel approaches to improve LLM performance for Text-to-SQL by leveraging judiciously-designed test-time computations in an agentic framework. As indicated by the upper bound in Table 1, utilizing LLMs’ intrinsic knowledge offers significant potential for improvement. We propose methods that generate a diverse set of high-quality can- didate responses and apply a selection mechanism to identify the best answer. Achieving both high-quality and diverse candidate responses is critical for the success of scoring-based selection methods. Low diversity limits improvement potential and reduces the difference be- tween self-consistency and scoring-based approaches. While techniques like increasing temperature or reordering prompt contents can boost diversity, they often compromise the quality of the candidates. To address this, we introduce effective candidate generators designed to enhance diversity while maintaining high-quality outputs. Specifically, we propose three distinct candidate generation approaches, each capable of producing high-quality responses. The first is inspired by the divide-and-conquer algorithm, which breaks down complex problems into smaller, manageable parts to handle difficult queries. The second employs a query execution-plan-based chain-of-thought strategy, where the reasoning process mirrors the steps a database engine takes during query execution. Lastly, we introduce a novel online synthetic example generation method, which helps the model better understand the underlying data schema of the test database. These methods, when used independently, can produce highly-accurate SQL outputs. To effectively select the best answer among candidates, we introduce a selection agent, trained with a classification objective, that assigns scores based on pairwise comparisons between candidate queries. With this agent, we construct a comparison matrix for all candidates and select the final response based on the highest cumulative score. By combining these candidate generation methods with the proposed scoring model, we create an ensemble approach that leverages the strengths of each strategy to significantly improve overall performance. We present comprehensive evaluations on the efficacy of proposed methodologies of CHASE-SQL. Our innovative candidate generation approaches demonstrate superior performance compared to traditional generic CoT prompts, illustrating their capability in guiding LLMs through the decomposition of complex problems into manageable intermediate steps. Furthermore, the proposed selection agent significantly outperforms conventional consistency-based methods, contributing to the state-of-the-art results. Specifically, CHASE- SQL reaches an execution accuracy of 73.01% and 73.0% on the development set and test set of the challenging BIRD Text-to-SQL dataset which outperforms all of the published and undisclosed methods on this benchmark, by a large margin. '},\n",
    " {'heading': '2 Related Work',\n",
    "  'chunk_id': 1,\n",
    "  'content': ' Early Text-to-SQL methods predominantly utilized sequence-to-sequence architectures, encoding user queries and database schemas using models such as Graph Neural Networks (GNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and pre-trained transformer encoders (Cai et al., 2021; Cao et al., 2021; Hwang et al., 2019). On the decoding side, these systems employed either slot-filling or auto-regressive modelling approaches to construct the final SQL queries from the encoded inputs (Choi et al., 2021; Wang et al., 2019). Additionally, tabular language models like TaBERT (Yin et al., 2020), 2 TaPas (Herzig et al., 2020), and Grappa (Yu et al., 2020) have been developed to encode both tables and textual data effectively. However, the landscape has evolved with the widespread use of LLMs, which have largely replaced earlier methods with their superior performance (Katsogiannis-Meimarakis & Koutrika, 2023; Quamar et al., 2022). Initially, efforts concentrated on optimizing prompt designs for these LLMs (Dong et al., 2023; Gao et al., 2023; Pourreza & Rafiei, 2024a). Subsequent advancements have introduced more complex methodologies, including schema linking (Li et al., 2024b; Pourreza & Rafiei, 2024a,b; Talaei et al., 2024), self-correction or self-debugging (Chen et al., 2023; Talaei et al., 2024; Wang et al., 2023), and self-consistency techniques (Lee et al., 2024; Maamari et al., 2024; Sun et al., 2023; Talaei et al., 2024), further enhancing the performance by proposing complex LLM-based pipelines. '},\n",
    " {'heading': '3 Methods', 'chunk_id': 1, 'content': ' '},\n",
    " {'heading': '3.1 Overall Framework',\n",
    "  'chunk_id': 1,\n",
    "  'content': ' This section outlines the proposed CHASE-SQL framework, which consists of four primary components: 1) Value retrieval, 2) Candidate generator, 3) Query fixer, and 4) Selection agent. As illustrated in Fig. 1. The proposed framework begins by retrieving relevant database values. Subsequently, all contextual information, including retrieved values, database metadata, and schema, is provided to an LLM to generate candidate queries. These candidate queries then undergo a fixing loop, and finally, all candidates are compared in a pairwise way using the trained selection agent to pick the correct answer. The following sections delve into the details of each component. Figure 1: Overview of the proposed CHASE-SQL framework for Text-to-SQL, with value retrieval and using a selection agent for improve picking of the answers among the generated candidates along with a fixer to provide feedback for refinement of the outputs. '},\n",
    " {'heading': '3.2 Value Retrieval',\n",
    "  'chunk_id': 1,\n",
    "  'content': ' Databases might contain very high number of rows, with often only a few being relevant to a query. Retrieving relevant values is crucial as they can be used in various SQL clauses like ‘WHERE’ and ‘HAVING’. Similar to the approach in (Talaei et al., 2024), we begin by extracting keywords from the given question using an LLM prompted with few-shot examples. For each keyword, we employ locality-sensitive hashing (LSH) (Datar et al., 2004) to retrieve the most syntactically-similar words, and re-rank them based on embedding-based similarity and edit distance. This approach is robust to typos in the question and considers keyword semantics during retrieval. '},\n",
    " {'heading': '3.3 Multi-path Candidate Generation',\n",
    "  'chunk_id': 1,\n",
    "  'content': ' As shown in Table 1, relying solely on consistency among responses can lead to sub-optimal performance. Therefore, we prioritize diversity in generation of multiple response candidates to increase the likelihood of generating at least one correct answer. Among the diverse responses generated by the candidate generators, we select one as the final response using a selection agent that compares candidates pairwise. To generate 3 diverse responses, we increase the next token sampling temperature, and also shuffle the order of columns and tables in the prompt. Chain-of-Thought (CoT) prompting (Wei et al., 2022) has been proposed to enhance LLMs’ reasoning abilities by conditioning their final responses on a step-by-step chain of reasoning. Most CoT prompting approaches rely on few-shot examples in the prompt to guide LLMs on thinking step-by-step, following the format M = (qi, ri, si), where qi is the example question, ri is the reasoning path, and si is the ground truth SQL query for qi. We employ two distinct reasoning methods and an online synthetic example generation approach. As shown in Fig. 3a, different generators can yield different outputs, indicating their effectiveness for specific questions and databases. '},\n",
    " {'heading': 'Divide and Conquer CoT',\n",
    "  'chunk_id': 1,\n",
    "  'content': ': Divide-and-conquer perspective brings breaking down complex problems into smaller sub-problems, solving each individually, and then combining the solutions to obtain the final answer. Along these lines, we propose a CoT prompting approach that first decomposes the given question into smaller sub-problems using pseudo-SQL queries. In the ’conquer’ step, the solutions to these sub-problems are aggregated to construct the final answer. Finally, an optimization step is applied to the constructed query to remove redundant clauses and conditions. This approach is particularly powerful handling complex scenarios that involve nested queries, e.g. intricate WHERE or HAVING conditions, and queries requiring advanced mathematical operations. In Appendix Fig. 17, we exemplify a question and its corresponding SQL query that was successfully solved using this generator, a scenario the other methods considered in this paper could not address due to the query’s complex conditions and SQL clauses. For a more detailed view of the divide-and-conquer prompt, please see Appendix Fig. 16. Additionally, Alg. 1 outlines the step-by-step process of this strategy to generate the final SQL output using a single LLM call. Algorithm 1 Divide and Conquer Chain-of-Thought (CoT) Strategy for Text-to-SQL. Input: Set of human-annotated few-shot examples M, user question Qu, target database D associated with the question, and a large language model (LLM) θ. Divide: 1: Sq ←θ(M, D, Qu) // Decompose the original question Qu into a set of sub-questions Sq 2: Ssql ←∅// Initialize an empty set Ssql to store partial SQL queries for each sub-question Conquer: 3: for each sub-question qi in Sq do 4: // Generate a partial SQL query for each sub-question qi 5: Ssql ←Ssql ∪{θ(M, D, Qu, q1, ..., qi, sql1, ..., sqli−1)} 6: end for Assemble: 7: Sf ←θ(M, D, Qu, Sq, Ssql) // Assemble the final SQL query Sf from all sub-queries in Ssql 8: return Sf '},\n",
    " {'heading': 'Query Plan CoT',\n",
    "  'chunk_id': 1,\n",
    "  'content': ': A query (execution) plan is a sequence of steps that the database engine follows to access or modify the data described by a SQL command. When a SQL query is executed, the database management systems’ query optimizers translate the SQL text into a query plan that the database engine can execute. This plan outlines how tables are accessed, how they are joined, and the specific operations performed on the data (see Appendix Fig. 19 as an example). Inspired by the step-by-step process that database engines use to execute SQL queries, we propose a reasoning strategy to construct the final SQL output. Query plans for any given SQL query can be obtained using the “EXPLAIN\" command, which provides a detailed breakdown of execution steps. However, this output is often presented in a format that is difficult to interpret by LLMs (e.g. in SQLite). To address this, we convert the output of “EXPLAIN\" command into a human-readable text format that aligns more closely with the pretraining data of LLMs. The human-readable version of query plans consists of three key steps: (1) identifying and locating the relevant tables for the question, (2) performing operations such as counting, filtering, or matching between tables, and (3) delivering the final result by selecting the appropriate columns to return. This reasoning method complements the divide-and-conquer CoT strategy. While the divide-and-conquer approach is better suited for decomposing complex questions, the query plan approach excels when questions require more reasoning over the relationships between different parts of the question and the database schema. It systematically explains which tables to scan, how to match columns, and how to apply filters. Appendix Fig. 20 shows an example of a question that was answered correctly only by this method. Appendix Fig. 18 provides the prompt used for this reasoning strategy. '},\n",
    " {'heading': '4 Online Synthetic Example Generation',\n",
    "  'chunk_id': 2,\n",
    "  'content': ': Using M demonstrations for few-shot in-context learning has shown promising results on various related tasks (Pourreza & Rafiei, 2024a). Besides helping with specifying the task and illustrate the step-by-step process deriving the output, demonstrations constructed using relevant tables and columns can also help the model understand the underlying data schema. Based on this insight, we propose a synthetic demonstration generation strategy for Text-to-SQL – given the user question Qu, the target database D, and the selected columns ti (using a column selection approach similar to (Talaei et al., 2024)). Algorithm 2 Online Synthetic example generation strategy for Text-to-SQL. Input: User question Qu, additional user hint Hu, target database D and filtered relevant table columns t associated with the question, LLM θ, guidelines Rf for generating examples by SQL features, guidelines Rt for generating examples with filtered schema, and the numbers of examples to generate nf, nt respectively 1: P ←∅// {(qi, si) | qi, si ∈Σ∗}, where qi is input question , si is output SQL for the i-th example 2: P ←P ∪{θ(D, Rf, nf)} // Generate n examples with entire database by common SQL features 3: P ←P ∪{θ(t, Rt, nt)} // Generate examples with filtered columns to highlight correct schema usage 4: return P Algorithm 2 outlines the online synthetic example generation approach with two LLM generation steps. The first step focuses on generating illustrative examples with common SQL features described in the guideline Rf. The SQL features include equality and non-equality predicates, single table and multi-table JOIN, nested JOIN, ORDER BY and LIMIT, GROUP BY and HAVING, various aggregation functions. These are widely applicable SQL clauses and functions – the generated example SQL queries, incorporating these features, follow the BIRD SQL feature distribution (Appendix Fig 23a). The second step focuses on generating examples highlighting correct interpretation of the underlying data schema – the model θ is asked to generate examples using ti and that are similar to the examples outlined in Rt. Appendix A.10 provides the prompts used for the example generation). While a relevant example (e.g. showing a nested JOIN query with multiple tables) can be helpful for questions that require complex JOIN queries, it might also mislead the LLM for overuse (e.g. when a simple single table query is sufficient). This and the inherent ambiguity of natural language query qi, for which we draw the examples by relevance, make the example selection challenging. Thus, we generate and inject the examples to the prompt online per qi. We ask the LLM to generate many input-output pairs for in-context learning. The final set of synthetic examples for qi contains examples generated with both Rf and Rt. This ensures that the example set is diverse both in SQL features/clauses and the choice of relevant tables/columns used. The diversity of the example set is desirable to avoid over-fitting the output to certain patterns (e.g., the model always writes a SQL with JOIN if shown mostly JOIN examples). Mixing various examples for various SQL features and database tables with and without column filtering is observed to result in better generation quality overall (please see Appendix Table 8). '},\n",
    " {'heading': '3.4 Query Fixer',\n",
    "  'chunk_id': 2,\n",
    "  'content': ' In some cases, LLMs might generate queries that are syntactically incorrect. These queries are clear candidates for correction, as they fail to provide the correct answers. To address this, we apply an LLM-based query fixer that leverages the self-reflection (Shinn et al., 2024) method. The fixer reflects on the previously generated query, using feedback such as syntax error details or empty result sets to guide the correction process. We continue this iterative fixing approach up to a specified number of attempts, β (set to three in this paper). Appendix Fig. 21 demonstrates the prompt used for this query fixing step. '},\n",
    " {'heading': '3.5 Selection Agent',\n",
    "  'chunk_id': 2,\n",
    "  'content': ' With three different methods for generating SQL queries, we can generate a set of candidate queries for any given question. The key challenge in this step is selecting the correct SQL query from this pool of candidates. A naive approach would be to measure consistency among the candidates by executing them, grouping them based on their execution results, and selecting a query from the largest group as the most likely correct answer. However, this would assume that the most consistent answer is always the best one, which is not always the case. Instead, we propose a more refined picking strategy, Algorithm 3, that relies on a selection agent. Given a set of candidates SQL queries C = {c1, c2, ..., cn}, the final responses are selected by finding 5 the candidate that has the highest score assigned by the selection model. This model θp can take k candidates and rank them based on how accurately each of them answers the given question. Concretely, we formulate the selection of the final response as: cf = arg max c∈C \\uf8eb \\uf8ec \\uf8ed (n k) X i=1 θp(ci1, . . . , cik | Qu, Hu, D) \\uf8f6 \\uf8f7 \\uf8f8, (1) where Qu refers to the user’s question, Hu is the provided hint, and D is the target database from which the question is being asked. In Eq. 1, we pass k candidates to the selection model to be ranked, with k being between 1 and n. In the extreme case of k = 1, the model is unable to make comparisons between candidates, which complicates the evaluation process for the model. As k increases, comparing more candidates makes the process more challenging for the model, as it needs to consider different aspects simultaneously. Consequently, we set k = 2 and train a model with a classification objective to compare only two candidates at a time. Having a set of high-quality and diverse candidates, the most straightforward solution is to employ off-the-shelf LLMs to make pairwise selections. However, experiments with Gemini-1.5-pro showed that using the LLM without fine-tuning resulted in only 58.01% binary classification accuracy. This is primarily due to the candidates being very similar to one another, requiring a fine-tuned model to learn the nuances and make more accurate decisions. To train the selection agent, we first generate candidate SQL queries on the training set (of Text-to-SQL benchmarks), and group them into clusters based on their execution results. For cases where at least one cluster contains correct queries and others contains incorrect ones, we create training examples in the form of tuples (Qu, Ci, Cj, Dij, yij), where Qu is the user’s question, Ci and Cj are the two candidate queries being compared, Dij is the database schema used by both candidates, and yij ∈0, 1 is the label indicating whether Ci or Cj is the correct query. To avoid order bias during training, we randomly shuffle the order of correct and incorrect queries in each pair. Since the number of cases with both correct and incorrect candidates is limited, for instances where no correct candidate exists, we include the ground truth SQL query in the prompt as a hint to guide the model in generating correct candidates. Algorithm 3 Picking the final SQL query from a pool of candidates. Input: Set of candidate SQL queries C = {c1, c2, ..., cn}, user question Qu, hint Hu, target database D, and a selection model θp, er(ci, D) as the execution result of ci on D 1: ri ←0 for all ci ∈C // Initialize the score ri for each candidate query to zero 2: for each distinct pair (ci, cj) where i ̸= j do 3: if er(ci, D) = er(cj, D) then 4: w ←i // ci is the winner if the execution results match 5: else 6: Si,j ←schema_union(ci, cj, D) // Construct union of schemas used in ci and cj 7: w ←θp(Si,j, Qu, Hu, ci, cj)w ∈{i, j} // Use binary classifier θp to select the winner, w ∈{i, j} 8: end if 9: rw ←rw + 1 // Increase the score of the winner cw by 1 10: end for 11: cf ←arg maxci∈C ri // Select the candidate with the highest score as the final SQL query cf 12: return cf In the pseudo-code for Algorithm 3, we begin by initializing a score of zero for each candidate query. Then, for every distinct pair of queries (ci, cj), we compare both (ci, cj) and (cj, ci) to mitigate any order bias, ensuring that both candidates in a pair are fairly evaluated. If both queries produce the same execution result on the database, we mark one as the winner and increment its score, as these results suggest consistency. If the execution results differ, we generate a union of the schema used by both queries and use the binary classifier to determine which query is more likely to be correct. The classifier takes into account the question, the two candidate queries, and the combined schema to make its decision. The winner’s score is then updated accordingly. After all comparisons, the candidate with the highest score is selected as the final query. In the rare case of a tie in the final scores, we break the tie by selecting one of the candidates arbitrarily. 6 '},\n",
    " {'heading': '4 Experiments', 'chunk_id': 3, 'content': ' '},\n",
    " {'heading': '4.1 Datasets and Models',\n",
    "  'chunk_id': 3,\n",
    "  'content': ' We evaluate the performance of the proposed CHASE-SQL framework on two widely-recognized cross-domain datasets: BIRD (Li et al., 2024c) and Spider (Yu et al., 2018). BIRD contains over 12,751 unique question-SQL pairs from 95 large databases, spanning more than 37 professional domains, with databases designed to resemble real-world scenarios, featuring messy data rows and complex schemas. Spider consists of 10,181 questions and 5,693 unique complex SQL queries across 200 databases, covering 138 domains. The Spider dataset is divided into non-overlapping training, development, and test sets similar to BIRD. For both, we use execution accuracy (EX), the official metric for their respective leaderboard, as the primary evaluation metric to compare methods. Details of the models and their hyperparameters are provided in Appendix section A.2. '},\n",
    " {'heading': '4.2 BIRD results',\n",
    "  'chunk_id': 3,\n",
    "  'content': ' We present the end-to-end Text-to-SQL performance of the proposed CHASE-SQL framework using Claude- 3.5-sonnet and Gemini 1.5 pro on the BIRD development set, and Gemini 1.5 pro on the BIRD test set. We compare with both published methods (either with an available codebase and/or paper) and undisclosed methods. For a fair comparison with Gemini 1.5 pro, all LLM calls in the Claude-3.5-sonnet setting, except for the selection model, are made using Claude-3.5-sonnet (previously-trained selection model is reused). As shown in Table 2, CHASE-SQL with Gemini 1.5 pro achieves 73.01% accuracy on the BIRD development set and 73.0% on the BIRD holdout test set, outperforming all previous works and setting a new state-of-the-art performance. Table 2: Performance Comparison of different Text-to-SQL methods on BIRD benchmark. Method EX (Dev) EX (Test) Published CHASE-SQL + Gemini 1.5 (Ours) 73.01 73.0 CHASE-SQL + Claude 3.5 Sonnet (Ours) 69.53 – Distillery + GPT-4o (Maamari et al., 2024) 67.21 71.83 CHESS (Talaei et al., 2024) 65.00 66.69 MCS-SQL + GPT-4 (Lee et al., 2024) 63.36 65.45 SuperSQL (Li et al., 2024a) 58.5 62.66 Undisclosed Insights AI 72.16 70.26 AskData + GPT-4o 72.03 72.39 OpenSearch-v2 + GPT-4o 69.3 72.28 PURPLE-RED + GPT-4o 68.12 70.21 Arcwise + GPT-4o 67.99 66.21 ExSL + granite-34b-code 67.47 67.76 Table 3: Performance Comparison of different Text-to-SQL methods on Spider test set. Method EX Training with Spider MCS-SQL + GPT-4 (Lee et al., 2024) 89.6 ✓ CHASE-SQL + Gemini 1.5 (Ours) 87.6 ✗ CHESS (Talaei et al., 2024) 87.2 ✗ DAIL-SQL + GPT-4 (Gao et al., 2023) 86.6 ✓ DIN-SQL + GPT-4 (Pourreza & Rafiei, 2024a) 85.3 ✓ C3 + ChatGPT (Dong et al., 2023) 82.3 ✓ RESDSQL 3B (Li et al., 2023a) 79.9 ✓ DIN-SQL + CodeX (Pourreza & Rafiei, 2024a) 78.2 ✓ T5-3B+NatSQL (Rai et al., 2023) 78.0 ✓ Graphix-3B+PICARD (Li et al., 2023b) 77.6 ✓ '},\n",
    " {'heading': '4.3 Spider results',\n",
    "  'chunk_id': 4,\n",
    "  'content': ' We assess the generalizability of the proposed CHASE-SQL by evaluating it in an end-to-end way on the Spider test set without modifying the few-shot samples in the prompts or training a new selection model, i.e. without using and data from the target distribution. This approach allows us to test the performance of CHASE-SQL on different unseen query and database distributions compared to the data from training distributions. Table 3 demonstrates that CHASE-SQL achieves an execution accuracy of 87.6% on the Spider test set, placing it second among methods that have undergone specific training or prompt optimization for the Spider dataset. This highlights the strong generalizability of CHASE-SQL and its potential for generating high quality Text-to-SQL for unseen samples coming from very different distributions and unique challenges. 7 '},\n",
    " {'heading': '4.4 Generator and selection performance',\n",
    "  'chunk_id': 4,\n",
    "  'content': ' Table 4: Ablation studies on single candidate gen- eration performance of the candidate generators compared with original BIRD prompt + zero-shot CoT with Gemini 1.5 pro on the BIRD dev set. Selector is our final performance applying the se- lection agent to 21 candidates generated from all generators. Method Execution Accuracy (%) ∆(%) Baseline 57.75 - QP CoT 63.62 +5.87 DC CoT 63.92 +6.17 OS CoT 67.09 +9.34 Baseline w Query Fixer 61.58 +3.83 QP CoT w Query Fixer 65.51 +7.76 DC CoT w Query Fixer 65.77 +8.02 OS CoT w Query Fixer 68.02 +10.27 Generator + Fixer: To reveal performance of genera- tors, we conducted an ablation study to evaluate the per- formance of each candidate generation method before and after applying the query fixer. We compare the performance of the proposed generators in producing a single candidate query against the original BIRD prompt (Li et al., 2024c), augmented with zero-shot CoT reasoning (Kojima et al., 2022), which serves as the baseline for assessing the quality of prompts. The results, shown in Table 4, indicate that the proposed methods significantly improve SQL generation performance, compared to the naive baseline, towards the goal of producing high-quality candidates while maintain- ing diversity. Among the candidate generators, the online synthetic data generation approach produced an impressive performance of 68.02%, demonstrating its effectiveness in leveraging test-time compute to improve LLM performance by generating high-quality synthetic examples. Furthermore, the query fixer proved crucial, enhancing the quality of the candidate pool and increasing performance by nearly 2% across all candidate generators. Table 5: Evaluating the binary selection accuracy of the different selection models. '},\n",
    " {'heading': 'Selection',\n",
    "  'chunk_id': 5,\n",
    "  'content': ' Model Binary Acc. (%) Claude-3.5-sonnet 60.21 Gemini-1.5-pro 63.98 Tuned Gemma 2 9B 64.28 Tuned Gemini-1.5-flash 71.01 Selection: We conducted an analysis on the binary selection accu- racy of the selection agent for cases where, in a pairwise comparison, one candidate is correct and the other is incorrect. We exclude cases where both candidates are either correct or incorrect, as the selection would not affect the outcome since both candidates have the same label. We compare the performance of Claude-3.5-sonnet and Gemini-1.5-pro (both out-of-the-box without fine-tuning) with two fine-tuned models: 1) Gemma 2 9B and 2) Gemini-1.5-flash. As shown in Table 5, both fine-tuned models achieve higher accuracy than the untuned counterparts, demonstrating the importance of fine-tuning to teach the model about the specific preferences. '},\n",
    " {'heading': 'Candidate Generation Analysis',\n",
    "  'chunk_id': 5,\n",
    "  'content': ': We analyze the performance of each candidate generator method individually. To better understand the performance potential when effectively selecting the correct SQL query from the candidate pool, we generate seven candidate SQL queries from each generator method (21 candidates in total) for all samples in the BIRD development set. We determine this number of candidates based on the observation that increasing the candidate pool beyond 20 did not yield significant improvements, as illustrated in Fig. 2d. By assuming access to an oracle selection model that always selects the correct SQL query from the seven candidates, we calculate the upper-bound performance achievable for each generator. Conversely, by assuming an adversarial selection model that always selects the wrong SQL query, we determine the lower-bound performance. Fig. 2 illustrates the upper-bound and lower-bound performance for all three methods together with the performance of our selection agent. As shown, the upper-bound performance of the two different CoT methods is generally higher than that of the synthetic example generation method for different number of candidates. However, their lower-bound performance is also lower than the synthetic method. Lower-bound accuracy reflects cases where all candidates are correct, reducing the noise in the selection process since it doesn’t matter which candidate is chosen, so a higher lower-bound is preferred. This is evident in the selection agent’s performance, where a drop in the lower bound leads to diminishing returns from increasing the upper bound, causing the selection agent’s performance to plateau. Additionally, the upper-bound performance of combining all three methods reaches 82.79%, highlighting the significant room for improvement through better candidate picking methods. This demonstrates that the LLM’s parametric knowledge already contains the information needed to solve most questions, highlighting the need for ensemble approaches to effectively extract and utilize this knowledge. 8 (a) Upper-bound and lower-bound Accuracy for Divide and Conquer CoT (b) Upper-bound and lower-bound Accuracy for Online Synthetic Example (c) Upper-bound and lower-bound performance for Query Plan CoT. (d) Upper-bound performance of all three can- didate generators across different difficulty cat- egories. Figure 2: Comparison of the upper- and lower-bound performance of different candidate generators. Additionally, we evaluate the upper-bound performance by combining all candidates from three candidate generation methods across the simple, moderate, and challenging difficulty levels for the BIRD development set. These difficulty categories are assigned by human experts during the creation of the BIRD development set. Fig. 2d shows that, as expected, the upper-bound performance increases with the number of candidates across all difficulty levels. However, for the challenging and moderate classes, the improvement plateaus earlier than in the simple class, suggesting that generating more samples does not further improve the upper-bound performance for these two difficulty levels. Fig. 2 presents a Venn diagram showcasing the performance of three generation methods: Query Plan, Divide and Conquer, and with Synthetic Examples. The numbers within the intersecting regions represent the instances where multiple methods generated at least one correct candidate. This diagram visually highlights the unique contributions of each method, which indicates the necessity of using all three generators. Additionally, in Fig. 3b, we compare the number of correct queries generated by each SQL generation method that are not correct by the other generators. The divide-and-conquer approach outperforms the others on challenging questions, while the query plan method excels on moderately difficult queries. To further analyze the performance of the generators across different domains and varying numbers of columns and tables, we compare the number of correct queries generated for each database, as shown in Appendix Fig. 4. As illustrated, both CoT methods generally perform similarly across databases, while the online synthetic example generation method significantly increases diversity, resulting in more correct answers overall across different databases. '},\n",
    " {'heading': 'Selection Agent Analysis',\n",
    "  'chunk_id': 5,\n",
    "  'content': ': We evaluate the query-picking performance by comparing the Text-to-SQL execution accuracy of the selection agent with the self-consistency method (using majority voting) Wang et al. (2022), an oracle model (upper bound), and an adversarial model (lower bound). To conduct the evaluation, we generate 10 samples from each candidate generation method using two different sampling temperatures: 0.5 and 1.8. The results, shown in Table 6, demonstrate that the selection agent significantly outperforms the self-consistency method with a large margin, roughly 6%. As expected, increasing the sampling temperature 9 35 30 33 38 72 23 1045 '},\n",
    " {'heading': 'Query Plan', 'chunk_id': 6, 'content': ' '},\n",
    " {'heading': 'Synthetic Example', 'chunk_id': 6, 'content': ' '},\n",
    " {'heading': 'Divide and Conquer',\n",
    "  'chunk_id': 6,\n",
    "  'content': ' Unsolved Questions: 258 (a) Venn diagram illustrating the num- ber of instances for which each method: Query Plan, Synthetic Example, Divide and Conquer, produces at least one cor- rect candidate. The overlap regions repre- sent multiple methods generating correct candidates. (b) Number of correct queries across different complexity levels that were answered by each method. Figure 3: Comparison of SQL generation methods: Venn diagram showing unique and overlapping correct answers (left) and the performance across different complexity levels (right). raises the upper bound but also lowers the lower bound. This effect is more pronounced for the synthetic data generation method compared to the two CoT methods, mainly because LLMs generate reasoning steps before producing the final SQL query, which helps mitigate the randomness introduced by high-temperature sampling. The performance with self-consistency method generally decreases as temperature increases, since the majority cluster becomes smaller with more random queries. However, the proposed trained selection agent is less affected by temperature scaling and, in two cases, even improved its performance with a more diverse pool of samples. Table 6: Performance comparison of different picking methods on the candidates generated by the candidate generators on BIRD development set with two different temperatures. QP refers to query plan COT, DC refers to divide and conquer COT, and OS is the online synthetic example generation method. Picking Method QP (T=0.5) QP (T=1.8) DC (T=0.5) DC (T=1.8) OS (T=0.5) OS (T=1.8) Lower Bound 50.46 48.63 51.37 47.39 60.43 50.98 Upper Bound 78.55 80.44 78.42 79.34 74.77 79.66 Self-consistency 65.78 65.51 66.43 64.41 67.34 66.88 Our Selection Agent 71.7 71.73 71.31 70.53 70.4 71.38 '},\n",
    " {'heading': '4.5 Ablation Studies',\n",
    "  'chunk_id': 6,\n",
    "  'content': ' In the previous sections, we evaluate the importance of the selection agent and each candidate generation method. Next, we focus on the analysis of the remaining components of CHASE-SQL: LSH for value retrieval, the query fixer, and three reasoning strategies (QP, OS, and DC). Table 7 shows the performance of CHASE- SQL without each of these steps, highlighting their significance in achieving higher-quality performance. The results demonstrate the contribution of each component, where removing LSH, the query fixer, or any of the candidate generators leads to a reduction in execution accuracy, further validating the importance of these components of CHASE-SQL. Moreover, the table compares the performance of our binary selection agent with two other selection methods: self-consistency (Wang et al., 2022) and a ranker agent. The ranker agent receives all candidates generated by our three candidate generators in a single prompt, compares them, and produce a ranking for each. For the ranker agent, we select the query with the lowest rank as the best answer. The binary selection agent significantly outperforms both the self-consistency and ranker agents, demonstrating the effectiveness of the proposed method. 10 '},\n",
    " {'heading': '5 Conclusion',\n",
    "  'chunk_id': 6,\n",
    "  'content': ' Table 7: Ablation studies on the performance of CHASE- SQL after removing the query fixer, LSH for value re- trieval, and reasoning strategies, i.e., QP, OS, and DC. Method Execution Accuracy (%) ∆(%) CHASE-SQL All 73.01 - CHASE-SQL w self-consistency 68.84 -4.17 CHASE-SQL w ranker agent 65.51 -7.5 CHASE-SQL w/o LSH 70.09 -2.92 CHASE-SQL w/o Query Fixer 69.23 -3.78 CHASE-SQL w/o QP 72.36 -0.65 CHASE-SQL w/o OS 72.16 -0.85 CHASE-SQL w/o DC 71.77 -1.24 We introduce a novel agentic framework, CHASE-SQL, to leverage test-time compute for generating diverse, high-quality SQL queries and accurately selecting the correct one. We propose multiple chain-of-thought prompting methods and an online synthetic example generation technique, along with a query selection mechanism that scores candidates based on pairwise comparisons. Our framework, CHASE-SQL, sets a new state-of-the-art in the notable public Text-to-SQL leaderboard (at the time of the submission), demon- strating the effectiveness of test-time computation for both generating diverse queries and selecting the most accurate response. CHASE-SQL addresses key issues like query diversity and selection optimization, paving the way for further improvements in complex reasoning tasks encountered at real-world Text-to-SQL challenges. '},\n",
    " {'heading': 'Acknowledgments',\n",
    "  'chunk_id': 7,\n",
    "  'content': ' We would like to thank Per Jacobsson, Raj Sinha, Zeke Miller, Reza Sherkat, James Su, Zhixian Yan, David Culler, and Xiance Si, for their valuable comments and feedbacks on our paper. We would also like to thank the BIRD team for their invaluable assistance with the evaluation of the BIRD test set. '},\n",
    " {'heading': 'References',\n",
    "  'chunk_id': 7,\n",
    "  'content': ' Ion Androutsopoulos, Graeme D Ritchie, and Peter Thanisch. Natural language interfaces to databases–an introduction. Natural language engineering, 1(1):29–81, 1995. Ruichu Cai, Jinjie Yuan, Boyan Xu, and Zhifeng Hao. Sadga: Structure-aware dual graph aggregation network for text-to-sql. Advances in Neural Information Processing Systems, 34:7664–7676, 2021. Ruisheng Cao, Lu Chen, Zhi Chen, Yanbin Zhao, Su Zhu, and Kai Yu. Lgesql: line graph enhanced text-to-sql model with mixed local and non-local relations. arXiv preprint arXiv:2106.01093, 2021. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023. DongHyun Choi, Myeong Cheol Shin, EungGyun Kim, and Dong Ryeol Shin. Ryansql: Recursively applying sketch-based slot fillings for complex text-to-sql in cross-domain databases. Computational Linguistics, 47 (2):309–332, 2021. Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S Mirrokni. Locality-sensitive hashing scheme based on p-stable distributions. In Proceedings of the twentieth annual symposium on Computational geometry, pp. 253–262, 2004. Xuemei Dong, Chao Zhang, Yuhang Ge, Yuren Mao, Yunjun Gao, Jinshu Lin, Dongfang Lou, et al. C3: Zero-shot text-to-sql with chatgpt. arXiv preprint arXiv:2307.07306, 2023. Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, and Jingren Zhou. Text-to-sql empowered by large language models: A benchmark evaluation. arXiv preprint arXiv:2308.15363, 2023. Jonathan Herzig, Paweł Krzysztof Nowak, Thomas Müller, Francesco Piccinno, and Julian Martin Eisenschlos. Tapas: Weakly supervised table parsing via pre-training. arXiv preprint arXiv:2004.02349, 2020. 11 Wonseok Hwang, Jinyeong Yim, Seunghyun Park, and Minjoon Seo. A comprehensive exploration on wikisql with table-aware word contextualization. arXiv preprint arXiv:1902.01069, 2019. George Katsogiannis-Meimarakis and Georgia Koutrika. A survey on deep learning approaches for text-to-sql. The VLDB Journal, 32(4):905–936, 2023. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199–22213, 2022. Dongjun Lee, Choongwon Park, Jaehyuk Kim, and Heesoo Park. Mcs-sql: Leveraging multiple prompts and multiple-choice selection for text-to-sql generation. arXiv preprint arXiv:2405.07467, 2024. Boyan Li, Yuyu Luo, Chengliang Chai, Guoliang Li, and Nan Tang. The dawn of natural language to sql: Are we fully ready? arXiv preprint arXiv:2406.01265, 2024a. Fei Li and Hosagrahar V Jagadish. Constructing an interactive natural language interface for relational databases. Proceedings of the VLDB Endowment, 8(1):73–84, 2014. Haoyang Li, Jing Zhang, Cuiping Li, and Hong Chen. Resdsql: Decoupling schema linking and skeleton parsing for text-to-sql. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 13067–13075, 2023a. Haoyang Li, Jing Zhang, Hanbing Liu, Ju Fan, Xiaokang Zhang, Jun Zhu, Renjie Wei, Hongyan Pan, Cuiping Li, and Hong Chen. Codes: Towards building open-source language models for text-to-sql. Proceedings of the ACM on Management of Data, 2(3):1–28, 2024b. Jinyang Li, Binyuan Hui, Reynold Cheng, Bowen Qin, Chenhao Ma, Nan Huo, Fei Huang, Wenyu Du, Luo Si, and Yongbin Li. Graphix-t5: Mixing pre-trained transformers with graph-aware layers for text-to-sql parsing. arXiv preprint arXiv:2301.07507, 2023b. Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, et al. Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls. Advances in Neural Information Processing Systems, 36, 2024c. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):1092–1097, 2022. Karime Maamari, Fadhil Abubaker, Daniel Jaroslawicz, and Amine Mhedhbi. The death of schema linking? text-to-sql in the age of well-reasoned language models. arXiv preprint arXiv:2408.07702, 2024. Ansong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-tau Yih, Sida Wang, and Xi Victoria Lin. Lever: Learning to verify language-to-code generation with execution. In International Conference on Machine Learning, pp. 26106–26128. PMLR, 2023. Rubén Pérez-Mercado, Antonio Balderas, Andrés Muñoz, Juan Francisco Cabrera, Manuel Palomo-Duarte, and Juan Manuel Dodero. Chatbotsql: Conversational agent to support relational database query language learning. SoftwareX, 22:101346, 2023. Mohammadreza Pourreza and Davood Rafiei. Din-sql: Decomposed in-context learning of text-to-sql with self-correction. Advances in Neural Information Processing Systems, 36, 2024a. Mohammadreza Pourreza and Davood Rafiei. Dts-sql: Decomposed text-to-sql with small large language models. arXiv preprint arXiv:2402.01117, 2024b. Mohammadreza Pourreza, Ruoxi Sun, Hailong Li, Lesly Miculicich, Tomas Pfister, and Sercan O Arik. Sql-gen: Bridging the dialect gap for text-to-sql via synthetic data and model merging. arXiv preprint arXiv:2408.12733, 2024. 12 Abdul Quamar, Vasilis Efthymiou, Chuan Lei, and Fatma Özcan. Natural language interfaces to data. Found. Trends Databases, 11(4):319–414, 2022. doi: 10.1561/1900000078. URL https://doi.org/10. 1561/1900000078.'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from t2j.prem_sdk import PremSDK  \n",
    "from t2j.prompts import Prompts\n",
    "from t2j.chunker import DocumentChunker\n",
    "from t2j.utils import *\n",
    "\n",
    "import json\n",
    "\n",
    "N = 100\n",
    "promptsClass = Prompts()\n",
    "model = PremSDK()\n",
    "chunker = DocumentChunker(prompts=promptsClass, model=model)\n",
    "\n",
    "mapping, headings = chunker.create_content_mapping(final_headings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Databases might contain very high number of rows, with often only a few being relevant to a query. Retrieving relevant values is crucial as they can be used in various SQL clauses like ‘WHERE’ and ‘HAVING’. Similar to the approach in (Talaei et al., 2024), we begin by extracting keywords from the given question using an LLM prompted with few-shot examples. For each keyword, we employ locality-sensitive hashing (LSH) (Datar et al., 2004) to retrieve the most syntactically-similar words, and re-rank them based on embedding-based similarity and edit distance. This approach is robust to typos in the question and considers keyword semantics during retrieval. '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping['4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chunker.create_raw_json(headings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"1 Introduction\": {\n",
      "    \"content\": \"0\",\n",
      "    \"sub-headings\": []\n",
      "  },\n",
      "  \"2 Related Work\": {\n",
      "    \"content\": \"1\",\n",
      "    \"sub-headings\": []\n",
      "  },\n",
      "  \"3 Methods\": {\n",
      "    \"content\": \"2\",\n",
      "    \"sub-headings\": [\n",
      "      {\n",
      "        \"3.1 Overall Framework\": {\n",
      "          \"content\": \"3\",\n",
      "          \"sub-headings\": []\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"3.2 Value Retrieval\": {\n",
      "          \"content\": \"4\",\n",
      "          \"sub-headings\": []\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"3.3 Multi-path Candidate Generation\": {\n",
      "          \"content\": \"5\",\n",
      "          \"sub-headings\": [\n",
      "            {\n",
      "              \"Divide and Conquer CoT\": {\n",
      "                \"content\": \"6\",\n",
      "                \"sub-headings\": []\n",
      "              }\n",
      "            },\n",
      "            {\n",
      "              \"Query Plan CoT\": {\n",
      "                \"content\": \"7\",\n",
      "                \"sub-headings\": []\n",
      "              }\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"3.4 Query Fixer\": {\n",
      "          \"content\": \"9\",\n",
      "          \"sub-headings\": []\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"3.5 Selection Agent\": {\n",
      "          \"content\": \"10\",\n",
      "          \"sub-headings\": []\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"4 Online Synthetic Example Generation\": {\n",
      "    \"content\": \"8\",\n",
      "    \"sub-headings\": []\n",
      "  },\n",
      "  \"4 Experiments\": {\n",
      "    \"content\": \"11\",\n",
      "    \"sub-headings\": [\n",
      "      {\n",
      "        \"4.1 Datasets and Models\": {\n",
      "          \"content\": \"12\",\n",
      "          \"sub-headings\": []\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"4.2 BIRD results\": {\n",
      "          \"content\": \"13\",\n",
      "          \"sub-headings\": []\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"4.3 Spider results\": {\n",
      "          \"content\": \"14\",\n",
      "          \"sub-headings\": []\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"4.4 Generator and selection performance\": {\n",
      "          \"content\": \"15\",\n",
      "          \"sub-headings\": [\n",
      "            {\n",
      "              \"Selection\": {\n",
      "                \"content\": \"16\",\n",
      "                \"sub-headings\": []\n",
      "              }\n",
      "            },\n",
      "            {\n",
      "              \"Candidate Generation Analysis\": {\n",
      "                \"content\": \"17\",\n",
      "                \"sub-headings\": []\n",
      "              }\n",
      "            },\n",
      "            {\n",
      "              \"Selection Agent Analysis\": {\n",
      "                \"content\": \"18\",\n",
      "                \"sub-headings\": []\n",
      "              }\n",
      "            },\n",
      "            {\n",
      "              \"Query Plan\": {\n",
      "                \"content\": \"19\",\n",
      "                \"sub-headings\": []\n",
      "              }\n",
      "            },\n",
      "            {\n",
      "              \"Synthetic Example\": {\n",
      "                \"content\": \"20\",\n",
      "                \"sub-headings\": []\n",
      "              }\n",
      "            },\n",
      "            {\n",
      "              \"Divide and Conquer\": {\n",
      "                \"content\": \"21\",\n",
      "                \"sub-headings\": []\n",
      "              }\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"4.5 Ablation Studies\": {\n",
      "          \"content\": \"22\",\n",
      "          \"sub-headings\": []\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"5 Conclusion\": {\n",
      "    \"content\": \"23\",\n",
      "    \"sub-headings\": []\n",
      "  },\n",
      "  \"Acknowledgments\": {\n",
      "    \"content\": \"24\",\n",
      "    \"sub-headings\": []\n",
      "  },\n",
      "  \"References\": {\n",
      "    \"content\": \"25\",\n",
      "    \"sub-headings\": []\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(res, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1 Introduction': {'content': ' Text-to-SQL, as a bridge between human language and machine-readable structured query languages, is crucial for many use cases, converting natural language questions into executable SQL commands (Androutsopoulos et al., 1995; Li & Jagadish, 2014; Li et al., 2024c; Yu et al., 2018; ?). By enabling users to interact with complex database systems without requiring SQL proficiency, Text-to-SQL empowers users to extract valuable insights, perform streamlined data exploration, make informed decisions, generate data-driven reports and mine better features for machine learning (Chen et al., 2023; Pérez-Mercado et al., 2023; Pourreza & Rafiei, 2024a; Pourreza et al., 2024; Sun et al., 2023; Wang et al., 2019; Xie et al., 2023). Furthermore, Text-to-SQL systems play a pivotal role in automating data analytics with complex reasoning and powering conversational agents, expanding their applications beyond traditional data retrieval (Sun et al., 2023; Xie et al., 2023). As data continues to grow exponentially, the ability to query databases efficiently without extensive SQL knowledge becomes increasingly vital for a broad range of applications. Text-to-SQL can be considered a specialized form of code generation, with the contextual information potentially including the database schema, its metadata and along with the values. In the broader code generation domain, utilizing LLMs to generate a wide range of diverse candidates and select the best one has proven to be effective (Chen et al., 2021; Li et al., 2022; Ni et al., 2023). However, it is non-obvious what 1 arXiv:2410.01943v1  [cs.LG]  2 Oct 2024 leads to most effective candidate proposal and winner selector mechanisms. A straightforward yet effective approach involves generating candidates using zero-/few-shot or open-ended prompting, followed by selecting the best options utilizing self-consistency (Wang et al., 2022), which entails clustering candidates based on their execution outputs. This approach has demonstrated promising results in several studies (Lee et al., 2024; Maamari et al., 2024; Talaei et al., 2024; Wang et al., 2023). However, a single prompt design might not fully unleash the extensive Text-to-SQL knowledge of LLMs, and self-consistency methods might not be always effective. In fact, as illustrated in Table 1, the most consistent answers would not always be the correct ones, with an upper-bound performance 14% higher than that achieved through self-consistency. This substantial gap highlights the potential for significant improvement by implementing more effective selection methods to identify the best answer from the pool of candidate queries. Table 1: Evaluating single-query gen- eration vs. ensemble methods of self- consistency and the upper bound that can be achieved for Text-to-SQL with Gemini 1.5 Pro on the BIRD dev set. EX stands for execution accuracy. Method EX (%) Single query 63.01 Self-consistency 68.84 (+ 5.84) Upper-bound 82.79 (+ 19.78) Building on the challenges outlined in the previous section, we pro- pose novel approaches to improve LLM performance for Text-to-SQL by leveraging judiciously-designed test-time computations in an agentic framework. As indicated by the upper bound in Table 1, utilizing LLMs’ intrinsic knowledge offers significant potential for improvement. We propose methods that generate a diverse set of high-quality can- didate responses and apply a selection mechanism to identify the best answer. Achieving both high-quality and diverse candidate responses is critical for the success of scoring-based selection methods. Low diversity limits improvement potential and reduces the difference be- tween self-consistency and scoring-based approaches. While techniques like increasing temperature or reordering prompt contents can boost diversity, they often compromise the quality of the candidates. To address this, we introduce effective candidate generators designed to enhance diversity while maintaining high-quality outputs. Specifically, we propose three distinct candidate generation approaches, each capable of producing high-quality responses. The first is inspired by the divide-and-conquer algorithm, which breaks down complex problems into smaller, manageable parts to handle difficult queries. The second employs a query execution-plan-based chain-of-thought strategy, where the reasoning process mirrors the steps a database engine takes during query execution. Lastly, we introduce a novel online synthetic example generation method, which helps the model better understand the underlying data schema of the test database. These methods, when used independently, can produce highly-accurate SQL outputs. To effectively select the best answer among candidates, we introduce a selection agent, trained with a classification objective, that assigns scores based on pairwise comparisons between candidate queries. With this agent, we construct a comparison matrix for all candidates and select the final response based on the highest cumulative score. By combining these candidate generation methods with the proposed scoring model, we create an ensemble approach that leverages the strengths of each strategy to significantly improve overall performance. We present comprehensive evaluations on the efficacy of proposed methodologies of CHASE-SQL. Our innovative candidate generation approaches demonstrate superior performance compared to traditional generic CoT prompts, illustrating their capability in guiding LLMs through the decomposition of complex problems into manageable intermediate steps. Furthermore, the proposed selection agent significantly outperforms conventional consistency-based methods, contributing to the state-of-the-art results. Specifically, CHASE- SQL reaches an execution accuracy of 73.01% and 73.0% on the development set and test set of the challenging BIRD Text-to-SQL dataset which outperforms all of the published and undisclosed methods on this benchmark, by a large margin. ',\n",
       "  'sub-headings': []},\n",
       " '2 Related Work': {'content': ' Early Text-to-SQL methods predominantly utilized sequence-to-sequence architectures, encoding user queries and database schemas using models such as Graph Neural Networks (GNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and pre-trained transformer encoders (Cai et al., 2021; Cao et al., 2021; Hwang et al., 2019). On the decoding side, these systems employed either slot-filling or auto-regressive modelling approaches to construct the final SQL queries from the encoded inputs (Choi et al., 2021; Wang et al., 2019). Additionally, tabular language models like TaBERT (Yin et al., 2020), 2 TaPas (Herzig et al., 2020), and Grappa (Yu et al., 2020) have been developed to encode both tables and textual data effectively. However, the landscape has evolved with the widespread use of LLMs, which have largely replaced earlier methods with their superior performance (Katsogiannis-Meimarakis & Koutrika, 2023; Quamar et al., 2022). Initially, efforts concentrated on optimizing prompt designs for these LLMs (Dong et al., 2023; Gao et al., 2023; Pourreza & Rafiei, 2024a). Subsequent advancements have introduced more complex methodologies, including schema linking (Li et al., 2024b; Pourreza & Rafiei, 2024a,b; Talaei et al., 2024), self-correction or self-debugging (Chen et al., 2023; Talaei et al., 2024; Wang et al., 2023), and self-consistency techniques (Lee et al., 2024; Maamari et al., 2024; Sun et al., 2023; Talaei et al., 2024), further enhancing the performance by proposing complex LLM-based pipelines. ',\n",
       "  'sub-headings': []},\n",
       " '3 Methods': {'content': ' ',\n",
       "  'sub-headings': [{'3.1 Overall Framework': {'content': ' This section outlines the proposed CHASE-SQL framework, which consists of four primary components: 1) Value retrieval, 2) Candidate generator, 3) Query fixer, and 4) Selection agent. As illustrated in Fig. 1. The proposed framework begins by retrieving relevant database values. Subsequently, all contextual information, including retrieved values, database metadata, and schema, is provided to an LLM to generate candidate queries. These candidate queries then undergo a fixing loop, and finally, all candidates are compared in a pairwise way using the trained selection agent to pick the correct answer. The following sections delve into the details of each component. Figure 1: Overview of the proposed CHASE-SQL framework for Text-to-SQL, with value retrieval and using a selection agent for improve picking of the answers among the generated candidates along with a fixer to provide feedback for refinement of the outputs. ',\n",
       "     'sub-headings': []}},\n",
       "   {'3.2 Value Retrieval': {'content': ' Databases might contain very high number of rows, with often only a few being relevant to a query. Retrieving relevant values is crucial as they can be used in various SQL clauses like ‘WHERE’ and ‘HAVING’. Similar to the approach in (Talaei et al., 2024), we begin by extracting keywords from the given question using an LLM prompted with few-shot examples. For each keyword, we employ locality-sensitive hashing (LSH) (Datar et al., 2004) to retrieve the most syntactically-similar words, and re-rank them based on embedding-based similarity and edit distance. This approach is robust to typos in the question and considers keyword semantics during retrieval. ',\n",
       "     'sub-headings': []}},\n",
       "   {'3.3 Multi-path Candidate Generation': {'content': ' As shown in Table 1, relying solely on consistency among responses can lead to sub-optimal performance. Therefore, we prioritize diversity in generation of multiple response candidates to increase the likelihood of generating at least one correct answer. Among the diverse responses generated by the candidate generators, we select one as the final response using a selection agent that compares candidates pairwise. To generate 3 diverse responses, we increase the next token sampling temperature, and also shuffle the order of columns and tables in the prompt. Chain-of-Thought (CoT) prompting (Wei et al., 2022) has been proposed to enhance LLMs’ reasoning abilities by conditioning their final responses on a step-by-step chain of reasoning. Most CoT prompting approaches rely on few-shot examples in the prompt to guide LLMs on thinking step-by-step, following the format M = (qi, ri, si), where qi is the example question, ri is the reasoning path, and si is the ground truth SQL query for qi. We employ two distinct reasoning methods and an online synthetic example generation approach. As shown in Fig. 3a, different generators can yield different outputs, indicating their effectiveness for specific questions and databases. ',\n",
       "     'sub-headings': [{'Divide and Conquer CoT': {'content': ': Divide-and-conquer perspective brings breaking down complex problems into smaller sub-problems, solving each individually, and then combining the solutions to obtain the final answer. Along these lines, we propose a CoT prompting approach that first decomposes the given question into smaller sub-problems using pseudo-SQL queries. In the ’conquer’ step, the solutions to these sub-problems are aggregated to construct the final answer. Finally, an optimization step is applied to the constructed query to remove redundant clauses and conditions. This approach is particularly powerful handling complex scenarios that involve nested queries, e.g. intricate WHERE or HAVING conditions, and queries requiring advanced mathematical operations. In Appendix Fig. 17, we exemplify a question and its corresponding SQL query that was successfully solved using this generator, a scenario the other methods considered in this paper could not address due to the query’s complex conditions and SQL clauses. For a more detailed view of the divide-and-conquer prompt, please see Appendix Fig. 16. Additionally, Alg. 1 outlines the step-by-step process of this strategy to generate the final SQL output using a single LLM call. Algorithm 1 Divide and Conquer Chain-of-Thought (CoT) Strategy for Text-to-SQL. Input: Set of human-annotated few-shot examples M, user question Qu, target database D associated with the question, and a large language model (LLM) θ. Divide: 1: Sq ←θ(M, D, Qu) // Decompose the original question Qu into a set of sub-questions Sq 2: Ssql ←∅// Initialize an empty set Ssql to store partial SQL queries for each sub-question Conquer: 3: for each sub-question qi in Sq do 4: // Generate a partial SQL query for each sub-question qi 5: Ssql ←Ssql ∪{θ(M, D, Qu, q1, ..., qi, sql1, ..., sqli−1)} 6: end for Assemble: 7: Sf ←θ(M, D, Qu, Sq, Ssql) // Assemble the final SQL query Sf from all sub-queries in Ssql 8: return Sf ',\n",
       "        'sub-headings': []}},\n",
       "      {'Query Plan CoT': {'content': ': A query (execution) plan is a sequence of steps that the database engine follows to access or modify the data described by a SQL command. When a SQL query is executed, the database management systems’ query optimizers translate the SQL text into a query plan that the database engine can execute. This plan outlines how tables are accessed, how they are joined, and the specific operations performed on the data (see Appendix Fig. 19 as an example). Inspired by the step-by-step process that database engines use to execute SQL queries, we propose a reasoning strategy to construct the final SQL output. Query plans for any given SQL query can be obtained using the “EXPLAIN\" command, which provides a detailed breakdown of execution steps. However, this output is often presented in a format that is difficult to interpret by LLMs (e.g. in SQLite). To address this, we convert the output of “EXPLAIN\" command into a human-readable text format that aligns more closely with the pretraining data of LLMs. The human-readable version of query plans consists of three key steps: (1) identifying and locating the relevant tables for the question, (2) performing operations such as counting, filtering, or matching between tables, and (3) delivering the final result by selecting the appropriate columns to return. This reasoning method complements the divide-and-conquer CoT strategy. While the divide-and-conquer approach is better suited for decomposing complex questions, the query plan approach excels when questions require more reasoning over the relationships between different parts of the question and the database schema. It systematically explains which tables to scan, how to match columns, and how to apply filters. Appendix Fig. 20 shows an example of a question that was answered correctly only by this method. Appendix Fig. 18 provides the prompt used for this reasoning strategy. ',\n",
       "        'sub-headings': []}}]}},\n",
       "   {'3.4 Query Fixer': {'content': ' In some cases, LLMs might generate queries that are syntactically incorrect. These queries are clear candidates for correction, as they fail to provide the correct answers. To address this, we apply an LLM-based query fixer that leverages the self-reflection (Shinn et al., 2024) method. The fixer reflects on the previously generated query, using feedback such as syntax error details or empty result sets to guide the correction process. We continue this iterative fixing approach up to a specified number of attempts, β (set to three in this paper). Appendix Fig. 21 demonstrates the prompt used for this query fixing step. ',\n",
       "     'sub-headings': []}},\n",
       "   {'3.5 Selection Agent': {'content': ' With three different methods for generating SQL queries, we can generate a set of candidate queries for any given question. The key challenge in this step is selecting the correct SQL query from this pool of candidates. A naive approach would be to measure consistency among the candidates by executing them, grouping them based on their execution results, and selecting a query from the largest group as the most likely correct answer. However, this would assume that the most consistent answer is always the best one, which is not always the case. Instead, we propose a more refined picking strategy, Algorithm 3, that relies on a selection agent. Given a set of candidates SQL queries C = {c1, c2, ..., cn}, the final responses are selected by finding 5 the candidate that has the highest score assigned by the selection model. This model θp can take k candidates and rank them based on how accurately each of them answers the given question. Concretely, we formulate the selection of the final response as: cf = arg max c∈C \\uf8eb \\uf8ec \\uf8ed (n k) X i=1 θp(ci1, . . . , cik | Qu, Hu, D) \\uf8f6 \\uf8f7 \\uf8f8, (1) where Qu refers to the user’s question, Hu is the provided hint, and D is the target database from which the question is being asked. In Eq. 1, we pass k candidates to the selection model to be ranked, with k being between 1 and n. In the extreme case of k = 1, the model is unable to make comparisons between candidates, which complicates the evaluation process for the model. As k increases, comparing more candidates makes the process more challenging for the model, as it needs to consider different aspects simultaneously. Consequently, we set k = 2 and train a model with a classification objective to compare only two candidates at a time. Having a set of high-quality and diverse candidates, the most straightforward solution is to employ off-the-shelf LLMs to make pairwise selections. However, experiments with Gemini-1.5-pro showed that using the LLM without fine-tuning resulted in only 58.01% binary classification accuracy. This is primarily due to the candidates being very similar to one another, requiring a fine-tuned model to learn the nuances and make more accurate decisions. To train the selection agent, we first generate candidate SQL queries on the training set (of Text-to-SQL benchmarks), and group them into clusters based on their execution results. For cases where at least one cluster contains correct queries and others contains incorrect ones, we create training examples in the form of tuples (Qu, Ci, Cj, Dij, yij), where Qu is the user’s question, Ci and Cj are the two candidate queries being compared, Dij is the database schema used by both candidates, and yij ∈0, 1 is the label indicating whether Ci or Cj is the correct query. To avoid order bias during training, we randomly shuffle the order of correct and incorrect queries in each pair. Since the number of cases with both correct and incorrect candidates is limited, for instances where no correct candidate exists, we include the ground truth SQL query in the prompt as a hint to guide the model in generating correct candidates. Algorithm 3 Picking the final SQL query from a pool of candidates. Input: Set of candidate SQL queries C = {c1, c2, ..., cn}, user question Qu, hint Hu, target database D, and a selection model θp, er(ci, D) as the execution result of ci on D 1: ri ←0 for all ci ∈C // Initialize the score ri for each candidate query to zero 2: for each distinct pair (ci, cj) where i ̸= j do 3: if er(ci, D) = er(cj, D) then 4: w ←i // ci is the winner if the execution results match 5: else 6: Si,j ←schema_union(ci, cj, D) // Construct union of schemas used in ci and cj 7: w ←θp(Si,j, Qu, Hu, ci, cj)w ∈{i, j} // Use binary classifier θp to select the winner, w ∈{i, j} 8: end if 9: rw ←rw + 1 // Increase the score of the winner cw by 1 10: end for 11: cf ←arg maxci∈C ri // Select the candidate with the highest score as the final SQL query cf 12: return cf In the pseudo-code for Algorithm 3, we begin by initializing a score of zero for each candidate query. Then, for every distinct pair of queries (ci, cj), we compare both (ci, cj) and (cj, ci) to mitigate any order bias, ensuring that both candidates in a pair are fairly evaluated. If both queries produce the same execution result on the database, we mark one as the winner and increment its score, as these results suggest consistency. If the execution results differ, we generate a union of the schema used by both queries and use the binary classifier to determine which query is more likely to be correct. The classifier takes into account the question, the two candidate queries, and the combined schema to make its decision. The winner’s score is then updated accordingly. After all comparisons, the candidate with the highest score is selected as the final query. In the rare case of a tie in the final scores, we break the tie by selecting one of the candidates arbitrarily. 6 ',\n",
       "     'sub-headings': []}}]},\n",
       " '4 Online Synthetic Example Generation': {'content': ': Using M demonstrations for few-shot in-context learning has shown promising results on various related tasks (Pourreza & Rafiei, 2024a). Besides helping with specifying the task and illustrate the step-by-step process deriving the output, demonstrations constructed using relevant tables and columns can also help the model understand the underlying data schema. Based on this insight, we propose a synthetic demonstration generation strategy for Text-to-SQL – given the user question Qu, the target database D, and the selected columns ti (using a column selection approach similar to (Talaei et al., 2024)). Algorithm 2 Online Synthetic example generation strategy for Text-to-SQL. Input: User question Qu, additional user hint Hu, target database D and filtered relevant table columns t associated with the question, LLM θ, guidelines Rf for generating examples by SQL features, guidelines Rt for generating examples with filtered schema, and the numbers of examples to generate nf, nt respectively 1: P ←∅// {(qi, si) | qi, si ∈Σ∗}, where qi is input question , si is output SQL for the i-th example 2: P ←P ∪{θ(D, Rf, nf)} // Generate n examples with entire database by common SQL features 3: P ←P ∪{θ(t, Rt, nt)} // Generate examples with filtered columns to highlight correct schema usage 4: return P Algorithm 2 outlines the online synthetic example generation approach with two LLM generation steps. The first step focuses on generating illustrative examples with common SQL features described in the guideline Rf. The SQL features include equality and non-equality predicates, single table and multi-table JOIN, nested JOIN, ORDER BY and LIMIT, GROUP BY and HAVING, various aggregation functions. These are widely applicable SQL clauses and functions – the generated example SQL queries, incorporating these features, follow the BIRD SQL feature distribution (Appendix Fig 23a). The second step focuses on generating examples highlighting correct interpretation of the underlying data schema – the model θ is asked to generate examples using ti and that are similar to the examples outlined in Rt. Appendix A.10 provides the prompts used for the example generation). While a relevant example (e.g. showing a nested JOIN query with multiple tables) can be helpful for questions that require complex JOIN queries, it might also mislead the LLM for overuse (e.g. when a simple single table query is sufficient). This and the inherent ambiguity of natural language query qi, for which we draw the examples by relevance, make the example selection challenging. Thus, we generate and inject the examples to the prompt online per qi. We ask the LLM to generate many input-output pairs for in-context learning. The final set of synthetic examples for qi contains examples generated with both Rf and Rt. This ensures that the example set is diverse both in SQL features/clauses and the choice of relevant tables/columns used. The diversity of the example set is desirable to avoid over-fitting the output to certain patterns (e.g., the model always writes a SQL with JOIN if shown mostly JOIN examples). Mixing various examples for various SQL features and database tables with and without column filtering is observed to result in better generation quality overall (please see Appendix Table 8). ',\n",
       "  'sub-headings': []},\n",
       " '4 Experiments': {'content': ' ',\n",
       "  'sub-headings': [{'4.1 Datasets and Models': {'content': ' We evaluate the performance of the proposed CHASE-SQL framework on two widely-recognized cross-domain datasets: BIRD (Li et al., 2024c) and Spider (Yu et al., 2018). BIRD contains over 12,751 unique question-SQL pairs from 95 large databases, spanning more than 37 professional domains, with databases designed to resemble real-world scenarios, featuring messy data rows and complex schemas. Spider consists of 10,181 questions and 5,693 unique complex SQL queries across 200 databases, covering 138 domains. The Spider dataset is divided into non-overlapping training, development, and test sets similar to BIRD. For both, we use execution accuracy (EX), the official metric for their respective leaderboard, as the primary evaluation metric to compare methods. Details of the models and their hyperparameters are provided in Appendix section A.2. ',\n",
       "     'sub-headings': []}},\n",
       "   {'4.2 BIRD results': {'content': ' We present the end-to-end Text-to-SQL performance of the proposed CHASE-SQL framework using Claude- 3.5-sonnet and Gemini 1.5 pro on the BIRD development set, and Gemini 1.5 pro on the BIRD test set. We compare with both published methods (either with an available codebase and/or paper) and undisclosed methods. For a fair comparison with Gemini 1.5 pro, all LLM calls in the Claude-3.5-sonnet setting, except for the selection model, are made using Claude-3.5-sonnet (previously-trained selection model is reused). As shown in Table 2, CHASE-SQL with Gemini 1.5 pro achieves 73.01% accuracy on the BIRD development set and 73.0% on the BIRD holdout test set, outperforming all previous works and setting a new state-of-the-art performance. Table 2: Performance Comparison of different Text-to-SQL methods on BIRD benchmark. Method EX (Dev) EX (Test) Published CHASE-SQL + Gemini 1.5 (Ours) 73.01 73.0 CHASE-SQL + Claude 3.5 Sonnet (Ours) 69.53 – Distillery + GPT-4o (Maamari et al., 2024) 67.21 71.83 CHESS (Talaei et al., 2024) 65.00 66.69 MCS-SQL + GPT-4 (Lee et al., 2024) 63.36 65.45 SuperSQL (Li et al., 2024a) 58.5 62.66 Undisclosed Insights AI 72.16 70.26 AskData + GPT-4o 72.03 72.39 OpenSearch-v2 + GPT-4o 69.3 72.28 PURPLE-RED + GPT-4o 68.12 70.21 Arcwise + GPT-4o 67.99 66.21 ExSL + granite-34b-code 67.47 67.76 Table 3: Performance Comparison of different Text-to-SQL methods on Spider test set. Method EX Training with Spider MCS-SQL + GPT-4 (Lee et al., 2024) 89.6 ✓ CHASE-SQL + Gemini 1.5 (Ours) 87.6 ✗ CHESS (Talaei et al., 2024) 87.2 ✗ DAIL-SQL + GPT-4 (Gao et al., 2023) 86.6 ✓ DIN-SQL + GPT-4 (Pourreza & Rafiei, 2024a) 85.3 ✓ C3 + ChatGPT (Dong et al., 2023) 82.3 ✓ RESDSQL 3B (Li et al., 2023a) 79.9 ✓ DIN-SQL + CodeX (Pourreza & Rafiei, 2024a) 78.2 ✓ T5-3B+NatSQL (Rai et al., 2023) 78.0 ✓ Graphix-3B+PICARD (Li et al., 2023b) 77.6 ✓ ',\n",
       "     'sub-headings': []}},\n",
       "   {'4.3 Spider results': {'content': ' We assess the generalizability of the proposed CHASE-SQL by evaluating it in an end-to-end way on the Spider test set without modifying the few-shot samples in the prompts or training a new selection model, i.e. without using and data from the target distribution. This approach allows us to test the performance of CHASE-SQL on different unseen query and database distributions compared to the data from training distributions. Table 3 demonstrates that CHASE-SQL achieves an execution accuracy of 87.6% on the Spider test set, placing it second among methods that have undergone specific training or prompt optimization for the Spider dataset. This highlights the strong generalizability of CHASE-SQL and its potential for generating high quality Text-to-SQL for unseen samples coming from very different distributions and unique challenges. 7 ',\n",
       "     'sub-headings': []}},\n",
       "   {'4.4 Generator and selection performance': {'content': ' Table 4: Ablation studies on single candidate gen- eration performance of the candidate generators compared with original BIRD prompt + zero-shot CoT with Gemini 1.5 pro on the BIRD dev set. Selector is our final performance applying the se- lection agent to 21 candidates generated from all generators. Method Execution Accuracy (%) ∆(%) Baseline 57.75 - QP CoT 63.62 +5.87 DC CoT 63.92 +6.17 OS CoT 67.09 +9.34 Baseline w Query Fixer 61.58 +3.83 QP CoT w Query Fixer 65.51 +7.76 DC CoT w Query Fixer 65.77 +8.02 OS CoT w Query Fixer 68.02 +10.27 Generator + Fixer: To reveal performance of genera- tors, we conducted an ablation study to evaluate the per- formance of each candidate generation method before and after applying the query fixer. We compare the performance of the proposed generators in producing a single candidate query against the original BIRD prompt (Li et al., 2024c), augmented with zero-shot CoT reasoning (Kojima et al., 2022), which serves as the baseline for assessing the quality of prompts. The results, shown in Table 4, indicate that the proposed methods significantly improve SQL generation performance, compared to the naive baseline, towards the goal of producing high-quality candidates while maintain- ing diversity. Among the candidate generators, the online synthetic data generation approach produced an impressive performance of 68.02%, demonstrating its effectiveness in leveraging test-time compute to improve LLM performance by generating high-quality synthetic examples. Furthermore, the query fixer proved crucial, enhancing the quality of the candidate pool and increasing performance by nearly 2% across all candidate generators. Table 5: Evaluating the binary selection accuracy of the different selection models. ',\n",
       "     'sub-headings': [{'Selection': {'content': ' Model Binary Acc. (%) Claude-3.5-sonnet 60.21 Gemini-1.5-pro 63.98 Tuned Gemma 2 9B 64.28 Tuned Gemini-1.5-flash 71.01 Selection: We conducted an analysis on the binary selection accu- racy of the selection agent for cases where, in a pairwise comparison, one candidate is correct and the other is incorrect. We exclude cases where both candidates are either correct or incorrect, as the selection would not affect the outcome since both candidates have the same label. We compare the performance of Claude-3.5-sonnet and Gemini-1.5-pro (both out-of-the-box without fine-tuning) with two fine-tuned models: 1) Gemma 2 9B and 2) Gemini-1.5-flash. As shown in Table 5, both fine-tuned models achieve higher accuracy than the untuned counterparts, demonstrating the importance of fine-tuning to teach the model about the specific preferences. ',\n",
       "        'sub-headings': []}},\n",
       "      {'Candidate Generation Analysis': {'content': ': We analyze the performance of each candidate generator method individually. To better understand the performance potential when effectively selecting the correct SQL query from the candidate pool, we generate seven candidate SQL queries from each generator method (21 candidates in total) for all samples in the BIRD development set. We determine this number of candidates based on the observation that increasing the candidate pool beyond 20 did not yield significant improvements, as illustrated in Fig. 2d. By assuming access to an oracle selection model that always selects the correct SQL query from the seven candidates, we calculate the upper-bound performance achievable for each generator. Conversely, by assuming an adversarial selection model that always selects the wrong SQL query, we determine the lower-bound performance. Fig. 2 illustrates the upper-bound and lower-bound performance for all three methods together with the performance of our selection agent. As shown, the upper-bound performance of the two different CoT methods is generally higher than that of the synthetic example generation method for different number of candidates. However, their lower-bound performance is also lower than the synthetic method. Lower-bound accuracy reflects cases where all candidates are correct, reducing the noise in the selection process since it doesn’t matter which candidate is chosen, so a higher lower-bound is preferred. This is evident in the selection agent’s performance, where a drop in the lower bound leads to diminishing returns from increasing the upper bound, causing the selection agent’s performance to plateau. Additionally, the upper-bound performance of combining all three methods reaches 82.79%, highlighting the significant room for improvement through better candidate picking methods. This demonstrates that the LLM’s parametric knowledge already contains the information needed to solve most questions, highlighting the need for ensemble approaches to effectively extract and utilize this knowledge. 8 (a) Upper-bound and lower-bound Accuracy for Divide and Conquer CoT (b) Upper-bound and lower-bound Accuracy for Online Synthetic Example (c) Upper-bound and lower-bound performance for Query Plan CoT. (d) Upper-bound performance of all three can- didate generators across different difficulty cat- egories. Figure 2: Comparison of the upper- and lower-bound performance of different candidate generators. Additionally, we evaluate the upper-bound performance by combining all candidates from three candidate generation methods across the simple, moderate, and challenging difficulty levels for the BIRD development set. These difficulty categories are assigned by human experts during the creation of the BIRD development set. Fig. 2d shows that, as expected, the upper-bound performance increases with the number of candidates across all difficulty levels. However, for the challenging and moderate classes, the improvement plateaus earlier than in the simple class, suggesting that generating more samples does not further improve the upper-bound performance for these two difficulty levels. Fig. 2 presents a Venn diagram showcasing the performance of three generation methods: Query Plan, Divide and Conquer, and with Synthetic Examples. The numbers within the intersecting regions represent the instances where multiple methods generated at least one correct candidate. This diagram visually highlights the unique contributions of each method, which indicates the necessity of using all three generators. Additionally, in Fig. 3b, we compare the number of correct queries generated by each SQL generation method that are not correct by the other generators. The divide-and-conquer approach outperforms the others on challenging questions, while the query plan method excels on moderately difficult queries. To further analyze the performance of the generators across different domains and varying numbers of columns and tables, we compare the number of correct queries generated for each database, as shown in Appendix Fig. 4. As illustrated, both CoT methods generally perform similarly across databases, while the online synthetic example generation method significantly increases diversity, resulting in more correct answers overall across different databases. ',\n",
       "        'sub-headings': []}},\n",
       "      {'Selection Agent Analysis': {'content': ': We evaluate the query-picking performance by comparing the Text-to-SQL execution accuracy of the selection agent with the self-consistency method (using majority voting) Wang et al. (2022), an oracle model (upper bound), and an adversarial model (lower bound). To conduct the evaluation, we generate 10 samples from each candidate generation method using two different sampling temperatures: 0.5 and 1.8. The results, shown in Table 6, demonstrate that the selection agent significantly outperforms the self-consistency method with a large margin, roughly 6%. As expected, increasing the sampling temperature 9 35 30 33 38 72 23 1045 ',\n",
       "        'sub-headings': []}},\n",
       "      {'Query Plan': {'content': ' ', 'sub-headings': []}},\n",
       "      {'Synthetic Example': {'content': ' ', 'sub-headings': []}},\n",
       "      {'Divide and Conquer': {'content': ' Unsolved Questions: 258 (a) Venn diagram illustrating the num- ber of instances for which each method: Query Plan, Synthetic Example, Divide and Conquer, produces at least one cor- rect candidate. The overlap regions repre- sent multiple methods generating correct candidates. (b) Number of correct queries across different complexity levels that were answered by each method. Figure 3: Comparison of SQL generation methods: Venn diagram showing unique and overlapping correct answers (left) and the performance across different complexity levels (right). raises the upper bound but also lowers the lower bound. This effect is more pronounced for the synthetic data generation method compared to the two CoT methods, mainly because LLMs generate reasoning steps before producing the final SQL query, which helps mitigate the randomness introduced by high-temperature sampling. The performance with self-consistency method generally decreases as temperature increases, since the majority cluster becomes smaller with more random queries. However, the proposed trained selection agent is less affected by temperature scaling and, in two cases, even improved its performance with a more diverse pool of samples. Table 6: Performance comparison of different picking methods on the candidates generated by the candidate generators on BIRD development set with two different temperatures. QP refers to query plan COT, DC refers to divide and conquer COT, and OS is the online synthetic example generation method. Picking Method QP (T=0.5) QP (T=1.8) DC (T=0.5) DC (T=1.8) OS (T=0.5) OS (T=1.8) Lower Bound 50.46 48.63 51.37 47.39 60.43 50.98 Upper Bound 78.55 80.44 78.42 79.34 74.77 79.66 Self-consistency 65.78 65.51 66.43 64.41 67.34 66.88 Our Selection Agent 71.7 71.73 71.31 70.53 70.4 71.38 ',\n",
       "        'sub-headings': []}}]}},\n",
       "   {'4.5 Ablation Studies': {'content': ' In the previous sections, we evaluate the importance of the selection agent and each candidate generation method. Next, we focus on the analysis of the remaining components of CHASE-SQL: LSH for value retrieval, the query fixer, and three reasoning strategies (QP, OS, and DC). Table 7 shows the performance of CHASE- SQL without each of these steps, highlighting their significance in achieving higher-quality performance. The results demonstrate the contribution of each component, where removing LSH, the query fixer, or any of the candidate generators leads to a reduction in execution accuracy, further validating the importance of these components of CHASE-SQL. Moreover, the table compares the performance of our binary selection agent with two other selection methods: self-consistency (Wang et al., 2022) and a ranker agent. The ranker agent receives all candidates generated by our three candidate generators in a single prompt, compares them, and produce a ranking for each. For the ranker agent, we select the query with the lowest rank as the best answer. The binary selection agent significantly outperforms both the self-consistency and ranker agents, demonstrating the effectiveness of the proposed method. 10 ',\n",
       "     'sub-headings': []}}]},\n",
       " '5 Conclusion': {'content': ' Table 7: Ablation studies on the performance of CHASE- SQL after removing the query fixer, LSH for value re- trieval, and reasoning strategies, i.e., QP, OS, and DC. Method Execution Accuracy (%) ∆(%) CHASE-SQL All 73.01 - CHASE-SQL w self-consistency 68.84 -4.17 CHASE-SQL w ranker agent 65.51 -7.5 CHASE-SQL w/o LSH 70.09 -2.92 CHASE-SQL w/o Query Fixer 69.23 -3.78 CHASE-SQL w/o QP 72.36 -0.65 CHASE-SQL w/o OS 72.16 -0.85 CHASE-SQL w/o DC 71.77 -1.24 We introduce a novel agentic framework, CHASE-SQL, to leverage test-time compute for generating diverse, high-quality SQL queries and accurately selecting the correct one. We propose multiple chain-of-thought prompting methods and an online synthetic example generation technique, along with a query selection mechanism that scores candidates based on pairwise comparisons. Our framework, CHASE-SQL, sets a new state-of-the-art in the notable public Text-to-SQL leaderboard (at the time of the submission), demon- strating the effectiveness of test-time computation for both generating diverse queries and selecting the most accurate response. CHASE-SQL addresses key issues like query diversity and selection optimization, paving the way for further improvements in complex reasoning tasks encountered at real-world Text-to-SQL challenges. ',\n",
       "  'sub-headings': []},\n",
       " 'Acknowledgments': {'content': ' We would like to thank Per Jacobsson, Raj Sinha, Zeke Miller, Reza Sherkat, James Su, Zhixian Yan, David Culler, and Xiance Si, for their valuable comments and feedbacks on our paper. We would also like to thank the BIRD team for their invaluable assistance with the evaluation of the BIRD test set. ',\n",
       "  'sub-headings': []},\n",
       " 'References': {'content': ' Ion Androutsopoulos, Graeme D Ritchie, and Peter Thanisch. Natural language interfaces to databases–an introduction. Natural language engineering, 1(1):29–81, 1995. Ruichu Cai, Jinjie Yuan, Boyan Xu, and Zhifeng Hao. Sadga: Structure-aware dual graph aggregation network for text-to-sql. Advances in Neural Information Processing Systems, 34:7664–7676, 2021. Ruisheng Cao, Lu Chen, Zhi Chen, Yanbin Zhao, Su Zhu, and Kai Yu. Lgesql: line graph enhanced text-to-sql model with mixed local and non-local relations. arXiv preprint arXiv:2106.01093, 2021. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023. DongHyun Choi, Myeong Cheol Shin, EungGyun Kim, and Dong Ryeol Shin. Ryansql: Recursively applying sketch-based slot fillings for complex text-to-sql in cross-domain databases. Computational Linguistics, 47 (2):309–332, 2021. Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S Mirrokni. Locality-sensitive hashing scheme based on p-stable distributions. In Proceedings of the twentieth annual symposium on Computational geometry, pp. 253–262, 2004. Xuemei Dong, Chao Zhang, Yuhang Ge, Yuren Mao, Yunjun Gao, Jinshu Lin, Dongfang Lou, et al. C3: Zero-shot text-to-sql with chatgpt. arXiv preprint arXiv:2307.07306, 2023. Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, and Jingren Zhou. Text-to-sql empowered by large language models: A benchmark evaluation. arXiv preprint arXiv:2308.15363, 2023. Jonathan Herzig, Paweł Krzysztof Nowak, Thomas Müller, Francesco Piccinno, and Julian Martin Eisenschlos. Tapas: Weakly supervised table parsing via pre-training. arXiv preprint arXiv:2004.02349, 2020. 11 Wonseok Hwang, Jinyeong Yim, Seunghyun Park, and Minjoon Seo. A comprehensive exploration on wikisql with table-aware word contextualization. arXiv preprint arXiv:1902.01069, 2019. George Katsogiannis-Meimarakis and Georgia Koutrika. A survey on deep learning approaches for text-to-sql. The VLDB Journal, 32(4):905–936, 2023. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199–22213, 2022. Dongjun Lee, Choongwon Park, Jaehyuk Kim, and Heesoo Park. Mcs-sql: Leveraging multiple prompts and multiple-choice selection for text-to-sql generation. arXiv preprint arXiv:2405.07467, 2024. Boyan Li, Yuyu Luo, Chengliang Chai, Guoliang Li, and Nan Tang. The dawn of natural language to sql: Are we fully ready? arXiv preprint arXiv:2406.01265, 2024a. Fei Li and Hosagrahar V Jagadish. Constructing an interactive natural language interface for relational databases. Proceedings of the VLDB Endowment, 8(1):73–84, 2014. Haoyang Li, Jing Zhang, Cuiping Li, and Hong Chen. Resdsql: Decoupling schema linking and skeleton parsing for text-to-sql. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 13067–13075, 2023a. Haoyang Li, Jing Zhang, Hanbing Liu, Ju Fan, Xiaokang Zhang, Jun Zhu, Renjie Wei, Hongyan Pan, Cuiping Li, and Hong Chen. Codes: Towards building open-source language models for text-to-sql. Proceedings of the ACM on Management of Data, 2(3):1–28, 2024b. Jinyang Li, Binyuan Hui, Reynold Cheng, Bowen Qin, Chenhao Ma, Nan Huo, Fei Huang, Wenyu Du, Luo Si, and Yongbin Li. Graphix-t5: Mixing pre-trained transformers with graph-aware layers for text-to-sql parsing. arXiv preprint arXiv:2301.07507, 2023b. Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, et al. Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls. Advances in Neural Information Processing Systems, 36, 2024c. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):1092–1097, 2022. Karime Maamari, Fadhil Abubaker, Daniel Jaroslawicz, and Amine Mhedhbi. The death of schema linking? text-to-sql in the age of well-reasoned language models. arXiv preprint arXiv:2408.07702, 2024. Ansong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-tau Yih, Sida Wang, and Xi Victoria Lin. Lever: Learning to verify language-to-code generation with execution. In International Conference on Machine Learning, pp. 26106–26128. PMLR, 2023. Rubén Pérez-Mercado, Antonio Balderas, Andrés Muñoz, Juan Francisco Cabrera, Manuel Palomo-Duarte, and Juan Manuel Dodero. Chatbotsql: Conversational agent to support relational database query language learning. SoftwareX, 22:101346, 2023. Mohammadreza Pourreza and Davood Rafiei. Din-sql: Decomposed in-context learning of text-to-sql with self-correction. Advances in Neural Information Processing Systems, 36, 2024a. Mohammadreza Pourreza and Davood Rafiei. Dts-sql: Decomposed text-to-sql with small large language models. arXiv preprint arXiv:2402.01117, 2024b. Mohammadreza Pourreza, Ruoxi Sun, Hailong Li, Lesly Miculicich, Tomas Pfister, and Sercan O Arik. Sql-gen: Bridging the dialect gap for text-to-sql via synthetic data and model merging. arXiv preprint arXiv:2408.12733, 2024. 12 Abdul Quamar, Vasilis Efthymiou, Chuan Lei, and Fatma Özcan. Natural language interfaces to data. Found. Trends Databases, 11(4):319–414, 2022. doi: 10.1561/1900000078. URL https://doi.org/10. 1561/1900000078.',\n",
       "  'sub-headings': []}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replace_content(data, content_map):\n",
    "    for key, value in data.items():\n",
    "        # Replace content\n",
    "        if 'content' in value:\n",
    "            content_id = value['content']\n",
    "            if content_id in content_map:\n",
    "                value['content'] = content_map[content_id]\n",
    "\n",
    "        # Recurse into sub-headings\n",
    "        if 'sub-headings' in value and isinstance(value['sub-headings'], list):\n",
    "            for sub_item in value['sub-headings']:\n",
    "                for sub_key, sub_value in sub_item.items():\n",
    "                    replace_content({sub_key: sub_value}, content_map)\n",
    "                    \n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASDASD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trunk 0  title\n",
      " trunk 0  related_work[]\n",
      " trunk 0  authors[]\n",
      " trunk 0  abstract\n",
      " trunk 0  approach[]\n",
      "branch 1 ──── approach_name\n",
      "branch 1 ──── description\n",
      "branch 1 ──── steps[]\n",
      "branch 2 ──────── step_title\n",
      "branch 2 ──────── details\n",
      "branch 1 ──── improvements\n",
      "branch 2 ──────── metric\n",
      "branch 2 ──────── value_added\n",
      " trunk 0  dataset[]\n",
      "branch 1 ──── name\n",
      "branch 1 ──── source\n",
      "branch 1 ──── preprocessing\n",
      "branch 2 ──────── steps\n",
      "branch 2 ──────── tools_used\n",
      " trunk 0  experiment_results[]\n",
      "branch 1 ──── experiment_name\n",
      "branch 1 ──── metrics[]\n",
      "branch 2 ──────── metric_name\n",
      "branch 2 ──────── value\n",
      " trunk 0  references[]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from t2j.decomposer import SchemaDecomposer  # adjust import if needed\n",
    "\n",
    "SCHEMA_PATH = r\"C:\\Users\\Pratyush\\Desktop\\text-to-json\\examples\\text2SQL-solutions\\schema.json\"\n",
    "\n",
    "with open(SCHEMA_PATH, 'r') as f:\n",
    "    schema_dict = json.load(f)\n",
    "\n",
    "sd = SchemaDecomposer(schema_dict)\n",
    "res = sd.decompose()\n",
    "sd.print_schema_tree(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'path': 'title',\n",
       "  'type': 'string',\n",
       "  'description': 'Title of the paper or document',\n",
       "  'node_type': 'trunk'},\n",
       " {'path': 'related_work[]',\n",
       "  'type': 'array<string>',\n",
       "  'description': 'List of strings, with related works done; keep the points short and precise',\n",
       "  'node_type': 'trunk'},\n",
       " {'path': 'authors[]',\n",
       "  'type': 'array<string>',\n",
       "  'description': 'List of authors of the paper',\n",
       "  'node_type': 'trunk'},\n",
       " {'path': 'abstract',\n",
       "  'type': 'string',\n",
       "  'description': 'Up to 5 bullet points describing what is new in the approach',\n",
       "  'node_type': 'trunk'},\n",
       " {'path': 'approach[]',\n",
       "  'type': 'array<object>',\n",
       "  'description': 'List of methods used in the approach',\n",
       "  'node_type': 'trunk'},\n",
       " {'path': 'approach[].approach_name',\n",
       "  'type': 'string',\n",
       "  'description': 'Name of the method or technique',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'approach[].description',\n",
       "  'type': 'string',\n",
       "  'description': 'Brief summary of what the method is',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'approach[].steps[]',\n",
       "  'type': 'array<object>',\n",
       "  'description': 'Step-by-step breakdown of the approach',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'approach[].steps[].step_title',\n",
       "  'type': 'string',\n",
       "  'description': 'title of the step',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'approach[].steps[].details',\n",
       "  'type': 'string',\n",
       "  'description': 'more details regarding the step',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'approach[].improvements',\n",
       "  'type': 'object',\n",
       "  'description': 'Impact of the approach',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'approach[].improvements.metric',\n",
       "  'type': 'string',\n",
       "  'description': '',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'approach[].improvements.value_added',\n",
       "  'type': 'string',\n",
       "  'description': '',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'dataset[]',\n",
       "  'type': 'array<object>',\n",
       "  'description': 'List of datasets used for training and experimentation',\n",
       "  'node_type': 'trunk'},\n",
       " {'path': 'dataset[].name',\n",
       "  'type': 'string',\n",
       "  'description': 'Name of the dataset used here',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'dataset[].source',\n",
       "  'type': 'string',\n",
       "  'description': 'source of the dataset, if known',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'dataset[].preprocessing',\n",
       "  'type': 'object',\n",
       "  'description': 'preprocessing steps if any done in the paper',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'dataset[].preprocessing.steps',\n",
       "  'type': 'string',\n",
       "  'description': 'Steps done for preprocessing in numbered format',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'dataset[].preprocessing.tools_used',\n",
       "  'type': 'string',\n",
       "  'description': 'any specific tools used for preprocessing',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'experiment_results[]',\n",
       "  'type': 'array<object>',\n",
       "  'description': 'Summarized studies and their results, if available',\n",
       "  'node_type': 'trunk'},\n",
       " {'path': 'experiment_results[].experiment_name',\n",
       "  'type': 'string',\n",
       "  'description': 'name of the experiment done',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'experiment_results[].metrics[]',\n",
       "  'type': 'array<object>',\n",
       "  'description': 'what all metrics were checked during the process',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'experiment_results[].metrics[].metric_name',\n",
       "  'type': 'string',\n",
       "  'description': 'name of the metric checked',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'experiment_results[].metrics[].value',\n",
       "  'type': 'string',\n",
       "  'description': 'value of the results, if known',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'references[]',\n",
       "  'type': 'array<string>',\n",
       "  'description': 'List of references used in the paper',\n",
       "  'node_type': 'trunk'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'path': 'title', 'type': 'string', 'description': 'Title of the paper or document', 'node_type': 'trunk'}\n",
      "0 {'path': 'related_work[]', 'type': 'array<string>', 'description': 'List of strings, with related works done; keep the points short and precise', 'node_type': 'trunk'}\n",
      "0 {'path': 'authors[]', 'type': 'array<string>', 'description': 'List of authors of the paper', 'node_type': 'trunk'}\n",
      "0 {'path': 'abstract', 'type': 'string', 'description': 'Up to 5 bullet points describing what is new in the approach', 'node_type': 'trunk'}\n",
      "2 {'path': 'approach[]', 'type': 'array<object>', 'description': 'List of methods used in the approach', 'node_type': 'trunk'}\n",
      "0 {'path': 'approach[].approach_name', 'type': 'string', 'description': 'Name of the method or technique', 'node_type': 'branch'}\n",
      "0 {'path': 'approach[].description', 'type': 'string', 'description': 'Brief summary of what the method is', 'node_type': 'branch'}\n",
      "0 {'path': 'approach[].steps[]', 'type': 'array<object>', 'description': 'Step-by-step breakdown of the approach', 'node_type': 'branch'}\n",
      "0 {'path': 'approach[].steps[].step_title', 'type': 'string', 'description': 'title of the step', 'node_type': 'branch'}\n",
      "0 {'path': 'approach[].steps[].details', 'type': 'string', 'description': 'more details regarding the step', 'node_type': 'branch'}\n",
      "0 {'path': 'approach[].improvements', 'type': 'object', 'description': 'Impact of the approach', 'node_type': 'branch'}\n",
      "0 {'path': 'approach[].improvements.metric', 'type': 'string', 'description': '', 'node_type': 'branch'}\n",
      "0 {'path': 'approach[].improvements.value_added', 'type': 'string', 'description': '', 'node_type': 'branch'}\n",
      "1 {'path': 'dataset[]', 'type': 'array<object>', 'description': 'List of datasets used for training and experimentation', 'node_type': 'trunk'}\n",
      "0 {'path': 'dataset[].name', 'type': 'string', 'description': 'Name of the dataset used here', 'node_type': 'branch'}\n",
      "0 {'path': 'dataset[].source', 'type': 'string', 'description': 'source of the dataset, if known', 'node_type': 'branch'}\n",
      "0 {'path': 'dataset[].preprocessing', 'type': 'object', 'description': 'preprocessing steps if any done in the paper', 'node_type': 'branch'}\n",
      "0 {'path': 'dataset[].preprocessing.steps', 'type': 'string', 'description': 'Steps done for preprocessing in numbered format', 'node_type': 'branch'}\n",
      "0 {'path': 'dataset[].preprocessing.tools_used', 'type': 'string', 'description': 'any specific tools used for preprocessing', 'node_type': 'branch'}\n",
      "2 {'path': 'experiment_results[]', 'type': 'array<object>', 'description': 'Summarized studies and their results, if available', 'node_type': 'trunk'}\n",
      "0 {'path': 'experiment_results[].experiment_name', 'type': 'string', 'description': 'name of the experiment done', 'node_type': 'branch'}\n",
      "0 {'path': 'experiment_results[].metrics[]', 'type': 'array<object>', 'description': 'what all metrics were checked during the process', 'node_type': 'branch'}\n",
      "0 {'path': 'experiment_results[].metrics[].metric_name', 'type': 'string', 'description': 'name of the metric checked', 'node_type': 'branch'}\n",
      "0 {'path': 'experiment_results[].metrics[].value', 'type': 'string', 'description': 'value of the results, if known', 'node_type': 'branch'}\n",
      "0 {'path': 'references[]', 'type': 'array<string>', 'description': 'List of references used in the paper', 'node_type': 'trunk'}\n"
     ]
    }
   ],
   "source": [
    "def max_child_depth(schema_list, trunk_path):\n",
    "    def get_depth(path):\n",
    "        return path.count(\"[]\") + path.count(\".\")\n",
    "\n",
    "    trunk_path = trunk_path.rstrip('.')\n",
    "\n",
    "    max_depth = 0\n",
    "    for field in schema_list:\n",
    "        path = field['path']\n",
    "        if path == trunk_path:\n",
    "            continue\n",
    "        if path.startswith(trunk_path + \".\") or path.startswith(trunk_path + \"[]\"):\n",
    "            relative_path = path[len(trunk_path):].lstrip('.').lstrip('[]')\n",
    "            depth = get_depth(relative_path)\n",
    "            max_depth = max(max_depth, depth)\n",
    "\n",
    "    return max_depth\n",
    "\n",
    "for r in res:\n",
    "    print(max_child_depth(res, r['path']), r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sd \u001b[38;5;241m=\u001b[39m SchemaDecomposer(\u001b[43mchunks\u001b[49m)\n\u001b[0;32m      2\u001b[0m chunks \u001b[38;5;241m=\u001b[39m sd\u001b[38;5;241m.\u001b[39mdecompose()\n\u001b[0;32m      3\u001b[0m sd\u001b[38;5;241m.\u001b[39mprint_schema_tree(chunks)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'chunks' is not defined"
     ]
    }
   ],
   "source": [
    "# sd = SchemaDecomposer(chunks)\n",
    "# chunks = sd.decompose()\n",
    "# sd.print_schema_tree(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove content\n",
    "chunks = {'1 Introduction': {'content': ' Text-to-SQL, as a bridge between human language and machine-readable structured query languages, is crucial for many use cases, converting natural language questions into executable SQL commands (Androutsopoulos et al., 1995; Li & Jagadish, 2014; Li et al., 2024c; Yu et al., 2018; ?). By enabling users to interact with complex database systems without requiring SQL proficiency, Text-to-SQL empowers users to extract valuable insights, perform streamlined data exploration, make informed decisions, generate data-driven reports and mine better features for machine learning (Chen et al., 2023; Pérez-Mercado et al., 2023; Pourreza & Rafiei, 2024a; Pourreza et al., 2024; Sun et al., 2023; Wang et al., 2019; Xie et al., 2023). Furthermore, Text-to-SQL systems play a pivotal role in automating data analytics with complex reasoning and powering conversational agents, expanding their applications beyond traditional data retrieval (Sun et al., 2023; Xie et al., 2023). As data continues to grow exponentially, the ability to query databases efficiently without extensive SQL knowledge becomes increasingly vital for a broad range of applications. Text-to-SQL can be considered a specialized form of code generation, with the contextual information potentially including the database schema, its metadata and along with the values. In the broader code generation domain, utilizing LLMs to generate a wide range of diverse candidates and select the best one has proven to be effective (Chen et al., 2021; Li et al., 2022; Ni et al., 2023). However, it is non-obvious what 1 arXiv:2410.01943v1  [cs.LG]  2 Oct 2024 leads to most effective candidate proposal and winner selector mechanisms. A straightforward yet effective approach involves generating candidates using zero-/few-shot or open-ended prompting, followed by selecting the best options utilizing self-consistency (Wang et al., 2022), which entails clustering candidates based on their execution outputs. This approach has demonstrated promising results in several studies (Lee et al., 2024; Maamari et al., 2024; Talaei et al., 2024; Wang et al., 2023). However, a single prompt design might not fully unleash the extensive Text-to-SQL knowledge of LLMs, and self-consistency methods might not be always effective. In fact, as illustrated in Table 1, the most consistent answers would not always be the correct ones, with an upper-bound performance 14% higher than that achieved through self-consistency. This substantial gap highlights the potential for significant improvement by implementing more effective selection methods to identify the best answer from the pool of candidate queries. Table 1: Evaluating single-query gen- eration vs. ensemble methods of self- consistency and the upper bound that can be achieved for Text-to-SQL with Gemini 1.5 Pro on the BIRD dev set. EX stands for execution accuracy. Method EX (%) Single query 63.01 Self-consistency 68.84 (+ 5.84) Upper-bound 82.79 (+ 19.78) Building on the challenges outlined in the previous section, we pro- pose novel approaches to improve LLM performance for Text-to-SQL by leveraging judiciously-designed test-time computations in an agentic framework. As indicated by the upper bound in Table 1, utilizing LLMs’ intrinsic knowledge offers significant potential for improvement. We propose methods that generate a diverse set of high-quality can- didate responses and apply a selection mechanism to identify the best answer. Achieving both high-quality and diverse candidate responses is critical for the success of scoring-based selection methods. Low diversity limits improvement potential and reduces the difference be- tween self-consistency and scoring-based approaches. While techniques like increasing temperature or reordering prompt contents can boost diversity, they often compromise the quality of the candidates. To address this, we introduce effective candidate generators designed to enhance diversity while maintaining high-quality outputs. Specifically, we propose three distinct candidate generation approaches, each capable of producing high-quality responses. The first is inspired by the divide-and-conquer algorithm, which breaks down complex problems into smaller, manageable parts to handle difficult queries. The second employs a query execution-plan-based chain-of-thought strategy, where the reasoning process mirrors the steps a database engine takes during query execution. Lastly, we introduce a novel online synthetic example generation method, which helps the model better understand the underlying data schema of the test database. These methods, when used independently, can produce highly-accurate SQL outputs. To effectively select the best answer among candidates, we introduce a selection agent, trained with a classification objective, that assigns scores based on pairwise comparisons between candidate queries. With this agent, we construct a comparison matrix for all candidates and select the final response based on the highest cumulative score. By combining these candidate generation methods with the proposed scoring model, we create an ensemble approach that leverages the strengths of each strategy to significantly improve overall performance. We present comprehensive evaluations on the efficacy of proposed methodologies of CHASE-SQL. Our innovative candidate generation approaches demonstrate superior performance compared to traditional generic CoT prompts, illustrating their capability in guiding LLMs through the decomposition of complex problems into manageable intermediate steps. Furthermore, the proposed selection agent significantly outperforms conventional consistency-based methods, contributing to the state-of-the-art results. Specifically, CHASE- SQL reaches an execution accuracy of 73.01% and 73.0% on the development set and test set of the challenging BIRD Text-to-SQL dataset which outperforms all of the published and undisclosed methods on this benchmark, by a large margin. ',\n",
    "  'sub-headings': []},\n",
    " '2 Related Work': {'content': ' Early Text-to-SQL methods predominantly utilized sequence-to-sequence architectures, encoding user queries and database schemas using models such as Graph Neural Networks (GNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and pre-trained transformer encoders (Cai et al., 2021; Cao et al., 2021; Hwang et al., 2019). On the decoding side, these systems employed either slot-filling or auto-regressive modelling approaches to construct the final SQL queries from the encoded inputs (Choi et al., 2021; Wang et al., 2019). Additionally, tabular language models like TaBERT (Yin et al., 2020), 2 TaPas (Herzig et al., 2020), and Grappa (Yu et al., 2020) have been developed to encode both tables and textual data effectively. However, the landscape has evolved with the widespread use of LLMs, which have largely replaced earlier methods with their superior performance (Katsogiannis-Meimarakis & Koutrika, 2023; Quamar et al., 2022). Initially, efforts concentrated on optimizing prompt designs for these LLMs (Dong et al., 2023; Gao et al., 2023; Pourreza & Rafiei, 2024a). Subsequent advancements have introduced more complex methodologies, including schema linking (Li et al., 2024b; Pourreza & Rafiei, 2024a,b; Talaei et al., 2024), self-correction or self-debugging (Chen et al., 2023; Talaei et al., 2024; Wang et al., 2023), and self-consistency techniques (Lee et al., 2024; Maamari et al., 2024; Sun et al., 2023; Talaei et al., 2024), further enhancing the performance by proposing complex LLM-based pipelines. ',\n",
    "  'sub-headings': []},\n",
    " '3 Methods': {'content': ' ',\n",
    "  'sub-headings': [{'3.1 Overall Framework': {'content': ' This section outlines the proposed CHASE-SQL framework, which consists of four primary components: 1) Value retrieval, 2) Candidate generator, 3) Query fixer, and 4) Selection agent. As illustrated in Fig. 1. The proposed framework begins by retrieving relevant database values. Subsequently, all contextual information, including retrieved values, database metadata, and schema, is provided to an LLM to generate candidate queries. These candidate queries then undergo a fixing loop, and finally, all candidates are compared in a pairwise way using the trained selection agent to pick the correct answer. The following sections delve into the details of each component. Figure 1: Overview of the proposed CHASE-SQL framework for Text-to-SQL, with value retrieval and using a selection agent for improve picking of the answers among the generated candidates along with a fixer to provide feedback for refinement of the outputs. ',\n",
    "     'sub-headings': []}},\n",
    "   {'3.2 Value Retrieval': {'content': ' Databases might contain very high number of rows, with often only a few being relevant to a query. Retrieving relevant values is crucial as they can be used in various SQL clauses like ‘WHERE’ and ‘HAVING’. Similar to the approach in (Talaei et al., 2024), we begin by extracting keywords from the given question using an LLM prompted with few-shot examples. For each keyword, we employ locality-sensitive hashing (LSH) (Datar et al., 2004) to retrieve the most syntactically-similar words, and re-rank them based on embedding-based similarity and edit distance. This approach is robust to typos in the question and considers keyword semantics during retrieval. ',\n",
    "     'sub-headings': []}},\n",
    "   {'3.3 Multi-path Candidate Generation': {'content': ' As shown in Table 1, relying solely on consistency among responses can lead to sub-optimal performance. Therefore, we prioritize diversity in generation of multiple response candidates to increase the likelihood of generating at least one correct answer. Among the diverse responses generated by the candidate generators, we select one as the final response using a selection agent that compares candidates pairwise. To generate 3 diverse responses, we increase the next token sampling temperature, and also shuffle the order of columns and tables in the prompt. Chain-of-Thought (CoT) prompting (Wei et al., 2022) has been proposed to enhance LLMs’ reasoning abilities by conditioning their final responses on a step-by-step chain of reasoning. Most CoT prompting approaches rely on few-shot examples in the prompt to guide LLMs on thinking step-by-step, following the format M = (qi, ri, si), where qi is the example question, ri is the reasoning path, and si is the ground truth SQL query for qi. We employ two distinct reasoning methods and an online synthetic example generation approach. As shown in Fig. 3a, different generators can yield different outputs, indicating their effectiveness for specific questions and databases. ',\n",
    "     'sub-headings': [{'Divide and Conquer CoT': {'content': ': Divide-and-conquer perspective brings breaking down complex problems into smaller sub-problems, solving each individually, and then combining the solutions to obtain the final answer. Along these lines, we propose a CoT prompting approach that first decomposes the given question into smaller sub-problems using pseudo-SQL queries. In the ’conquer’ step, the solutions to these sub-problems are aggregated to construct the final answer. Finally, an optimization step is applied to the constructed query to remove redundant clauses and conditions. This approach is particularly powerful handling complex scenarios that involve nested queries, e.g. intricate WHERE or HAVING conditions, and queries requiring advanced mathematical operations. In Appendix Fig. 17, we exemplify a question and its corresponding SQL query that was successfully solved using this generator, a scenario the other methods considered in this paper could not address due to the query’s complex conditions and SQL clauses. For a more detailed view of the divide-and-conquer prompt, please see Appendix Fig. 16. Additionally, Alg. 1 outlines the step-by-step process of this strategy to generate the final SQL output using a single LLM call. Algorithm 1 Divide and Conquer Chain-of-Thought (CoT) Strategy for Text-to-SQL. Input: Set of human-annotated few-shot examples M, user question Qu, target database D associated with the question, and a large language model (LLM) θ. Divide: 1: Sq ←θ(M, D, Qu) // Decompose the original question Qu into a set of sub-questions Sq 2: Ssql ←∅// Initialize an empty set Ssql to store partial SQL queries for each sub-question Conquer: 3: for each sub-question qi in Sq do 4: // Generate a partial SQL query for each sub-question qi 5: Ssql ←Ssql ∪{θ(M, D, Qu, q1, ..., qi, sql1, ..., sqli−1)} 6: end for Assemble: 7: Sf ←θ(M, D, Qu, Sq, Ssql) // Assemble the final SQL query Sf from all sub-queries in Ssql 8: return Sf ',\n",
    "        'sub-headings': []}},\n",
    "      {'Query Plan CoT': {'content': ': A query (execution) plan is a sequence of steps that the database engine follows to access or modify the data described by a SQL command. When a SQL query is executed, the database management systems’ query optimizers translate the SQL text into a query plan that the database engine can execute. This plan outlines how tables are accessed, how they are joined, and the specific operations performed on the data (see Appendix Fig. 19 as an example). Inspired by the step-by-step process that database engines use to execute SQL queries, we propose a reasoning strategy to construct the final SQL output. Query plans for any given SQL query can be obtained using the “EXPLAIN\" command, which provides a detailed breakdown of execution steps. However, this output is often presented in a format that is difficult to interpret by LLMs (e.g. in SQLite). To address this, we convert the output of “EXPLAIN\" command into a human-readable text format that aligns more closely with the pretraining data of LLMs. The human-readable version of query plans consists of three key steps: (1) identifying and locating the relevant tables for the question, (2) performing operations such as counting, filtering, or matching between tables, and (3) delivering the final result by selecting the appropriate columns to return. This reasoning method complements the divide-and-conquer CoT strategy. While the divide-and-conquer approach is better suited for decomposing complex questions, the query plan approach excels when questions require more reasoning over the relationships between different parts of the question and the database schema. It systematically explains which tables to scan, how to match columns, and how to apply filters. Appendix Fig. 20 shows an example of a question that was answered correctly only by this method. Appendix Fig. 18 provides the prompt used for this reasoning strategy. ',\n",
    "        'sub-headings': []}}]}},\n",
    "   {'3.4 Query Fixer': {'content': ' In some cases, LLMs might generate queries that are syntactically incorrect. These queries are clear candidates for correction, as they fail to provide the correct answers. To address this, we apply an LLM-based query fixer that leverages the self-reflection (Shinn et al., 2024) method. The fixer reflects on the previously generated query, using feedback such as syntax error details or empty result sets to guide the correction process. We continue this iterative fixing approach up to a specified number of attempts, β (set to three in this paper). Appendix Fig. 21 demonstrates the prompt used for this query fixing step. ',\n",
    "     'sub-headings': []}},\n",
    "   {'3.5 Selection Agent': {'content': ' With three different methods for generating SQL queries, we can generate a set of candidate queries for any given question. The key challenge in this step is selecting the correct SQL query from this pool of candidates. A naive approach would be to measure consistency among the candidates by executing them, grouping them based on their execution results, and selecting a query from the largest group as the most likely correct answer. However, this would assume that the most consistent answer is always the best one, which is not always the case. Instead, we propose a more refined picking strategy, Algorithm 3, that relies on a selection agent. Given a set of candidates SQL queries C = {c1, c2, ..., cn}, the final responses are selected by finding 5 the candidate that has the highest score assigned by the selection model. This model θp can take k candidates and rank them based on how accurately each of them answers the given question. Concretely, we formulate the selection of the final response as: cf = arg max c∈C \\uf8eb \\uf8ec \\uf8ed (n k) X i=1 θp(ci1, . . . , cik | Qu, Hu, D) \\uf8f6 \\uf8f7 \\uf8f8, (1) where Qu refers to the user’s question, Hu is the provided hint, and D is the target database from which the question is being asked. In Eq. 1, we pass k candidates to the selection model to be ranked, with k being between 1 and n. In the extreme case of k = 1, the model is unable to make comparisons between candidates, which complicates the evaluation process for the model. As k increases, comparing more candidates makes the process more challenging for the model, as it needs to consider different aspects simultaneously. Consequently, we set k = 2 and train a model with a classification objective to compare only two candidates at a time. Having a set of high-quality and diverse candidates, the most straightforward solution is to employ off-the-shelf LLMs to make pairwise selections. However, experiments with Gemini-1.5-pro showed that using the LLM without fine-tuning resulted in only 58.01% binary classification accuracy. This is primarily due to the candidates being very similar to one another, requiring a fine-tuned model to learn the nuances and make more accurate decisions. To train the selection agent, we first generate candidate SQL queries on the training set (of Text-to-SQL benchmarks), and group them into clusters based on their execution results. For cases where at least one cluster contains correct queries and others contains incorrect ones, we create training examples in the form of tuples (Qu, Ci, Cj, Dij, yij), where Qu is the user’s question, Ci and Cj are the two candidate queries being compared, Dij is the database schema used by both candidates, and yij ∈0, 1 is the label indicating whether Ci or Cj is the correct query. To avoid order bias during training, we randomly shuffle the order of correct and incorrect queries in each pair. Since the number of cases with both correct and incorrect candidates is limited, for instances where no correct candidate exists, we include the ground truth SQL query in the prompt as a hint to guide the model in generating correct candidates. Algorithm 3 Picking the final SQL query from a pool of candidates. Input: Set of candidate SQL queries C = {c1, c2, ..., cn}, user question Qu, hint Hu, target database D, and a selection model θp, er(ci, D) as the execution result of ci on D 1: ri ←0 for all ci ∈C // Initialize the score ri for each candidate query to zero 2: for each distinct pair (ci, cj) where i ̸= j do 3: if er(ci, D) = er(cj, D) then 4: w ←i // ci is the winner if the execution results match 5: else 6: Si,j ←schema_union(ci, cj, D) // Construct union of schemas used in ci and cj 7: w ←θp(Si,j, Qu, Hu, ci, cj)w ∈{i, j} // Use binary classifier θp to select the winner, w ∈{i, j} 8: end if 9: rw ←rw + 1 // Increase the score of the winner cw by 1 10: end for 11: cf ←arg maxci∈C ri // Select the candidate with the highest score as the final SQL query cf 12: return cf In the pseudo-code for Algorithm 3, we begin by initializing a score of zero for each candidate query. Then, for every distinct pair of queries (ci, cj), we compare both (ci, cj) and (cj, ci) to mitigate any order bias, ensuring that both candidates in a pair are fairly evaluated. If both queries produce the same execution result on the database, we mark one as the winner and increment its score, as these results suggest consistency. If the execution results differ, we generate a union of the schema used by both queries and use the binary classifier to determine which query is more likely to be correct. The classifier takes into account the question, the two candidate queries, and the combined schema to make its decision. The winner’s score is then updated accordingly. After all comparisons, the candidate with the highest score is selected as the final query. In the rare case of a tie in the final scores, we break the tie by selecting one of the candidates arbitrarily. 6 ',\n",
    "     'sub-headings': []}}]},\n",
    " '4 Online Synthetic Example Generation': {'content': ': Using M demonstrations for few-shot in-context learning has shown promising results on various related tasks (Pourreza & Rafiei, 2024a). Besides helping with specifying the task and illustrate the step-by-step process deriving the output, demonstrations constructed using relevant tables and columns can also help the model understand the underlying data schema. Based on this insight, we propose a synthetic demonstration generation strategy for Text-to-SQL – given the user question Qu, the target database D, and the selected columns ti (using a column selection approach similar to (Talaei et al., 2024)). Algorithm 2 Online Synthetic example generation strategy for Text-to-SQL. Input: User question Qu, additional user hint Hu, target database D and filtered relevant table columns t associated with the question, LLM θ, guidelines Rf for generating examples by SQL features, guidelines Rt for generating examples with filtered schema, and the numbers of examples to generate nf, nt respectively 1: P ←∅// {(qi, si) | qi, si ∈Σ∗}, where qi is input question , si is output SQL for the i-th example 2: P ←P ∪{θ(D, Rf, nf)} // Generate n examples with entire database by common SQL features 3: P ←P ∪{θ(t, Rt, nt)} // Generate examples with filtered columns to highlight correct schema usage 4: return P Algorithm 2 outlines the online synthetic example generation approach with two LLM generation steps. The first step focuses on generating illustrative examples with common SQL features described in the guideline Rf. The SQL features include equality and non-equality predicates, single table and multi-table JOIN, nested JOIN, ORDER BY and LIMIT, GROUP BY and HAVING, various aggregation functions. These are widely applicable SQL clauses and functions – the generated example SQL queries, incorporating these features, follow the BIRD SQL feature distribution (Appendix Fig 23a). The second step focuses on generating examples highlighting correct interpretation of the underlying data schema – the model θ is asked to generate examples using ti and that are similar to the examples outlined in Rt. Appendix A.10 provides the prompts used for the example generation). While a relevant example (e.g. showing a nested JOIN query with multiple tables) can be helpful for questions that require complex JOIN queries, it might also mislead the LLM for overuse (e.g. when a simple single table query is sufficient). This and the inherent ambiguity of natural language query qi, for which we draw the examples by relevance, make the example selection challenging. Thus, we generate and inject the examples to the prompt online per qi. We ask the LLM to generate many input-output pairs for in-context learning. The final set of synthetic examples for qi contains examples generated with both Rf and Rt. This ensures that the example set is diverse both in SQL features/clauses and the choice of relevant tables/columns used. The diversity of the example set is desirable to avoid over-fitting the output to certain patterns (e.g., the model always writes a SQL with JOIN if shown mostly JOIN examples). Mixing various examples for various SQL features and database tables with and without column filtering is observed to result in better generation quality overall (please see Appendix Table 8). ',\n",
    "  'sub-headings': []},\n",
    " '4 Experiments': {'content': ' ',\n",
    "  'sub-headings': [{'4.1 Datasets and Models': {'content': ' We evaluate the performance of the proposed CHASE-SQL framework on two widely-recognized cross-domain datasets: BIRD (Li et al., 2024c) and Spider (Yu et al., 2018). BIRD contains over 12,751 unique question-SQL pairs from 95 large databases, spanning more than 37 professional domains, with databases designed to resemble real-world scenarios, featuring messy data rows and complex schemas. Spider consists of 10,181 questions and 5,693 unique complex SQL queries across 200 databases, covering 138 domains. The Spider dataset is divided into non-overlapping training, development, and test sets similar to BIRD. For both, we use execution accuracy (EX), the official metric for their respective leaderboard, as the primary evaluation metric to compare methods. Details of the models and their hyperparameters are provided in Appendix section A.2. ',\n",
    "     'sub-headings': []}},\n",
    "   {'4.2 BIRD results': {'content': ' We present the end-to-end Text-to-SQL performance of the proposed CHASE-SQL framework using Claude- 3.5-sonnet and Gemini 1.5 pro on the BIRD development set, and Gemini 1.5 pro on the BIRD test set. We compare with both published methods (either with an available codebase and/or paper) and undisclosed methods. For a fair comparison with Gemini 1.5 pro, all LLM calls in the Claude-3.5-sonnet setting, except for the selection model, are made using Claude-3.5-sonnet (previously-trained selection model is reused). As shown in Table 2, CHASE-SQL with Gemini 1.5 pro achieves 73.01% accuracy on the BIRD development set and 73.0% on the BIRD holdout test set, outperforming all previous works and setting a new state-of-the-art performance. Table 2: Performance Comparison of different Text-to-SQL methods on BIRD benchmark. Method EX (Dev) EX (Test) Published CHASE-SQL + Gemini 1.5 (Ours) 73.01 73.0 CHASE-SQL + Claude 3.5 Sonnet (Ours) 69.53 – Distillery + GPT-4o (Maamari et al., 2024) 67.21 71.83 CHESS (Talaei et al., 2024) 65.00 66.69 MCS-SQL + GPT-4 (Lee et al., 2024) 63.36 65.45 SuperSQL (Li et al., 2024a) 58.5 62.66 Undisclosed Insights AI 72.16 70.26 AskData + GPT-4o 72.03 72.39 OpenSearch-v2 + GPT-4o 69.3 72.28 PURPLE-RED + GPT-4o 68.12 70.21 Arcwise + GPT-4o 67.99 66.21 ExSL + granite-34b-code 67.47 67.76 Table 3: Performance Comparison of different Text-to-SQL methods on Spider test set. Method EX Training with Spider MCS-SQL + GPT-4 (Lee et al., 2024) 89.6 ✓ CHASE-SQL + Gemini 1.5 (Ours) 87.6 ✗ CHESS (Talaei et al., 2024) 87.2 ✗ DAIL-SQL + GPT-4 (Gao et al., 2023) 86.6 ✓ DIN-SQL + GPT-4 (Pourreza & Rafiei, 2024a) 85.3 ✓ C3 + ChatGPT (Dong et al., 2023) 82.3 ✓ RESDSQL 3B (Li et al., 2023a) 79.9 ✓ DIN-SQL + CodeX (Pourreza & Rafiei, 2024a) 78.2 ✓ T5-3B+NatSQL (Rai et al., 2023) 78.0 ✓ Graphix-3B+PICARD (Li et al., 2023b) 77.6 ✓ ',\n",
    "     'sub-headings': []}},\n",
    "   {'4.3 Spider results': {'content': ' We assess the generalizability of the proposed CHASE-SQL by evaluating it in an end-to-end way on the Spider test set without modifying the few-shot samples in the prompts or training a new selection model, i.e. without using and data from the target distribution. This approach allows us to test the performance of CHASE-SQL on different unseen query and database distributions compared to the data from training distributions. Table 3 demonstrates that CHASE-SQL achieves an execution accuracy of 87.6% on the Spider test set, placing it second among methods that have undergone specific training or prompt optimization for the Spider dataset. This highlights the strong generalizability of CHASE-SQL and its potential for generating high quality Text-to-SQL for unseen samples coming from very different distributions and unique challenges. 7 ',\n",
    "     'sub-headings': []}},\n",
    "   {'4.4 Generator and selection performance': {'content': ' Table 4: Ablation studies on single candidate gen- eration performance of the candidate generators compared with original BIRD prompt + zero-shot CoT with Gemini 1.5 pro on the BIRD dev set. Selector is our final performance applying the se- lection agent to 21 candidates generated from all generators. Method Execution Accuracy (%) ∆(%) Baseline 57.75 - QP CoT 63.62 +5.87 DC CoT 63.92 +6.17 OS CoT 67.09 +9.34 Baseline w Query Fixer 61.58 +3.83 QP CoT w Query Fixer 65.51 +7.76 DC CoT w Query Fixer 65.77 +8.02 OS CoT w Query Fixer 68.02 +10.27 Generator + Fixer: To reveal performance of genera- tors, we conducted an ablation study to evaluate the per- formance of each candidate generation method before and after applying the query fixer. We compare the performance of the proposed generators in producing a single candidate query against the original BIRD prompt (Li et al., 2024c), augmented with zero-shot CoT reasoning (Kojima et al., 2022), which serves as the baseline for assessing the quality of prompts. The results, shown in Table 4, indicate that the proposed methods significantly improve SQL generation performance, compared to the naive baseline, towards the goal of producing high-quality candidates while maintain- ing diversity. Among the candidate generators, the online synthetic data generation approach produced an impressive performance of 68.02%, demonstrating its effectiveness in leveraging test-time compute to improve LLM performance by generating high-quality synthetic examples. Furthermore, the query fixer proved crucial, enhancing the quality of the candidate pool and increasing performance by nearly 2% across all candidate generators. Table 5: Evaluating the binary selection accuracy of the different selection models. ',\n",
    "     'sub-headings': [{'Selection': {'content': ' Model Binary Acc. (%) Claude-3.5-sonnet 60.21 Gemini-1.5-pro 63.98 Tuned Gemma 2 9B 64.28 Tuned Gemini-1.5-flash 71.01 Selection: We conducted an analysis on the binary selection accu- racy of the selection agent for cases where, in a pairwise comparison, one candidate is correct and the other is incorrect. We exclude cases where both candidates are either correct or incorrect, as the selection would not affect the outcome since both candidates have the same label. We compare the performance of Claude-3.5-sonnet and Gemini-1.5-pro (both out-of-the-box without fine-tuning) with two fine-tuned models: 1) Gemma 2 9B and 2) Gemini-1.5-flash. As shown in Table 5, both fine-tuned models achieve higher accuracy than the untuned counterparts, demonstrating the importance of fine-tuning to teach the model about the specific preferences. ',\n",
    "        'sub-headings': []}},\n",
    "      {'Candidate Generation Analysis': {'content': ': We analyze the performance of each candidate generator method individually. To better understand the performance potential when effectively selecting the correct SQL query from the candidate pool, we generate seven candidate SQL queries from each generator method (21 candidates in total) for all samples in the BIRD development set. We determine this number of candidates based on the observation that increasing the candidate pool beyond 20 did not yield significant improvements, as illustrated in Fig. 2d. By assuming access to an oracle selection model that always selects the correct SQL query from the seven candidates, we calculate the upper-bound performance achievable for each generator. Conversely, by assuming an adversarial selection model that always selects the wrong SQL query, we determine the lower-bound performance. Fig. 2 illustrates the upper-bound and lower-bound performance for all three methods together with the performance of our selection agent. As shown, the upper-bound performance of the two different CoT methods is generally higher than that of the synthetic example generation method for different number of candidates. However, their lower-bound performance is also lower than the synthetic method. Lower-bound accuracy reflects cases where all candidates are correct, reducing the noise in the selection process since it doesn’t matter which candidate is chosen, so a higher lower-bound is preferred. This is evident in the selection agent’s performance, where a drop in the lower bound leads to diminishing returns from increasing the upper bound, causing the selection agent’s performance to plateau. Additionally, the upper-bound performance of combining all three methods reaches 82.79%, highlighting the significant room for improvement through better candidate picking methods. This demonstrates that the LLM’s parametric knowledge already contains the information needed to solve most questions, highlighting the need for ensemble approaches to effectively extract and utilize this knowledge. 8 (a) Upper-bound and lower-bound Accuracy for Divide and Conquer CoT (b) Upper-bound and lower-bound Accuracy for Online Synthetic Example (c) Upper-bound and lower-bound performance for Query Plan CoT. (d) Upper-bound performance of all three can- didate generators across different difficulty cat- egories. Figure 2: Comparison of the upper- and lower-bound performance of different candidate generators. Additionally, we evaluate the upper-bound performance by combining all candidates from three candidate generation methods across the simple, moderate, and challenging difficulty levels for the BIRD development set. These difficulty categories are assigned by human experts during the creation of the BIRD development set. Fig. 2d shows that, as expected, the upper-bound performance increases with the number of candidates across all difficulty levels. However, for the challenging and moderate classes, the improvement plateaus earlier than in the simple class, suggesting that generating more samples does not further improve the upper-bound performance for these two difficulty levels. Fig. 2 presents a Venn diagram showcasing the performance of three generation methods: Query Plan, Divide and Conquer, and with Synthetic Examples. The numbers within the intersecting regions represent the instances where multiple methods generated at least one correct candidate. This diagram visually highlights the unique contributions of each method, which indicates the necessity of using all three generators. Additionally, in Fig. 3b, we compare the number of correct queries generated by each SQL generation method that are not correct by the other generators. The divide-and-conquer approach outperforms the others on challenging questions, while the query plan method excels on moderately difficult queries. To further analyze the performance of the generators across different domains and varying numbers of columns and tables, we compare the number of correct queries generated for each database, as shown in Appendix Fig. 4. As illustrated, both CoT methods generally perform similarly across databases, while the online synthetic example generation method significantly increases diversity, resulting in more correct answers overall across different databases. ',\n",
    "        'sub-headings': []}},\n",
    "      {'Selection Agent Analysis': {'content': ': We evaluate the query-picking performance by comparing the Text-to-SQL execution accuracy of the selection agent with the self-consistency method (using majority voting) Wang et al. (2022), an oracle model (upper bound), and an adversarial model (lower bound). To conduct the evaluation, we generate 10 samples from each candidate generation method using two different sampling temperatures: 0.5 and 1.8. The results, shown in Table 6, demonstrate that the selection agent significantly outperforms the self-consistency method with a large margin, roughly 6%. As expected, increasing the sampling temperature 9 35 30 33 38 72 23 1045 ',\n",
    "        'sub-headings': []}},\n",
    "      {'Query Plan': {'content': ' ', 'sub-headings': []}},\n",
    "      {'Synthetic Example': {'content': ' ', 'sub-headings': []}},\n",
    "      {'Divide and Conquer': {'content': ' Unsolved Questions: 258 (a) Venn diagram illustrating the num- ber of instances for which each method: Query Plan, Synthetic Example, Divide and Conquer, produces at least one cor- rect candidate. The overlap regions repre- sent multiple methods generating correct candidates. (b) Number of correct queries across different complexity levels that were answered by each method. Figure 3: Comparison of SQL generation methods: Venn diagram showing unique and overlapping correct answers (left) and the performance across different complexity levels (right). raises the upper bound but also lowers the lower bound. This effect is more pronounced for the synthetic data generation method compared to the two CoT methods, mainly because LLMs generate reasoning steps before producing the final SQL query, which helps mitigate the randomness introduced by high-temperature sampling. The performance with self-consistency method generally decreases as temperature increases, since the majority cluster becomes smaller with more random queries. However, the proposed trained selection agent is less affected by temperature scaling and, in two cases, even improved its performance with a more diverse pool of samples. Table 6: Performance comparison of different picking methods on the candidates generated by the candidate generators on BIRD development set with two different temperatures. QP refers to query plan COT, DC refers to divide and conquer COT, and OS is the online synthetic example generation method. Picking Method QP (T=0.5) QP (T=1.8) DC (T=0.5) DC (T=1.8) OS (T=0.5) OS (T=1.8) Lower Bound 50.46 48.63 51.37 47.39 60.43 50.98 Upper Bound 78.55 80.44 78.42 79.34 74.77 79.66 Self-consistency 65.78 65.51 66.43 64.41 67.34 66.88 Our Selection Agent 71.7 71.73 71.31 70.53 70.4 71.38 ',\n",
    "        'sub-headings': []}}]}},\n",
    "   {'4.5 Ablation Studies': {'content': ' In the previous sections, we evaluate the importance of the selection agent and each candidate generation method. Next, we focus on the analysis of the remaining components of CHASE-SQL: LSH for value retrieval, the query fixer, and three reasoning strategies (QP, OS, and DC). Table 7 shows the performance of CHASE- SQL without each of these steps, highlighting their significance in achieving higher-quality performance. The results demonstrate the contribution of each component, where removing LSH, the query fixer, or any of the candidate generators leads to a reduction in execution accuracy, further validating the importance of these components of CHASE-SQL. Moreover, the table compares the performance of our binary selection agent with two other selection methods: self-consistency (Wang et al., 2022) and a ranker agent. The ranker agent receives all candidates generated by our three candidate generators in a single prompt, compares them, and produce a ranking for each. For the ranker agent, we select the query with the lowest rank as the best answer. The binary selection agent significantly outperforms both the self-consistency and ranker agents, demonstrating the effectiveness of the proposed method. 10 ',\n",
    "     'sub-headings': []}}]},\n",
    " '5 Conclusion': {'content': ' Table 7: Ablation studies on the performance of CHASE- SQL after removing the query fixer, LSH for value re- trieval, and reasoning strategies, i.e., QP, OS, and DC. Method Execution Accuracy (%) ∆(%) CHASE-SQL All 73.01 - CHASE-SQL w self-consistency 68.84 -4.17 CHASE-SQL w ranker agent 65.51 -7.5 CHASE-SQL w/o LSH 70.09 -2.92 CHASE-SQL w/o Query Fixer 69.23 -3.78 CHASE-SQL w/o QP 72.36 -0.65 CHASE-SQL w/o OS 72.16 -0.85 CHASE-SQL w/o DC 71.77 -1.24 We introduce a novel agentic framework, CHASE-SQL, to leverage test-time compute for generating diverse, high-quality SQL queries and accurately selecting the correct one. We propose multiple chain-of-thought prompting methods and an online synthetic example generation technique, along with a query selection mechanism that scores candidates based on pairwise comparisons. Our framework, CHASE-SQL, sets a new state-of-the-art in the notable public Text-to-SQL leaderboard (at the time of the submission), demon- strating the effectiveness of test-time computation for both generating diverse queries and selecting the most accurate response. CHASE-SQL addresses key issues like query diversity and selection optimization, paving the way for further improvements in complex reasoning tasks encountered at real-world Text-to-SQL challenges. ',\n",
    "  'sub-headings': []},\n",
    " 'Acknowledgments': {'content': ' We would like to thank Per Jacobsson, Raj Sinha, Zeke Miller, Reza Sherkat, James Su, Zhixian Yan, David Culler, and Xiance Si, for their valuable comments and feedbacks on our paper. We would also like to thank the BIRD team for their invaluable assistance with the evaluation of the BIRD test set. ',\n",
    "  'sub-headings': []},\n",
    " 'References': {'content': ' Ion Androutsopoulos, Graeme D Ritchie, and Peter Thanisch. Natural language interfaces to databases–an introduction. Natural language engineering, 1(1):29–81, 1995. Ruichu Cai, Jinjie Yuan, Boyan Xu, and Zhifeng Hao. Sadga: Structure-aware dual graph aggregation network for text-to-sql. Advances in Neural Information Processing Systems, 34:7664–7676, 2021. Ruisheng Cao, Lu Chen, Zhi Chen, Yanbin Zhao, Su Zhu, and Kai Yu. Lgesql: line graph enhanced text-to-sql model with mixed local and non-local relations. arXiv preprint arXiv:2106.01093, 2021. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023. DongHyun Choi, Myeong Cheol Shin, EungGyun Kim, and Dong Ryeol Shin. Ryansql: Recursively applying sketch-based slot fillings for complex text-to-sql in cross-domain databases. Computational Linguistics, 47 (2):309–332, 2021. Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S Mirrokni. Locality-sensitive hashing scheme based on p-stable distributions. In Proceedings of the twentieth annual symposium on Computational geometry, pp. 253–262, 2004. Xuemei Dong, Chao Zhang, Yuhang Ge, Yuren Mao, Yunjun Gao, Jinshu Lin, Dongfang Lou, et al. C3: Zero-shot text-to-sql with chatgpt. arXiv preprint arXiv:2307.07306, 2023. Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, and Jingren Zhou. Text-to-sql empowered by large language models: A benchmark evaluation. arXiv preprint arXiv:2308.15363, 2023. Jonathan Herzig, Paweł Krzysztof Nowak, Thomas Müller, Francesco Piccinno, and Julian Martin Eisenschlos. Tapas: Weakly supervised table parsing via pre-training. arXiv preprint arXiv:2004.02349, 2020. 11 Wonseok Hwang, Jinyeong Yim, Seunghyun Park, and Minjoon Seo. A comprehensive exploration on wikisql with table-aware word contextualization. arXiv preprint arXiv:1902.01069, 2019. George Katsogiannis-Meimarakis and Georgia Koutrika. A survey on deep learning approaches for text-to-sql. The VLDB Journal, 32(4):905–936, 2023. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199–22213, 2022. Dongjun Lee, Choongwon Park, Jaehyuk Kim, and Heesoo Park. Mcs-sql: Leveraging multiple prompts and multiple-choice selection for text-to-sql generation. arXiv preprint arXiv:2405.07467, 2024. Boyan Li, Yuyu Luo, Chengliang Chai, Guoliang Li, and Nan Tang. The dawn of natural language to sql: Are we fully ready? arXiv preprint arXiv:2406.01265, 2024a. Fei Li and Hosagrahar V Jagadish. Constructing an interactive natural language interface for relational databases. Proceedings of the VLDB Endowment, 8(1):73–84, 2014. Haoyang Li, Jing Zhang, Cuiping Li, and Hong Chen. Resdsql: Decoupling schema linking and skeleton parsing for text-to-sql. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 13067–13075, 2023a. Haoyang Li, Jing Zhang, Hanbing Liu, Ju Fan, Xiaokang Zhang, Jun Zhu, Renjie Wei, Hongyan Pan, Cuiping Li, and Hong Chen. Codes: Towards building open-source language models for text-to-sql. Proceedings of the ACM on Management of Data, 2(3):1–28, 2024b. Jinyang Li, Binyuan Hui, Reynold Cheng, Bowen Qin, Chenhao Ma, Nan Huo, Fei Huang, Wenyu Du, Luo Si, and Yongbin Li. Graphix-t5: Mixing pre-trained transformers with graph-aware layers for text-to-sql parsing. arXiv preprint arXiv:2301.07507, 2023b. Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, et al. Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls. Advances in Neural Information Processing Systems, 36, 2024c. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):1092–1097, 2022. Karime Maamari, Fadhil Abubaker, Daniel Jaroslawicz, and Amine Mhedhbi. The death of schema linking? text-to-sql in the age of well-reasoned language models. arXiv preprint arXiv:2408.07702, 2024. Ansong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-tau Yih, Sida Wang, and Xi Victoria Lin. Lever: Learning to verify language-to-code generation with execution. In International Conference on Machine Learning, pp. 26106–26128. PMLR, 2023. Rubén Pérez-Mercado, Antonio Balderas, Andrés Muñoz, Juan Francisco Cabrera, Manuel Palomo-Duarte, and Juan Manuel Dodero. Chatbotsql: Conversational agent to support relational database query language learning. SoftwareX, 22:101346, 2023. Mohammadreza Pourreza and Davood Rafiei. Din-sql: Decomposed in-context learning of text-to-sql with self-correction. Advances in Neural Information Processing Systems, 36, 2024a. Mohammadreza Pourreza and Davood Rafiei. Dts-sql: Decomposed text-to-sql with small large language models. arXiv preprint arXiv:2402.01117, 2024b. Mohammadreza Pourreza, Ruoxi Sun, Hailong Li, Lesly Miculicich, Tomas Pfister, and Sercan O Arik. Sql-gen: Bridging the dialect gap for text-to-sql via synthetic data and model merging. arXiv preprint arXiv:2408.12733, 2024. 12 Abdul Quamar, Vasilis Efthymiou, Chuan Lei, and Fatma Özcan. Natural language interfaces to data. Found. Trends Databases, 11(4):319–414, 2022. doi: 10.1561/1900000078. URL https://doi.org/10. 1561/1900000078.',\n",
    "  'sub-headings': []}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_all_content(json_data):\n",
    "#     contents = []\n",
    "\n",
    "#     def recursive_extract(data):\n",
    "#         if isinstance(data, dict):\n",
    "#             for key, value in data.items():\n",
    "#                 if key == \"content\" and isinstance(value, str):\n",
    "#                     contents.append(value)\n",
    "#                 else:\n",
    "#                     recursive_extract(value)\n",
    "#         elif isinstance(data, list):\n",
    "#             for item in data:\n",
    "#                 recursive_extract(item)\n",
    "\n",
    "#     recursive_extract(json_data)\n",
    "#     return \" \".join(contents)\n",
    "\n",
    "# extract_all_content(chunks['1 Introduction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'path': 'related_work[]', 'type': 'array<string>', 'description': 'List of strings, with related works done; keep the points short and precise', 'node_type': 'trunk'}\n",
      "{'path': 'approach[]', 'type': 'array<object>', 'description': 'List of methods used in the approach', 'node_type': 'trunk'}\n",
      "{'output': ['3 Methods -> sub-headings[0] -> 3.1 Overall Framework', '3 Methods -> sub-headings[1] -> 3.2 Value Retrieval', '3 Methods -> sub-headings[2] -> 3.3 Multi-path Candidate Generation', '3 Methods -> sub-headings[2] -> 3.3 Multi-path Candidate Generation -> sub-headings[0] -> Divide and Conquer CoT', '3 Methods -> sub-headings[2] -> 3.3 Multi-path Candidate Generation -> sub-headings[1] -> Query Plan CoT', '3 Methods -> sub-headings[3] -> 3.4 Query Fixer', '3 Methods -> sub-headings[4] -> 3.5 Selection Agent']}\n",
      " This section outlines the proposed CHASE-SQL framework, which consists of four primary components: 1) Value retrieval, 2) Candidate generator, 3) Query fixer, and 4) Selection agent. As illustrated in Fig. 1. The proposed framework begins by retrieving relevant database values. Subsequently, all contextual information, including retrieved values, database metadata, and schema, is provided to an LLM to generate candidate queries. These candidate queries then undergo a fixing loop, and finally, all candidates are compared in a pairwise way using the trained selection agent to pick the correct answer. The following sections delve into the details of each component. Figure 1: Overview of the proposed CHASE-SQL framework for Text-to-SQL, with value retrieval and using a selection agent for improve picking of the answers among the generated candidates along with a fixer to provide feedback for refinement of the outputs.  Databases might contain very high number of rows, with often only a few being relevant to a query. Retrieving relevant values is crucial as they can be used in various SQL clauses like ‘WHERE’ and ‘HAVING’. Similar to the approach in (Talaei et al., 2024), we begin by extracting keywords from the given question using an LLM prompted with few-shot examples. For each keyword, we employ locality-sensitive hashing (LSH) (Datar et al., 2004) to retrieve the most syntactically-similar words, and re-rank them based on embedding-based similarity and edit distance. This approach is robust to typos in the question and considers keyword semantics during retrieval.  As shown in Table 1, relying solely on consistency among responses can lead to sub-optimal performance. Therefore, we prioritize diversity in generation of multiple response candidates to increase the likelihood of generating at least one correct answer. Among the diverse responses generated by the candidate generators, we select one as the final response using a selection agent that compares candidates pairwise. To generate 3 diverse responses, we increase the next token sampling temperature, and also shuffle the order of columns and tables in the prompt. Chain-of-Thought (CoT) prompting (Wei et al., 2022) has been proposed to enhance LLMs’ reasoning abilities by conditioning their final responses on a step-by-step chain of reasoning. Most CoT prompting approaches rely on few-shot examples in the prompt to guide LLMs on thinking step-by-step, following the format M = (qi, ri, si), where qi is the example question, ri is the reasoning path, and si is the ground truth SQL query for qi. We employ two distinct reasoning methods and an online synthetic example generation approach. As shown in Fig. 3a, different generators can yield different outputs, indicating their effectiveness for specific questions and databases.  : Divide-and-conquer perspective brings breaking down complex problems into smaller sub-problems, solving each individually, and then combining the solutions to obtain the final answer. Along these lines, we propose a CoT prompting approach that first decomposes the given question into smaller sub-problems using pseudo-SQL queries. In the ’conquer’ step, the solutions to these sub-problems are aggregated to construct the final answer. Finally, an optimization step is applied to the constructed query to remove redundant clauses and conditions. This approach is particularly powerful handling complex scenarios that involve nested queries, e.g. intricate WHERE or HAVING conditions, and queries requiring advanced mathematical operations. In Appendix Fig. 17, we exemplify a question and its corresponding SQL query that was successfully solved using this generator, a scenario the other methods considered in this paper could not address due to the query’s complex conditions and SQL clauses. For a more detailed view of the divide-and-conquer prompt, please see Appendix Fig. 16. Additionally, Alg. 1 outlines the step-by-step process of this strategy to generate the final SQL output using a single LLM call. Algorithm 1 Divide and Conquer Chain-of-Thought (CoT) Strategy for Text-to-SQL. Input: Set of human-annotated few-shot examples M, user question Qu, target database D associated with the question, and a large language model (LLM) θ. Divide: 1: Sq ←θ(M, D, Qu) // Decompose the original question Qu into a set of sub-questions Sq 2: Ssql ←∅// Initialize an empty set Ssql to store partial SQL queries for each sub-question Conquer: 3: for each sub-question qi in Sq do 4: // Generate a partial SQL query for each sub-question qi 5: Ssql ←Ssql ∪{θ(M, D, Qu, q1, ..., qi, sql1, ..., sqli−1)} 6: end for Assemble: 7: Sf ←θ(M, D, Qu, Sq, Ssql) // Assemble the final SQL query Sf from all sub-queries in Ssql 8: return Sf  : A query (execution) plan is a sequence of steps that the database engine follows to access or modify the data described by a SQL command. When a SQL query is executed, the database management systems’ query optimizers translate the SQL text into a query plan that the database engine can execute. This plan outlines how tables are accessed, how they are joined, and the specific operations performed on the data (see Appendix Fig. 19 as an example). Inspired by the step-by-step process that database engines use to execute SQL queries, we propose a reasoning strategy to construct the final SQL output. Query plans for any given SQL query can be obtained using the “EXPLAIN\" command, which provides a detailed breakdown of execution steps. However, this output is often presented in a format that is difficult to interpret by LLMs (e.g. in SQLite). To address this, we convert the output of “EXPLAIN\" command into a human-readable text format that aligns more closely with the pretraining data of LLMs. The human-readable version of query plans consists of three key steps: (1) identifying and locating the relevant tables for the question, (2) performing operations such as counting, filtering, or matching between tables, and (3) delivering the final result by selecting the appropriate columns to return. This reasoning method complements the divide-and-conquer CoT strategy. While the divide-and-conquer approach is better suited for decomposing complex questions, the query plan approach excels when questions require more reasoning over the relationships between different parts of the question and the database schema. It systematically explains which tables to scan, how to match columns, and how to apply filters. Appendix Fig. 20 shows an example of a question that was answered correctly only by this method. Appendix Fig. 18 provides the prompt used for this reasoning strategy. : Divide-and-conquer perspective brings breaking down complex problems into smaller sub-problems, solving each individually, and then combining the solutions to obtain the final answer. Along these lines, we propose a CoT prompting approach that first decomposes the given question into smaller sub-problems using pseudo-SQL queries. In the ’conquer’ step, the solutions to these sub-problems are aggregated to construct the final answer. Finally, an optimization step is applied to the constructed query to remove redundant clauses and conditions. This approach is particularly powerful handling complex scenarios that involve nested queries, e.g. intricate WHERE or HAVING conditions, and queries requiring advanced mathematical operations. In Appendix Fig. 17, we exemplify a question and its corresponding SQL query that was successfully solved using this generator, a scenario the other methods considered in this paper could not address due to the query’s complex conditions and SQL clauses. For a more detailed view of the divide-and-conquer prompt, please see Appendix Fig. 16. Additionally, Alg. 1 outlines the step-by-step process of this strategy to generate the final SQL output using a single LLM call. Algorithm 1 Divide and Conquer Chain-of-Thought (CoT) Strategy for Text-to-SQL. Input: Set of human-annotated few-shot examples M, user question Qu, target database D associated with the question, and a large language model (LLM) θ. Divide: 1: Sq ←θ(M, D, Qu) // Decompose the original question Qu into a set of sub-questions Sq 2: Ssql ←∅// Initialize an empty set Ssql to store partial SQL queries for each sub-question Conquer: 3: for each sub-question qi in Sq do 4: // Generate a partial SQL query for each sub-question qi 5: Ssql ←Ssql ∪{θ(M, D, Qu, q1, ..., qi, sql1, ..., sqli−1)} 6: end for Assemble: 7: Sf ←θ(M, D, Qu, Sq, Ssql) // Assemble the final SQL query Sf from all sub-queries in Ssql 8: return Sf : A query (execution) plan is a sequence of steps that the database engine follows to access or modify the data described by a SQL command. When a SQL query is executed, the database management systems’ query optimizers translate the SQL text into a query plan that the database engine can execute. This plan outlines how tables are accessed, how they are joined, and the specific operations performed on the data (see Appendix Fig. 19 as an example). Inspired by the step-by-step process that database engines use to execute SQL queries, we propose a reasoning strategy to construct the final SQL output. Query plans for any given SQL query can be obtained using the “EXPLAIN\" command, which provides a detailed breakdown of execution steps. However, this output is often presented in a format that is difficult to interpret by LLMs (e.g. in SQLite). To address this, we convert the output of “EXPLAIN\" command into a human-readable text format that aligns more closely with the pretraining data of LLMs. The human-readable version of query plans consists of three key steps: (1) identifying and locating the relevant tables for the question, (2) performing operations such as counting, filtering, or matching between tables, and (3) delivering the final result by selecting the appropriate columns to return. This reasoning method complements the divide-and-conquer CoT strategy. While the divide-and-conquer approach is better suited for decomposing complex questions, the query plan approach excels when questions require more reasoning over the relationships between different parts of the question and the database schema. It systematically explains which tables to scan, how to match columns, and how to apply filters. Appendix Fig. 20 shows an example of a question that was answered correctly only by this method. Appendix Fig. 18 provides the prompt used for this reasoning strategy.  In some cases, LLMs might generate queries that are syntactically incorrect. These queries are clear candidates for correction, as they fail to provide the correct answers. To address this, we apply an LLM-based query fixer that leverages the self-reflection (Shinn et al., 2024) method. The fixer reflects on the previously generated query, using feedback such as syntax error details or empty result sets to guide the correction process. We continue this iterative fixing approach up to a specified number of attempts, β (set to three in this paper). Appendix Fig. 21 demonstrates the prompt used for this query fixing step.  With three different methods for generating SQL queries, we can generate a set of candidate queries for any given question. The key challenge in this step is selecting the correct SQL query from this pool of candidates. A naive approach would be to measure consistency among the candidates by executing them, grouping them based on their execution results, and selecting a query from the largest group as the most likely correct answer. However, this would assume that the most consistent answer is always the best one, which is not always the case. Instead, we propose a more refined picking strategy, Algorithm 3, that relies on a selection agent. Given a set of candidates SQL queries C = {c1, c2, ..., cn}, the final responses are selected by finding 5 the candidate that has the highest score assigned by the selection model. This model θp can take k candidates and rank them based on how accurately each of them answers the given question. Concretely, we formulate the selection of the final response as: cf = arg max c∈C    (n k) X i=1 θp(ci1, . . . , cik | Qu, Hu, D)   , (1) where Qu refers to the user’s question, Hu is the provided hint, and D is the target database from which the question is being asked. In Eq. 1, we pass k candidates to the selection model to be ranked, with k being between 1 and n. In the extreme case of k = 1, the model is unable to make comparisons between candidates, which complicates the evaluation process for the model. As k increases, comparing more candidates makes the process more challenging for the model, as it needs to consider different aspects simultaneously. Consequently, we set k = 2 and train a model with a classification objective to compare only two candidates at a time. Having a set of high-quality and diverse candidates, the most straightforward solution is to employ off-the-shelf LLMs to make pairwise selections. However, experiments with Gemini-1.5-pro showed that using the LLM without fine-tuning resulted in only 58.01% binary classification accuracy. This is primarily due to the candidates being very similar to one another, requiring a fine-tuned model to learn the nuances and make more accurate decisions. To train the selection agent, we first generate candidate SQL queries on the training set (of Text-to-SQL benchmarks), and group them into clusters based on their execution results. For cases where at least one cluster contains correct queries and others contains incorrect ones, we create training examples in the form of tuples (Qu, Ci, Cj, Dij, yij), where Qu is the user’s question, Ci and Cj are the two candidate queries being compared, Dij is the database schema used by both candidates, and yij ∈0, 1 is the label indicating whether Ci or Cj is the correct query. To avoid order bias during training, we randomly shuffle the order of correct and incorrect queries in each pair. Since the number of cases with both correct and incorrect candidates is limited, for instances where no correct candidate exists, we include the ground truth SQL query in the prompt as a hint to guide the model in generating correct candidates. Algorithm 3 Picking the final SQL query from a pool of candidates. Input: Set of candidate SQL queries C = {c1, c2, ..., cn}, user question Qu, hint Hu, target database D, and a selection model θp, er(ci, D) as the execution result of ci on D 1: ri ←0 for all ci ∈C // Initialize the score ri for each candidate query to zero 2: for each distinct pair (ci, cj) where i ̸= j do 3: if er(ci, D) = er(cj, D) then 4: w ←i // ci is the winner if the execution results match 5: else 6: Si,j ←schema_union(ci, cj, D) // Construct union of schemas used in ci and cj 7: w ←θp(Si,j, Qu, Hu, ci, cj)w ∈{i, j} // Use binary classifier θp to select the winner, w ∈{i, j} 8: end if 9: rw ←rw + 1 // Increase the score of the winner cw by 1 10: end for 11: cf ←arg maxci∈C ri // Select the candidate with the highest score as the final SQL query cf 12: return cf In the pseudo-code for Algorithm 3, we begin by initializing a score of zero for each candidate query. Then, for every distinct pair of queries (ci, cj), we compare both (ci, cj) and (cj, ci) to mitigate any order bias, ensuring that both candidates in a pair are fairly evaluated. If both queries produce the same execution result on the database, we mark one as the winner and increment its score, as these results suggest consistency. If the execution results differ, we generate a union of the schema used by both queries and use the binary classifier to determine which query is more likely to be correct. The classifier takes into account the question, the two candidate queries, and the combined schema to make its decision. The winner’s score is then updated accordingly. After all comparisons, the candidate with the highest score is selected as the final query. In the rare case of a tie in the final scores, we break the tie by selecting one of the candidates arbitrarily. 6 \n",
      "\n",
      "## TASK: Given a sub-json from a schema-json with it's metadata, based on format, fetch the value from given content.\n",
      "Follow the 'GUIDELINES' strictly\n",
      "\n",
      "## GUIDELINES:\n",
      "1. sub-json is given in 'SUB JSON', this is the sub-json for which we are fetching the value.\n",
      "2. Return list of these sub-json based on guidelines\n",
      "5. 'OUTPUT FORMAT', this tells which format should be of the output.\n",
      "6. 'CONTENT' contains all the content, from where you will be extracting information. Output cannot be empty as 'CONTENT' exists\n",
      "7. ONLY RETURN THE JSON, NO OTHER INFO\n",
      "\n",
      "## CONTENT:\n",
      " This section outlines the proposed CHASE-SQL framework, which consists of four primary components: 1) Value retrieval, 2) Candidate generator, 3) Query fixer, and 4) Selection agent. As illustrated in Fig. 1. The proposed framework begins by retrieving relevant database values. Subsequently, all contextual information, including retrieved values, database metadata, and schema, is provided to an LLM to generate candidate queries. These candidate queries then undergo a fixing loop, and finally, all candidates are compared in a pairwise way using the trained selection agent to pick the correct answer. The following sections delve into the details of each component. Figure 1: Overview of the proposed CHASE-SQL framework for Text-to-SQL, with value retrieval and using a selection agent for improve picking of the answers among the generated candidates along with a fixer to provide feedback for refinement of the outputs.  Databases might contain very high number of rows, with often only a few being relevant to a query. Retrieving relevant values is crucial as they can be used in various SQL clauses like ‘WHERE’ and ‘HAVING’. Similar to the approach in (Talaei et al., 2024), we begin by extracting keywords from the given question using an LLM prompted with few-shot examples. For each keyword, we employ locality-sensitive hashing (LSH) (Datar et al., 2004) to retrieve the most syntactically-similar words, and re-rank them based on embedding-based similarity and edit distance. This approach is robust to typos in the question and considers keyword semantics during retrieval.  As shown in Table 1, relying solely on consistency among responses can lead to sub-optimal performance. Therefore, we prioritize diversity in generation of multiple response candidates to increase the likelihood of generating at least one correct answer. Among the diverse responses generated by the candidate generators, we select one as the final response using a selection agent that compares candidates pairwise. To generate 3 diverse responses, we increase the next token sampling temperature, and also shuffle the order of columns and tables in the prompt. Chain-of-Thought (CoT) prompting (Wei et al., 2022) has been proposed to enhance LLMs’ reasoning abilities by conditioning their final responses on a step-by-step chain of reasoning. Most CoT prompting approaches rely on few-shot examples in the prompt to guide LLMs on thinking step-by-step, following the format M = (qi, ri, si), where qi is the example question, ri is the reasoning path, and si is the ground truth SQL query for qi. We employ two distinct reasoning methods and an online synthetic example generation approach. As shown in Fig. 3a, different generators can yield different outputs, indicating their effectiveness for specific questions and databases.  : Divide-and-conquer perspective brings breaking down complex problems into smaller sub-problems, solving each individually, and then combining the solutions to obtain the final answer. Along these lines, we propose a CoT prompting approach that first decomposes the given question into smaller sub-problems using pseudo-SQL queries. In the ’conquer’ step, the solutions to these sub-problems are aggregated to construct the final answer. Finally, an optimization step is applied to the constructed query to remove redundant clauses and conditions. This approach is particularly powerful handling complex scenarios that involve nested queries, e.g. intricate WHERE or HAVING conditions, and queries requiring advanced mathematical operations. In Appendix Fig. 17, we exemplify a question and its corresponding SQL query that was successfully solved using this generator, a scenario the other methods considered in this paper could not address due to the query’s complex conditions and SQL clauses. For a more detailed view of the divide-and-conquer prompt, please see Appendix Fig. 16. Additionally, Alg. 1 outlines the step-by-step process of this strategy to generate the final SQL output using a single LLM call. Algorithm 1 Divide and Conquer Chain-of-Thought (CoT) Strategy for Text-to-SQL. Input: Set of human-annotated few-shot examples M, user question Qu, target database D associated with the question, and a large language model (LLM) θ. Divide: 1: Sq ←θ(M, D, Qu) // Decompose the original question Qu into a set of sub-questions Sq 2: Ssql ←∅// Initialize an empty set Ssql to store partial SQL queries for each sub-question Conquer: 3: for each sub-question qi in Sq do 4: // Generate a partial SQL query for each sub-question qi 5: Ssql ←Ssql ∪{θ(M, D, Qu, q1, ..., qi, sql1, ..., sqli−1)} 6: end for Assemble: 7: Sf ←θ(M, D, Qu, Sq, Ssql) // Assemble the final SQL query Sf from all sub-queries in Ssql 8: return Sf  : A query (execution) plan is a sequence of steps that the database engine follows to access or modify the data described by a SQL command. When a SQL query is executed, the database management systems’ query optimizers translate the SQL text into a query plan that the database engine can execute. This plan outlines how tables are accessed, how they are joined, and the specific operations performed on the data (see Appendix Fig. 19 as an example). Inspired by the step-by-step process that database engines use to execute SQL queries, we propose a reasoning strategy to construct the final SQL output. Query plans for any given SQL query can be obtained using the “EXPLAIN\" command, which provides a detailed breakdown of execution steps. However, this output is often presented in a format that is difficult to interpret by LLMs (e.g. in SQLite). To address this, we convert the output of “EXPLAIN\" command into a human-readable text format that aligns more closely with the pretraining data of LLMs. The human-readable version of query plans consists of three key steps: (1) identifying and locating the relevant tables for the question, (2) performing operations such as counting, filtering, or matching between tables, and (3) delivering the final result by selecting the appropriate columns to return. This reasoning method complements the divide-and-conquer CoT strategy. While the divide-and-conquer approach is better suited for decomposing complex questions, the query plan approach excels when questions require more reasoning over the relationships between different parts of the question and the database schema. It systematically explains which tables to scan, how to match columns, and how to apply filters. Appendix Fig. 20 shows an example of a question that was answered correctly only by this method. Appendix Fig. 18 provides the prompt used for this reasoning strategy. : Divide-and-conquer perspective brings breaking down complex problems into smaller sub-problems, solving each individually, and then combining the solutions to obtain the final answer. Along these lines, we propose a CoT prompting approach that first decomposes the given question into smaller sub-problems using pseudo-SQL queries. In the ’conquer’ step, the solutions to these sub-problems are aggregated to construct the final answer. Finally, an optimization step is applied to the constructed query to remove redundant clauses and conditions. This approach is particularly powerful handling complex scenarios that involve nested queries, e.g. intricate WHERE or HAVING conditions, and queries requiring advanced mathematical operations. In Appendix Fig. 17, we exemplify a question and its corresponding SQL query that was successfully solved using this generator, a scenario the other methods considered in this paper could not address due to the query’s complex conditions and SQL clauses. For a more detailed view of the divide-and-conquer prompt, please see Appendix Fig. 16. Additionally, Alg. 1 outlines the step-by-step process of this strategy to generate the final SQL output using a single LLM call. Algorithm 1 Divide and Conquer Chain-of-Thought (CoT) Strategy for Text-to-SQL. Input: Set of human-annotated few-shot examples M, user question Qu, target database D associated with the question, and a large language model (LLM) θ. Divide: 1: Sq ←θ(M, D, Qu) // Decompose the original question Qu into a set of sub-questions Sq 2: Ssql ←∅// Initialize an empty set Ssql to store partial SQL queries for each sub-question Conquer: 3: for each sub-question qi in Sq do 4: // Generate a partial SQL query for each sub-question qi 5: Ssql ←Ssql ∪{θ(M, D, Qu, q1, ..., qi, sql1, ..., sqli−1)} 6: end for Assemble: 7: Sf ←θ(M, D, Qu, Sq, Ssql) // Assemble the final SQL query Sf from all sub-queries in Ssql 8: return Sf : A query (execution) plan is a sequence of steps that the database engine follows to access or modify the data described by a SQL command. When a SQL query is executed, the database management systems’ query optimizers translate the SQL text into a query plan that the database engine can execute. This plan outlines how tables are accessed, how they are joined, and the specific operations performed on the data (see Appendix Fig. 19 as an example). Inspired by the step-by-step process that database engines use to execute SQL queries, we propose a reasoning strategy to construct the final SQL output. Query plans for any given SQL query can be obtained using the “EXPLAIN\" command, which provides a detailed breakdown of execution steps. However, this output is often presented in a format that is difficult to interpret by LLMs (e.g. in SQLite). To address this, we convert the output of “EXPLAIN\" command into a human-readable text format that aligns more closely with the pretraining data of LLMs. The human-readable version of query plans consists of three key steps: (1) identifying and locating the relevant tables for the question, (2) performing operations such as counting, filtering, or matching between tables, and (3) delivering the final result by selecting the appropriate columns to return. This reasoning method complements the divide-and-conquer CoT strategy. While the divide-and-conquer approach is better suited for decomposing complex questions, the query plan approach excels when questions require more reasoning over the relationships between different parts of the question and the database schema. It systematically explains which tables to scan, how to match columns, and how to apply filters. Appendix Fig. 20 shows an example of a question that was answered correctly only by this method. Appendix Fig. 18 provides the prompt used for this reasoning strategy.  In some cases, LLMs might generate queries that are syntactically incorrect. These queries are clear candidates for correction, as they fail to provide the correct answers. To address this, we apply an LLM-based query fixer that leverages the self-reflection (Shinn et al., 2024) method. The fixer reflects on the previously generated query, using feedback such as syntax error details or empty result sets to guide the correction process. We continue this iterative fixing approach up to a specified number of attempts, β (set to three in this paper). Appendix Fig. 21 demonstrates the prompt used for this query fixing step.  With three different methods for generating SQL queries, we can generate a set of candidate queries for any given question. The key challenge in this step is selecting the correct SQL query from this pool of candidates. A naive approach would be to measure consistency among the candidates by executing them, grouping them based on their execution results, and selecting a query from the largest group as the most likely correct answer. However, this would assume that the most consistent answer is always the best one, which is not always the case. Instead, we propose a more refined picking strategy, Algorithm 3, that relies on a selection agent. Given a set of candidates SQL queries C = {c1, c2, ..., cn}, the final responses are selected by finding 5 the candidate that has the highest score assigned by the selection model. This model θp can take k candidates and rank them based on how accurately each of them answers the given question. Concretely, we formulate the selection of the final response as: cf = arg max c∈C    (n k) X i=1 θp(ci1, . . . , cik | Qu, Hu, D)   , (1) where Qu refers to the user’s question, Hu is the provided hint, and D is the target database from which the question is being asked. In Eq. 1, we pass k candidates to the selection model to be ranked, with k being between 1 and n. In the extreme case of k = 1, the model is unable to make comparisons between candidates, which complicates the evaluation process for the model. As k increases, comparing more candidates makes the process more challenging for the model, as it needs to consider different aspects simultaneously. Consequently, we set k = 2 and train a model with a classification objective to compare only two candidates at a time. Having a set of high-quality and diverse candidates, the most straightforward solution is to employ off-the-shelf LLMs to make pairwise selections. However, experiments with Gemini-1.5-pro showed that using the LLM without fine-tuning resulted in only 58.01% binary classification accuracy. This is primarily due to the candidates being very similar to one another, requiring a fine-tuned model to learn the nuances and make more accurate decisions. To train the selection agent, we first generate candidate SQL queries on the training set (of Text-to-SQL benchmarks), and group them into clusters based on their execution results. For cases where at least one cluster contains correct queries and others contains incorrect ones, we create training examples in the form of tuples (Qu, Ci, Cj, Dij, yij), where Qu is the user’s question, Ci and Cj are the two candidate queries being compared, Dij is the database schema used by both candidates, and yij ∈0, 1 is the label indicating whether Ci or Cj is the correct query. To avoid order bias during training, we randomly shuffle the order of correct and incorrect queries in each pair. Since the number of cases with both correct and incorrect candidates is limited, for instances where no correct candidate exists, we include the ground truth SQL query in the prompt as a hint to guide the model in generating correct candidates. Algorithm 3 Picking the final SQL query from a pool of candidates. Input: Set of candidate SQL queries C = {c1, c2, ..., cn}, user question Qu, hint Hu, target database D, and a selection model θp, er(ci, D) as the execution result of ci on D 1: ri ←0 for all ci ∈C // Initialize the score ri for each candidate query to zero 2: for each distinct pair (ci, cj) where i ̸= j do 3: if er(ci, D) = er(cj, D) then 4: w ←i // ci is the winner if the execution results match 5: else 6: Si,j ←schema_union(ci, cj, D) // Construct union of schemas used in ci and cj 7: w ←θp(Si,j, Qu, Hu, ci, cj)w ∈{i, j} // Use binary classifier θp to select the winner, w ∈{i, j} 8: end if 9: rw ←rw + 1 // Increase the score of the winner cw by 1 10: end for 11: cf ←arg maxci∈C ri // Select the candidate with the highest score as the final SQL query cf 12: return cf In the pseudo-code for Algorithm 3, we begin by initializing a score of zero for each candidate query. Then, for every distinct pair of queries (ci, cj), we compare both (ci, cj) and (cj, ci) to mitigate any order bias, ensuring that both candidates in a pair are fairly evaluated. If both queries produce the same execution result on the database, we mark one as the winner and increment its score, as these results suggest consistency. If the execution results differ, we generate a union of the schema used by both queries and use the binary classifier to determine which query is more likely to be correct. The classifier takes into account the question, the two candidate queries, and the combined schema to make its decision. The winner’s score is then updated accordingly. After all comparisons, the candidate with the highest score is selected as the final query. In the rare case of a tie in the final scores, we break the tie by selecting one of the candidates arbitrarily. 6 \n",
      "\n",
      "## SUB JSON:\n",
      "[{'path': 'approach[]', 'type': 'array<object>', 'description': 'List of methods used in the approach', 'node_type': 'trunk'}, {'path': 'approach[].approach_name', 'type': 'string', 'description': 'Name of the method or technique', 'node_type': 'branch'}, {'path': 'approach[].description', 'type': 'string', 'description': 'Brief summary of what the method is', 'node_type': 'branch'}, {'path': 'approach[].steps[]', 'type': 'array<object>', 'description': 'Step-by-step breakdown of the approach', 'node_type': 'branch'}, {'path': 'approach[].steps[].step_title', 'type': 'string', 'description': 'title of the step', 'node_type': 'branch'}, {'path': 'approach[].steps[].details', 'type': 'string', 'description': 'more details regarding the step', 'node_type': 'branch'}, {'path': 'approach[].improvements', 'type': 'object', 'description': 'Impact of the approach', 'node_type': 'branch'}, {'path': 'approach[].improvements.metric', 'type': 'string', 'description': '', 'node_type': 'branch'}, {'path': 'approach[].improvements.value_added', 'type': 'string', 'description': '', 'node_type': 'branch'}]\n",
      "\n",
      "## OUTPUT FORMAT:\n",
      "```json\n",
      "{\n",
      "    'output': [\n",
      "        {'approach': [{'approach_name': '', 'description': '', 'steps': [{'step_title': '', 'details': ''}], 'improvements': {'metric': '', 'value_added': ''}}]}\n",
      "    ]\n",
      "}\n",
      "```\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"output\": [\n",
      "        {\n",
      "            \"approach\": [\n",
      "                {\n",
      "                    \"approach_name\": \"Value retrieval\",\n",
      "                    \"description\": \"Retrieving relevant database values for SQL query generation.\",\n",
      "                    \"steps\": [\n",
      "                        {\n",
      "                            \"step_title\": \"Keyword Extraction\",\n",
      "                            \"details\": \"Extract keywords from the given question using an LLM prompted with few-shot examples.\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"step_title\": \"LSH Retrieval\",\n",
      "                            \"details\": \"Use locality-sensitive hashing (LSH) to retrieve the most syntactically-similar words.\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"step_title\": \"Re-ranking\",\n",
      "                            \"details\": \"Re-rank retrieved words based on embedding-based similarity and edit distance.\"\n",
      "                        }\n",
      "                    ],\n",
      "                    \"improvements\": {\n",
      "                        \"metric\": \"Robustness\",\n",
      "                        \"value_added\": \"Improves robustness to typos and considers keyword semantics.\"\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"approach_name\": \"Candidate generator\",\n",
      "                    \"description\": \"Generate diverse candidate SQL queries using LLMs.\",\n",
      "                    \"steps\": [\n",
      "                        {\n",
      "                            \"step_title\": \"Diversity in Generation\",\n",
      "                            \"details\": \"Increase next token sampling temperature and shuffle order of columns and tables.\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"step_title\": \"Chain-of-Thought Prompting\",\n",
      "                            \"details\": \"Use few-shot examples to guide LLMs on thinking step-by-step.\"\n",
      "                        }\n",
      "                    ],\n",
      "                    \"improvements\": {\n",
      "                        \"metric\": \"Diversity\",\n",
      "                        \"value_added\": \"Increases likelihood of generating at least one correct answer.\"\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"approach_name\": \"Query fixer\",\n",
      "                    \"description\": \"Fix syntactically incorrect SQL queries using LLM-based self-reflection.\",\n",
      "                    \"steps\": [\n",
      "                        {\n",
      "                            \"step_title\": \"Iterative Fixing\",\n",
      "                            \"details\": \"Reflect on previously generated query using feedback such as syntax error details.\"\n",
      "                        }\n",
      "                    ],\n",
      "                    \"improvements\": {\n",
      "                        \"metric\": \"Accuracy\",\n",
      "                        \"value_added\": \"Corrects queries that fail to provide correct answers.\"\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"approach_name\": \"Selection agent\",\n",
      "                    \"description\": \"Select the correct SQL query from a set of candidates using a trained selection model.\",\n",
      "                    \"steps\": [\n",
      "                        {\n",
      "                            \"step_title\": \"Pairwise Comparison\",\n",
      "                            \"details\": \"Compare candidates pairwise using a selection model to pick the correct answer.\"\n",
      "                        }\n",
      "                    ],\n",
      "                    \"improvements\": {\n",
      "                        \"metric\": \"Selection Accuracy\",\n",
      "                        \"value_added\": \"Improves picking of the correct answer among generated candidates.\"\n",
      "                    }\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n",
      "{'output': [{'approach': [{'approach_name': 'Value retrieval', 'description': 'Retrieving relevant database values for SQL query generation.', 'steps': [{'step_title': 'Keyword Extraction', 'details': 'Extract keywords from the given question using an LLM prompted with few-shot examples.'}, {'step_title': 'LSH Retrieval', 'details': 'Use locality-sensitive hashing (LSH) to retrieve the most syntactically-similar words.'}, {'step_title': 'Re-ranking', 'details': 'Re-rank retrieved words based on embedding-based similarity and edit distance.'}], 'improvements': {'metric': 'Robustness', 'value_added': 'Improves robustness to typos and considers keyword semantics.'}}, {'approach_name': 'Candidate generator', 'description': 'Generate diverse candidate SQL queries using LLMs.', 'steps': [{'step_title': 'Diversity in Generation', 'details': 'Increase next token sampling temperature and shuffle order of columns and tables.'}, {'step_title': 'Chain-of-Thought Prompting', 'details': 'Use few-shot examples to guide LLMs on thinking step-by-step.'}], 'improvements': {'metric': 'Diversity', 'value_added': 'Increases likelihood of generating at least one correct answer.'}}, {'approach_name': 'Query fixer', 'description': 'Fix syntactically incorrect SQL queries using LLM-based self-reflection.', 'steps': [{'step_title': 'Iterative Fixing', 'details': 'Reflect on previously generated query using feedback such as syntax error details.'}], 'improvements': {'metric': 'Accuracy', 'value_added': 'Corrects queries that fail to provide correct answers.'}}, {'approach_name': 'Selection agent', 'description': 'Select the correct SQL query from a set of candidates using a trained selection model.', 'steps': [{'step_title': 'Pairwise Comparison', 'details': 'Compare candidates pairwise using a selection model to pick the correct answer.'}], 'improvements': {'metric': 'Selection Accuracy', 'value_added': 'Improves picking of the correct answer among generated candidates.'}}]}]}\n",
      "{'path': 'approach[].approach_name', 'type': 'string', 'description': 'Name of the method or technique', 'node_type': 'branch'}\n",
      "{'path': 'approach[].description', 'type': 'string', 'description': 'Brief summary of what the method is', 'node_type': 'branch'}\n",
      "{'path': 'approach[].steps[]', 'type': 'array<object>', 'description': 'Step-by-step breakdown of the approach', 'node_type': 'branch'}\n",
      "{'path': 'approach[].steps[].step_title', 'type': 'string', 'description': 'title of the step', 'node_type': 'branch'}\n",
      "{'path': 'approach[].steps[].details', 'type': 'string', 'description': 'more details regarding the step', 'node_type': 'branch'}\n",
      "{'path': 'approach[].improvements', 'type': 'object', 'description': 'Impact of the approach', 'node_type': 'branch'}\n",
      "{'path': 'approach[].improvements.metric', 'type': 'string', 'description': '', 'node_type': 'branch'}\n",
      "{'path': 'approach[].improvements.value_added', 'type': 'string', 'description': '', 'node_type': 'branch'}\n",
      "{'path': 'dataset[]', 'type': 'array<object>', 'description': 'List of datasets used for training and experimentation', 'node_type': 'trunk'}\n",
      "{'output': ['4 Experiments -> sub-headings[0] -> 4.1 Datasets and Models']}\n",
      " We evaluate the performance of the proposed CHASE-SQL framework on two widely-recognized cross-domain datasets: BIRD (Li et al., 2024c) and Spider (Yu et al., 2018). BIRD contains over 12,751 unique question-SQL pairs from 95 large databases, spanning more than 37 professional domains, with databases designed to resemble real-world scenarios, featuring messy data rows and complex schemas. Spider consists of 10,181 questions and 5,693 unique complex SQL queries across 200 databases, covering 138 domains. The Spider dataset is divided into non-overlapping training, development, and test sets similar to BIRD. For both, we use execution accuracy (EX), the official metric for their respective leaderboard, as the primary evaluation metric to compare methods. Details of the models and their hyperparameters are provided in Appendix section A.2. \n",
      "\n",
      "## TASK: Given a sub-json from a schema-json with it's metadata, based on format, fetch the value from given content.\n",
      "Follow the 'GUIDELINES' strictly\n",
      "\n",
      "## GUIDELINES:\n",
      "1. sub-json is given in 'SUB JSON', this is the sub-json for which we are fetching the value.\n",
      "2. Return list of these sub-json based on guidelines\n",
      "5. 'OUTPUT FORMAT', this tells which format should be of the output.\n",
      "6. 'CONTENT' contains all the content, from where you will be extracting information. Output cannot be empty as 'CONTENT' exists\n",
      "7. ONLY RETURN THE JSON, NO OTHER INFO\n",
      "\n",
      "## CONTENT:\n",
      " We evaluate the performance of the proposed CHASE-SQL framework on two widely-recognized cross-domain datasets: BIRD (Li et al., 2024c) and Spider (Yu et al., 2018). BIRD contains over 12,751 unique question-SQL pairs from 95 large databases, spanning more than 37 professional domains, with databases designed to resemble real-world scenarios, featuring messy data rows and complex schemas. Spider consists of 10,181 questions and 5,693 unique complex SQL queries across 200 databases, covering 138 domains. The Spider dataset is divided into non-overlapping training, development, and test sets similar to BIRD. For both, we use execution accuracy (EX), the official metric for their respective leaderboard, as the primary evaluation metric to compare methods. Details of the models and their hyperparameters are provided in Appendix section A.2. \n",
      "\n",
      "## SUB JSON:\n",
      "[{'path': 'dataset[]', 'type': 'array<object>', 'description': 'List of datasets used for training and experimentation', 'node_type': 'trunk'}, {'path': 'dataset[].name', 'type': 'string', 'description': 'Name of the dataset used here', 'node_type': 'branch'}, {'path': 'dataset[].source', 'type': 'string', 'description': 'source of the dataset, if known', 'node_type': 'branch'}, {'path': 'dataset[].preprocessing', 'type': 'object', 'description': 'preprocessing steps if any done in the paper', 'node_type': 'branch'}, {'path': 'dataset[].preprocessing.steps', 'type': 'string', 'description': 'Steps done for preprocessing in numbered format', 'node_type': 'branch'}, {'path': 'dataset[].preprocessing.tools_used', 'type': 'string', 'description': 'any specific tools used for preprocessing', 'node_type': 'branch'}]\n",
      "\n",
      "## OUTPUT FORMAT:\n",
      "```json\n",
      "{\n",
      "    'output': [\n",
      "        {'dataset': [{'name': '', 'source': '', 'preprocessing': {'steps': '', 'tools_used': ''}}]}\n",
      "    ]\n",
      "}\n",
      "```\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"output\": [\n",
      "        {\n",
      "            \"dataset\": [\n",
      "                {\n",
      "                    \"name\": \"BIRD\",\n",
      "                    \"source\": \"Li et al., 2024c\",\n",
      "                    \"preprocessing\": {\n",
      "                        \"steps\": \"\",\n",
      "                        \"tools_used\": \"\"\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"name\": \"Spider\",\n",
      "                    \"source\": \"Yu et al., 2018\",\n",
      "                    \"preprocessing\": {\n",
      "                        \"steps\": \"\",\n",
      "                        \"tools_used\": \"\"\n",
      "                    }\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n",
      "{'output': [{'dataset': [{'name': 'BIRD', 'source': 'Li et al., 2024c', 'preprocessing': {'steps': '', 'tools_used': ''}}, {'name': 'Spider', 'source': 'Yu et al., 2018', 'preprocessing': {'steps': '', 'tools_used': ''}}]}]}\n",
      "{'path': 'dataset[].name', 'type': 'string', 'description': 'Name of the dataset used here', 'node_type': 'branch'}\n",
      "{'path': 'dataset[].source', 'type': 'string', 'description': 'source of the dataset, if known', 'node_type': 'branch'}\n",
      "{'path': 'dataset[].preprocessing', 'type': 'object', 'description': 'preprocessing steps if any done in the paper', 'node_type': 'branch'}\n",
      "{'path': 'dataset[].preprocessing.steps', 'type': 'string', 'description': 'Steps done for preprocessing in numbered format', 'node_type': 'branch'}\n",
      "{'path': 'dataset[].preprocessing.tools_used', 'type': 'string', 'description': 'any specific tools used for preprocessing', 'node_type': 'branch'}\n",
      "{'path': 'experiment_results[]', 'type': 'array<object>', 'description': 'Summarized studies and their results, if available', 'node_type': 'trunk'}\n",
      "{'output': ['4 Experiments -> sub-headings[1]', '4 Experiments -> sub-headings[2]', '4 Experiments -> sub-headings[3]', '4 Experiments -> sub-headings[4]']}\n",
      " We present the end-to-end Text-to-SQL performance of the proposed CHASE-SQL framework using Claude- 3.5-sonnet and Gemini 1.5 pro on the BIRD development set, and Gemini 1.5 pro on the BIRD test set. We compare with both published methods (either with an available codebase and/or paper) and undisclosed methods. For a fair comparison with Gemini 1.5 pro, all LLM calls in the Claude-3.5-sonnet setting, except for the selection model, are made using Claude-3.5-sonnet (previously-trained selection model is reused). As shown in Table 2, CHASE-SQL with Gemini 1.5 pro achieves 73.01% accuracy on the BIRD development set and 73.0% on the BIRD holdout test set, outperforming all previous works and setting a new state-of-the-art performance. Table 2: Performance Comparison of different Text-to-SQL methods on BIRD benchmark. Method EX (Dev) EX (Test) Published CHASE-SQL + Gemini 1.5 (Ours) 73.01 73.0 CHASE-SQL + Claude 3.5 Sonnet (Ours) 69.53 – Distillery + GPT-4o (Maamari et al., 2024) 67.21 71.83 CHESS (Talaei et al., 2024) 65.00 66.69 MCS-SQL + GPT-4 (Lee et al., 2024) 63.36 65.45 SuperSQL (Li et al., 2024a) 58.5 62.66 Undisclosed Insights AI 72.16 70.26 AskData + GPT-4o 72.03 72.39 OpenSearch-v2 + GPT-4o 69.3 72.28 PURPLE-RED + GPT-4o 68.12 70.21 Arcwise + GPT-4o 67.99 66.21 ExSL + granite-34b-code 67.47 67.76 Table 3: Performance Comparison of different Text-to-SQL methods on Spider test set. Method EX Training with Spider MCS-SQL + GPT-4 (Lee et al., 2024) 89.6 ✓ CHASE-SQL + Gemini 1.5 (Ours) 87.6 ✗ CHESS (Talaei et al., 2024) 87.2 ✗ DAIL-SQL + GPT-4 (Gao et al., 2023) 86.6 ✓ DIN-SQL + GPT-4 (Pourreza & Rafiei, 2024a) 85.3 ✓ C3 + ChatGPT (Dong et al., 2023) 82.3 ✓ RESDSQL 3B (Li et al., 2023a) 79.9 ✓ DIN-SQL + CodeX (Pourreza & Rafiei, 2024a) 78.2 ✓ T5-3B+NatSQL (Rai et al., 2023) 78.0 ✓ Graphix-3B+PICARD (Li et al., 2023b) 77.6 ✓  We assess the generalizability of the proposed CHASE-SQL by evaluating it in an end-to-end way on the Spider test set without modifying the few-shot samples in the prompts or training a new selection model, i.e. without using and data from the target distribution. This approach allows us to test the performance of CHASE-SQL on different unseen query and database distributions compared to the data from training distributions. Table 3 demonstrates that CHASE-SQL achieves an execution accuracy of 87.6% on the Spider test set, placing it second among methods that have undergone specific training or prompt optimization for the Spider dataset. This highlights the strong generalizability of CHASE-SQL and its potential for generating high quality Text-to-SQL for unseen samples coming from very different distributions and unique challenges. 7  Table 4: Ablation studies on single candidate gen- eration performance of the candidate generators compared with original BIRD prompt + zero-shot CoT with Gemini 1.5 pro on the BIRD dev set. Selector is our final performance applying the se- lection agent to 21 candidates generated from all generators. Method Execution Accuracy (%) ∆(%) Baseline 57.75 - QP CoT 63.62 +5.87 DC CoT 63.92 +6.17 OS CoT 67.09 +9.34 Baseline w Query Fixer 61.58 +3.83 QP CoT w Query Fixer 65.51 +7.76 DC CoT w Query Fixer 65.77 +8.02 OS CoT w Query Fixer 68.02 +10.27 Generator + Fixer: To reveal performance of genera- tors, we conducted an ablation study to evaluate the per- formance of each candidate generation method before and after applying the query fixer. We compare the performance of the proposed generators in producing a single candidate query against the original BIRD prompt (Li et al., 2024c), augmented with zero-shot CoT reasoning (Kojima et al., 2022), which serves as the baseline for assessing the quality of prompts. The results, shown in Table 4, indicate that the proposed methods significantly improve SQL generation performance, compared to the naive baseline, towards the goal of producing high-quality candidates while maintain- ing diversity. Among the candidate generators, the online synthetic data generation approach produced an impressive performance of 68.02%, demonstrating its effectiveness in leveraging test-time compute to improve LLM performance by generating high-quality synthetic examples. Furthermore, the query fixer proved crucial, enhancing the quality of the candidate pool and increasing performance by nearly 2% across all candidate generators. Table 5: Evaluating the binary selection accuracy of the different selection models.   Model Binary Acc. (%) Claude-3.5-sonnet 60.21 Gemini-1.5-pro 63.98 Tuned Gemma 2 9B 64.28 Tuned Gemini-1.5-flash 71.01 Selection: We conducted an analysis on the binary selection accu- racy of the selection agent for cases where, in a pairwise comparison, one candidate is correct and the other is incorrect. We exclude cases where both candidates are either correct or incorrect, as the selection would not affect the outcome since both candidates have the same label. We compare the performance of Claude-3.5-sonnet and Gemini-1.5-pro (both out-of-the-box without fine-tuning) with two fine-tuned models: 1) Gemma 2 9B and 2) Gemini-1.5-flash. As shown in Table 5, both fine-tuned models achieve higher accuracy than the untuned counterparts, demonstrating the importance of fine-tuning to teach the model about the specific preferences.  : We analyze the performance of each candidate generator method individually. To better understand the performance potential when effectively selecting the correct SQL query from the candidate pool, we generate seven candidate SQL queries from each generator method (21 candidates in total) for all samples in the BIRD development set. We determine this number of candidates based on the observation that increasing the candidate pool beyond 20 did not yield significant improvements, as illustrated in Fig. 2d. By assuming access to an oracle selection model that always selects the correct SQL query from the seven candidates, we calculate the upper-bound performance achievable for each generator. Conversely, by assuming an adversarial selection model that always selects the wrong SQL query, we determine the lower-bound performance. Fig. 2 illustrates the upper-bound and lower-bound performance for all three methods together with the performance of our selection agent. As shown, the upper-bound performance of the two different CoT methods is generally higher than that of the synthetic example generation method for different number of candidates. However, their lower-bound performance is also lower than the synthetic method. Lower-bound accuracy reflects cases where all candidates are correct, reducing the noise in the selection process since it doesn’t matter which candidate is chosen, so a higher lower-bound is preferred. This is evident in the selection agent’s performance, where a drop in the lower bound leads to diminishing returns from increasing the upper bound, causing the selection agent’s performance to plateau. Additionally, the upper-bound performance of combining all three methods reaches 82.79%, highlighting the significant room for improvement through better candidate picking methods. This demonstrates that the LLM’s parametric knowledge already contains the information needed to solve most questions, highlighting the need for ensemble approaches to effectively extract and utilize this knowledge. 8 (a) Upper-bound and lower-bound Accuracy for Divide and Conquer CoT (b) Upper-bound and lower-bound Accuracy for Online Synthetic Example (c) Upper-bound and lower-bound performance for Query Plan CoT. (d) Upper-bound performance of all three can- didate generators across different difficulty cat- egories. Figure 2: Comparison of the upper- and lower-bound performance of different candidate generators. Additionally, we evaluate the upper-bound performance by combining all candidates from three candidate generation methods across the simple, moderate, and challenging difficulty levels for the BIRD development set. These difficulty categories are assigned by human experts during the creation of the BIRD development set. Fig. 2d shows that, as expected, the upper-bound performance increases with the number of candidates across all difficulty levels. However, for the challenging and moderate classes, the improvement plateaus earlier than in the simple class, suggesting that generating more samples does not further improve the upper-bound performance for these two difficulty levels. Fig. 2 presents a Venn diagram showcasing the performance of three generation methods: Query Plan, Divide and Conquer, and with Synthetic Examples. The numbers within the intersecting regions represent the instances where multiple methods generated at least one correct candidate. This diagram visually highlights the unique contributions of each method, which indicates the necessity of using all three generators. Additionally, in Fig. 3b, we compare the number of correct queries generated by each SQL generation method that are not correct by the other generators. The divide-and-conquer approach outperforms the others on challenging questions, while the query plan method excels on moderately difficult queries. To further analyze the performance of the generators across different domains and varying numbers of columns and tables, we compare the number of correct queries generated for each database, as shown in Appendix Fig. 4. As illustrated, both CoT methods generally perform similarly across databases, while the online synthetic example generation method significantly increases diversity, resulting in more correct answers overall across different databases.  : We evaluate the query-picking performance by comparing the Text-to-SQL execution accuracy of the selection agent with the self-consistency method (using majority voting) Wang et al. (2022), an oracle model (upper bound), and an adversarial model (lower bound). To conduct the evaluation, we generate 10 samples from each candidate generation method using two different sampling temperatures: 0.5 and 1.8. The results, shown in Table 6, demonstrate that the selection agent significantly outperforms the self-consistency method with a large margin, roughly 6%. As expected, increasing the sampling temperature 9 35 30 33 38 72 23 1045       Unsolved Questions: 258 (a) Venn diagram illustrating the num- ber of instances for which each method: Query Plan, Synthetic Example, Divide and Conquer, produces at least one cor- rect candidate. The overlap regions repre- sent multiple methods generating correct candidates. (b) Number of correct queries across different complexity levels that were answered by each method. Figure 3: Comparison of SQL generation methods: Venn diagram showing unique and overlapping correct answers (left) and the performance across different complexity levels (right). raises the upper bound but also lowers the lower bound. This effect is more pronounced for the synthetic data generation method compared to the two CoT methods, mainly because LLMs generate reasoning steps before producing the final SQL query, which helps mitigate the randomness introduced by high-temperature sampling. The performance with self-consistency method generally decreases as temperature increases, since the majority cluster becomes smaller with more random queries. However, the proposed trained selection agent is less affected by temperature scaling and, in two cases, even improved its performance with a more diverse pool of samples. Table 6: Performance comparison of different picking methods on the candidates generated by the candidate generators on BIRD development set with two different temperatures. QP refers to query plan COT, DC refers to divide and conquer COT, and OS is the online synthetic example generation method. Picking Method QP (T=0.5) QP (T=1.8) DC (T=0.5) DC (T=1.8) OS (T=0.5) OS (T=1.8) Lower Bound 50.46 48.63 51.37 47.39 60.43 50.98 Upper Bound 78.55 80.44 78.42 79.34 74.77 79.66 Self-consistency 65.78 65.51 66.43 64.41 67.34 66.88 Our Selection Agent 71.7 71.73 71.31 70.53 70.4 71.38  In the previous sections, we evaluate the importance of the selection agent and each candidate generation method. Next, we focus on the analysis of the remaining components of CHASE-SQL: LSH for value retrieval, the query fixer, and three reasoning strategies (QP, OS, and DC). Table 7 shows the performance of CHASE- SQL without each of these steps, highlighting their significance in achieving higher-quality performance. The results demonstrate the contribution of each component, where removing LSH, the query fixer, or any of the candidate generators leads to a reduction in execution accuracy, further validating the importance of these components of CHASE-SQL. Moreover, the table compares the performance of our binary selection agent with two other selection methods: self-consistency (Wang et al., 2022) and a ranker agent. The ranker agent receives all candidates generated by our three candidate generators in a single prompt, compares them, and produce a ranking for each. For the ranker agent, we select the query with the lowest rank as the best answer. The binary selection agent significantly outperforms both the self-consistency and ranker agents, demonstrating the effectiveness of the proposed method. 10 \n",
      "\n",
      "## TASK: Given a sub-json from a schema-json with it's metadata, based on format, fetch the value from given content.\n",
      "Follow the 'GUIDELINES' strictly\n",
      "\n",
      "## GUIDELINES:\n",
      "1. sub-json is given in 'SUB JSON', this is the sub-json for which we are fetching the value.\n",
      "2. Return list of these sub-json based on guidelines\n",
      "5. 'OUTPUT FORMAT', this tells which format should be of the output.\n",
      "6. 'CONTENT' contains all the content, from where you will be extracting information. Output cannot be empty as 'CONTENT' exists\n",
      "7. ONLY RETURN THE JSON, NO OTHER INFO\n",
      "\n",
      "## CONTENT:\n",
      " We present the end-to-end Text-to-SQL performance of the proposed CHASE-SQL framework using Claude- 3.5-sonnet and Gemini 1.5 pro on the BIRD development set, and Gemini 1.5 pro on the BIRD test set. We compare with both published methods (either with an available codebase and/or paper) and undisclosed methods. For a fair comparison with Gemini 1.5 pro, all LLM calls in the Claude-3.5-sonnet setting, except for the selection model, are made using Claude-3.5-sonnet (previously-trained selection model is reused). As shown in Table 2, CHASE-SQL with Gemini 1.5 pro achieves 73.01% accuracy on the BIRD development set and 73.0% on the BIRD holdout test set, outperforming all previous works and setting a new state-of-the-art performance. Table 2: Performance Comparison of different Text-to-SQL methods on BIRD benchmark. Method EX (Dev) EX (Test) Published CHASE-SQL + Gemini 1.5 (Ours) 73.01 73.0 CHASE-SQL + Claude 3.5 Sonnet (Ours) 69.53 – Distillery + GPT-4o (Maamari et al., 2024) 67.21 71.83 CHESS (Talaei et al., 2024) 65.00 66.69 MCS-SQL + GPT-4 (Lee et al., 2024) 63.36 65.45 SuperSQL (Li et al., 2024a) 58.5 62.66 Undisclosed Insights AI 72.16 70.26 AskData + GPT-4o 72.03 72.39 OpenSearch-v2 + GPT-4o 69.3 72.28 PURPLE-RED + GPT-4o 68.12 70.21 Arcwise + GPT-4o 67.99 66.21 ExSL + granite-34b-code 67.47 67.76 Table 3: Performance Comparison of different Text-to-SQL methods on Spider test set. Method EX Training with Spider MCS-SQL + GPT-4 (Lee et al., 2024) 89.6 ✓ CHASE-SQL + Gemini 1.5 (Ours) 87.6 ✗ CHESS (Talaei et al., 2024) 87.2 ✗ DAIL-SQL + GPT-4 (Gao et al., 2023) 86.6 ✓ DIN-SQL + GPT-4 (Pourreza & Rafiei, 2024a) 85.3 ✓ C3 + ChatGPT (Dong et al., 2023) 82.3 ✓ RESDSQL 3B (Li et al., 2023a) 79.9 ✓ DIN-SQL + CodeX (Pourreza & Rafiei, 2024a) 78.2 ✓ T5-3B+NatSQL (Rai et al., 2023) 78.0 ✓ Graphix-3B+PICARD (Li et al., 2023b) 77.6 ✓  We assess the generalizability of the proposed CHASE-SQL by evaluating it in an end-to-end way on the Spider test set without modifying the few-shot samples in the prompts or training a new selection model, i.e. without using and data from the target distribution. This approach allows us to test the performance of CHASE-SQL on different unseen query and database distributions compared to the data from training distributions. Table 3 demonstrates that CHASE-SQL achieves an execution accuracy of 87.6% on the Spider test set, placing it second among methods that have undergone specific training or prompt optimization for the Spider dataset. This highlights the strong generalizability of CHASE-SQL and its potential for generating high quality Text-to-SQL for unseen samples coming from very different distributions and unique challenges. 7  Table 4: Ablation studies on single candidate gen- eration performance of the candidate generators compared with original BIRD prompt + zero-shot CoT with Gemini 1.5 pro on the BIRD dev set. Selector is our final performance applying the se- lection agent to 21 candidates generated from all generators. Method Execution Accuracy (%) ∆(%) Baseline 57.75 - QP CoT 63.62 +5.87 DC CoT 63.92 +6.17 OS CoT 67.09 +9.34 Baseline w Query Fixer 61.58 +3.83 QP CoT w Query Fixer 65.51 +7.76 DC CoT w Query Fixer 65.77 +8.02 OS CoT w Query Fixer 68.02 +10.27 Generator + Fixer: To reveal performance of genera- tors, we conducted an ablation study to evaluate the per- formance of each candidate generation method before and after applying the query fixer. We compare the performance of the proposed generators in producing a single candidate query against the original BIRD prompt (Li et al., 2024c), augmented with zero-shot CoT reasoning (Kojima et al., 2022), which serves as the baseline for assessing the quality of prompts. The results, shown in Table 4, indicate that the proposed methods significantly improve SQL generation performance, compared to the naive baseline, towards the goal of producing high-quality candidates while maintain- ing diversity. Among the candidate generators, the online synthetic data generation approach produced an impressive performance of 68.02%, demonstrating its effectiveness in leveraging test-time compute to improve LLM performance by generating high-quality synthetic examples. Furthermore, the query fixer proved crucial, enhancing the quality of the candidate pool and increasing performance by nearly 2% across all candidate generators. Table 5: Evaluating the binary selection accuracy of the different selection models.   Model Binary Acc. (%) Claude-3.5-sonnet 60.21 Gemini-1.5-pro 63.98 Tuned Gemma 2 9B 64.28 Tuned Gemini-1.5-flash 71.01 Selection: We conducted an analysis on the binary selection accu- racy of the selection agent for cases where, in a pairwise comparison, one candidate is correct and the other is incorrect. We exclude cases where both candidates are either correct or incorrect, as the selection would not affect the outcome since both candidates have the same label. We compare the performance of Claude-3.5-sonnet and Gemini-1.5-pro (both out-of-the-box without fine-tuning) with two fine-tuned models: 1) Gemma 2 9B and 2) Gemini-1.5-flash. As shown in Table 5, both fine-tuned models achieve higher accuracy than the untuned counterparts, demonstrating the importance of fine-tuning to teach the model about the specific preferences.  : We analyze the performance of each candidate generator method individually. To better understand the performance potential when effectively selecting the correct SQL query from the candidate pool, we generate seven candidate SQL queries from each generator method (21 candidates in total) for all samples in the BIRD development set. We determine this number of candidates based on the observation that increasing the candidate pool beyond 20 did not yield significant improvements, as illustrated in Fig. 2d. By assuming access to an oracle selection model that always selects the correct SQL query from the seven candidates, we calculate the upper-bound performance achievable for each generator. Conversely, by assuming an adversarial selection model that always selects the wrong SQL query, we determine the lower-bound performance. Fig. 2 illustrates the upper-bound and lower-bound performance for all three methods together with the performance of our selection agent. As shown, the upper-bound performance of the two different CoT methods is generally higher than that of the synthetic example generation method for different number of candidates. However, their lower-bound performance is also lower than the synthetic method. Lower-bound accuracy reflects cases where all candidates are correct, reducing the noise in the selection process since it doesn’t matter which candidate is chosen, so a higher lower-bound is preferred. This is evident in the selection agent’s performance, where a drop in the lower bound leads to diminishing returns from increasing the upper bound, causing the selection agent’s performance to plateau. Additionally, the upper-bound performance of combining all three methods reaches 82.79%, highlighting the significant room for improvement through better candidate picking methods. This demonstrates that the LLM’s parametric knowledge already contains the information needed to solve most questions, highlighting the need for ensemble approaches to effectively extract and utilize this knowledge. 8 (a) Upper-bound and lower-bound Accuracy for Divide and Conquer CoT (b) Upper-bound and lower-bound Accuracy for Online Synthetic Example (c) Upper-bound and lower-bound performance for Query Plan CoT. (d) Upper-bound performance of all three can- didate generators across different difficulty cat- egories. Figure 2: Comparison of the upper- and lower-bound performance of different candidate generators. Additionally, we evaluate the upper-bound performance by combining all candidates from three candidate generation methods across the simple, moderate, and challenging difficulty levels for the BIRD development set. These difficulty categories are assigned by human experts during the creation of the BIRD development set. Fig. 2d shows that, as expected, the upper-bound performance increases with the number of candidates across all difficulty levels. However, for the challenging and moderate classes, the improvement plateaus earlier than in the simple class, suggesting that generating more samples does not further improve the upper-bound performance for these two difficulty levels. Fig. 2 presents a Venn diagram showcasing the performance of three generation methods: Query Plan, Divide and Conquer, and with Synthetic Examples. The numbers within the intersecting regions represent the instances where multiple methods generated at least one correct candidate. This diagram visually highlights the unique contributions of each method, which indicates the necessity of using all three generators. Additionally, in Fig. 3b, we compare the number of correct queries generated by each SQL generation method that are not correct by the other generators. The divide-and-conquer approach outperforms the others on challenging questions, while the query plan method excels on moderately difficult queries. To further analyze the performance of the generators across different domains and varying numbers of columns and tables, we compare the number of correct queries generated for each database, as shown in Appendix Fig. 4. As illustrated, both CoT methods generally perform similarly across databases, while the online synthetic example generation method significantly increases diversity, resulting in more correct answers overall across different databases.  : We evaluate the query-picking performance by comparing the Text-to-SQL execution accuracy of the selection agent with the self-consistency method (using majority voting) Wang et al. (2022), an oracle model (upper bound), and an adversarial model (lower bound). To conduct the evaluation, we generate 10 samples from each candidate generation method using two different sampling temperatures: 0.5 and 1.8. The results, shown in Table 6, demonstrate that the selection agent significantly outperforms the self-consistency method with a large margin, roughly 6%. As expected, increasing the sampling temperature 9 35 30 33 38 72 23 1045       Unsolved Questions: 258 (a) Venn diagram illustrating the num- ber of instances for which each method: Query Plan, Synthetic Example, Divide and Conquer, produces at least one cor- rect candidate. The overlap regions repre- sent multiple methods generating correct candidates. (b) Number of correct queries across different complexity levels that were answered by each method. Figure 3: Comparison of SQL generation methods: Venn diagram showing unique and overlapping correct answers (left) and the performance across different complexity levels (right). raises the upper bound but also lowers the lower bound. This effect is more pronounced for the synthetic data generation method compared to the two CoT methods, mainly because LLMs generate reasoning steps before producing the final SQL query, which helps mitigate the randomness introduced by high-temperature sampling. The performance with self-consistency method generally decreases as temperature increases, since the majority cluster becomes smaller with more random queries. However, the proposed trained selection agent is less affected by temperature scaling and, in two cases, even improved its performance with a more diverse pool of samples. Table 6: Performance comparison of different picking methods on the candidates generated by the candidate generators on BIRD development set with two different temperatures. QP refers to query plan COT, DC refers to divide and conquer COT, and OS is the online synthetic example generation method. Picking Method QP (T=0.5) QP (T=1.8) DC (T=0.5) DC (T=1.8) OS (T=0.5) OS (T=1.8) Lower Bound 50.46 48.63 51.37 47.39 60.43 50.98 Upper Bound 78.55 80.44 78.42 79.34 74.77 79.66 Self-consistency 65.78 65.51 66.43 64.41 67.34 66.88 Our Selection Agent 71.7 71.73 71.31 70.53 70.4 71.38  In the previous sections, we evaluate the importance of the selection agent and each candidate generation method. Next, we focus on the analysis of the remaining components of CHASE-SQL: LSH for value retrieval, the query fixer, and three reasoning strategies (QP, OS, and DC). Table 7 shows the performance of CHASE- SQL without each of these steps, highlighting their significance in achieving higher-quality performance. The results demonstrate the contribution of each component, where removing LSH, the query fixer, or any of the candidate generators leads to a reduction in execution accuracy, further validating the importance of these components of CHASE-SQL. Moreover, the table compares the performance of our binary selection agent with two other selection methods: self-consistency (Wang et al., 2022) and a ranker agent. The ranker agent receives all candidates generated by our three candidate generators in a single prompt, compares them, and produce a ranking for each. For the ranker agent, we select the query with the lowest rank as the best answer. The binary selection agent significantly outperforms both the self-consistency and ranker agents, demonstrating the effectiveness of the proposed method. 10 \n",
      "\n",
      "## SUB JSON:\n",
      "[{'path': 'experiment_results[]', 'type': 'array<object>', 'description': 'Summarized studies and their results, if available', 'node_type': 'trunk'}, {'path': 'experiment_results[].experiment_name', 'type': 'string', 'description': 'name of the experiment done', 'node_type': 'branch'}, {'path': 'experiment_results[].metrics[]', 'type': 'array<object>', 'description': 'what all metrics were checked during the process', 'node_type': 'branch'}, {'path': 'experiment_results[].metrics[].metric_name', 'type': 'string', 'description': 'name of the metric checked', 'node_type': 'branch'}, {'path': 'experiment_results[].metrics[].value', 'type': 'string', 'description': 'value of the results, if known', 'node_type': 'branch'}]\n",
      "\n",
      "## OUTPUT FORMAT:\n",
      "```json\n",
      "{\n",
      "    'output': [\n",
      "        {'experiment_results': [{'experiment_name': '', 'metrics': [{'metric_name': '', 'value': ''}]}]}\n",
      "    ]\n",
      "}\n",
      "```\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"output\": [\n",
      "        {\n",
      "            \"experiment_results\": [\n",
      "                {\n",
      "                    \"experiment_name\": \"CHASE-SQL + Gemini 1.5 (Ours)\",\n",
      "                    \"metrics\": [\n",
      "                        {\n",
      "                            \"metric_name\": \"EX (Dev)\",\n",
      "                            \"value\": \"73.01\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"metric_name\": \"EX (Test)\",\n",
      "                            \"value\": \"73.0\"\n",
      "                        }\n",
      "                    ]\n",
      "                },\n",
      "                {\n",
      "                    \"experiment_name\": \"CHASE-SQL + Claude 3.5 Sonnet (Ours)\",\n",
      "                    \"metrics\": [\n",
      "                        {\n",
      "                            \"metric_name\": \"EX (Dev)\",\n",
      "                            \"value\": \"69.53\"\n",
      "                        }\n",
      "                    ]\n",
      "                },\n",
      "                {\n",
      "                    \"experiment_name\": \"Distillery + GPT-4o (Maamari et al., 2024)\",\n",
      "                    \"metrics\": [\n",
      "                        {\n",
      "                            \"metric_name\": \"EX (Dev)\",\n",
      "                            \"value\": \"67.21\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"metric_name\": \"EX (Test)\",\n",
      "                            \"value\": \"71.83\"\n",
      "                        }\n",
      "                    ]\n",
      "                },\n",
      "                {\n",
      "                    \"experiment_name\": \"CHESS (Talaei et al., 2024)\",\n",
      "                    \"metrics\": [\n",
      "                        {\n",
      "                            \"metric_name\": \"EX (Dev)\",\n",
      "                            \"value\": \"65.00\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"metric_name\": \"EX (Test)\",\n",
      "                            \"value\": \"66.69\"\n",
      "                        }\n",
      "                    ]\n",
      "                },\n",
      "                {\n",
      "                    \"experiment_name\": \"MCS-SQL + GPT-4 (Lee et al., 2024)\",\n",
      "                    \"metrics\": [\n",
      "                        {\n",
      "                            \"metric_name\": \"EX (Dev)\",\n",
      "                            \"value\": \"63.36\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"metric_name\": \"EX (Test)\",\n",
      "                            \"value\": \"65.45\"\n",
      "                        }\n",
      "                    ]\n",
      "                },\n",
      "                {\n",
      "                    \"experiment_name\": \"SuperSQL (Li et al., 2024a)\",\n",
      "                    \"metrics\": [\n",
      "                        {\n",
      "                            \"metric_name\": \"EX (Dev)\",\n",
      "                            \"value\": \"58.5\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"metric_name\": \"EX (Test)\",\n",
      "                            \"value\": \"62.66\"\n",
      "                        }\n",
      "                    ]\n",
      "                },\n",
      "                {\n",
      "                    \"experiment_name\": \"Undisclosed Insights AI\",\n",
      "                    \"metrics\": [\n",
      "                        {\n",
      "                            \"metric_name\": \"EX (Dev)\",\n",
      "                            \"value\": \"72.16\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"metric_name\": \"EX (Test)\",\n",
      "                            \"value\": \"70.26\"\n",
      "                        }\n",
      "                    ]\n",
      "                },\n",
      "                {\n",
      "                    \"experiment_name\": \"AskData + GPT-4o\",\n",
      "                    \"metrics\": [\n",
      "                        {\n",
      "                            \"metric_name\": \"EX (Dev)\",\n",
      "                            \"value\": \"72.03\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"metric_name\": \"EX (Test)\",\n",
      "                            \"value\": \"72.39\"\n",
      "                        }\n",
      "                    ]\n",
      "                },\n",
      "                {\n",
      "                    \"experiment_name\": \"OpenSearch-v2 + GPT-4o\",\n",
      "                    \"metrics\": [\n",
      "                        {\n",
      "                            \"metric_name\": \"EX (Dev)\",\n",
      "                            \"value\": \"69.3\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"metric_name\": \"EX (Test)\",\n",
      "                            \"value\": \"72.28\"\n",
      "                        }\n",
      "                    ]\n",
      "                },\n",
      "                {\n",
      "                    \"experiment_name\": \"PURPLE-RED + GPT-4o\",\n",
      "                    \"metrics\": [\n",
      "                        {\n",
      "                            \"metric_name\": \"EX (Dev)\",\n",
      "                            \"value\": \"68.12\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"metric_name\": \"EX (Test)\",\n",
      "                            \"value\": \"70.21\"\n",
      "                        }\n",
      "                    ]\n",
      "                },\n",
      "                {\n",
      "                    \"experiment_name\": \"Arcwise + GPT-4o\",\n",
      "                    \"metrics\": [\n",
      "                        {\n",
      "                            \"metric_name\": \"EX (Dev)\",\n",
      "                            \"value\": \"67.99\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"metric_name\": \"EX (Test)\",\n",
      "                            \"value\": \"66.21\"\n",
      "                        }\n",
      "                    ]\n",
      "                },\n",
      "                {\n",
      "                    \"experiment_name\": \"ExSL + granite-34b-code\",\n",
      "                    \"metrics\": [\n",
      "                        {\n",
      "                            \"metric_name\": \"EX (Dev)\",\n",
      "                            \"value\": \"67.47\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"metric_name\": \"EX (Test)\",\n",
      "                            \"value\": \"67.76\"\n",
      "                        }\n",
      "                    ]\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n",
      "{'output': [{'experiment_results': [{'experiment_name': 'CHASE-SQL + Gemini 1.5 (Ours)', 'metrics': [{'metric_name': 'EX (Dev)', 'value': '73.01'}, {'metric_name': 'EX (Test)', 'value': '73.0'}]}, {'experiment_name': 'CHASE-SQL + Claude 3.5 Sonnet (Ours)', 'metrics': [{'metric_name': 'EX (Dev)', 'value': '69.53'}]}, {'experiment_name': 'Distillery + GPT-4o (Maamari et al., 2024)', 'metrics': [{'metric_name': 'EX (Dev)', 'value': '67.21'}, {'metric_name': 'EX (Test)', 'value': '71.83'}]}, {'experiment_name': 'CHESS (Talaei et al., 2024)', 'metrics': [{'metric_name': 'EX (Dev)', 'value': '65.00'}, {'metric_name': 'EX (Test)', 'value': '66.69'}]}, {'experiment_name': 'MCS-SQL + GPT-4 (Lee et al., 2024)', 'metrics': [{'metric_name': 'EX (Dev)', 'value': '63.36'}, {'metric_name': 'EX (Test)', 'value': '65.45'}]}, {'experiment_name': 'SuperSQL (Li et al., 2024a)', 'metrics': [{'metric_name': 'EX (Dev)', 'value': '58.5'}, {'metric_name': 'EX (Test)', 'value': '62.66'}]}, {'experiment_name': 'Undisclosed Insights AI', 'metrics': [{'metric_name': 'EX (Dev)', 'value': '72.16'}, {'metric_name': 'EX (Test)', 'value': '70.26'}]}, {'experiment_name': 'AskData + GPT-4o', 'metrics': [{'metric_name': 'EX (Dev)', 'value': '72.03'}, {'metric_name': 'EX (Test)', 'value': '72.39'}]}, {'experiment_name': 'OpenSearch-v2 + GPT-4o', 'metrics': [{'metric_name': 'EX (Dev)', 'value': '69.3'}, {'metric_name': 'EX (Test)', 'value': '72.28'}]}, {'experiment_name': 'PURPLE-RED + GPT-4o', 'metrics': [{'metric_name': 'EX (Dev)', 'value': '68.12'}, {'metric_name': 'EX (Test)', 'value': '70.21'}]}, {'experiment_name': 'Arcwise + GPT-4o', 'metrics': [{'metric_name': 'EX (Dev)', 'value': '67.99'}, {'metric_name': 'EX (Test)', 'value': '66.21'}]}, {'experiment_name': 'ExSL + granite-34b-code', 'metrics': [{'metric_name': 'EX (Dev)', 'value': '67.47'}, {'metric_name': 'EX (Test)', 'value': '67.76'}]}]}]}\n",
      "{'path': 'experiment_results[].experiment_name', 'type': 'string', 'description': 'name of the experiment done', 'node_type': 'branch'}\n",
      "{'path': 'experiment_results[].metrics[]', 'type': 'array<object>', 'description': 'what all metrics were checked during the process', 'node_type': 'branch'}\n",
      "{'path': 'experiment_results[].metrics[].metric_name', 'type': 'string', 'description': 'name of the metric checked', 'node_type': 'branch'}\n",
      "{'path': 'experiment_results[].metrics[].value', 'type': 'string', 'description': 'value of the results, if known', 'node_type': 'branch'}\n",
      "{'path': 'references[]', 'type': 'array<string>', 'description': 'List of references used in the paper', 'node_type': 'trunk'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'schema_field': {'path': 'approach[]',\n",
       "   'type': 'array<object>',\n",
       "   'description': 'List of methods used in the approach',\n",
       "   'node_type': 'trunk'},\n",
       "  'data': [{'approach': [{'approach_name': 'Value retrieval',\n",
       "      'description': 'Retrieving relevant database values for SQL query generation.',\n",
       "      'steps': [{'step_title': 'Keyword Extraction',\n",
       "        'details': 'Extract keywords from the given question using an LLM prompted with few-shot examples.'},\n",
       "       {'step_title': 'LSH Retrieval',\n",
       "        'details': 'Use locality-sensitive hashing (LSH) to retrieve the most syntactically-similar words.'},\n",
       "       {'step_title': 'Re-ranking',\n",
       "        'details': 'Re-rank retrieved words based on embedding-based similarity and edit distance.'}],\n",
       "      'improvements': {'metric': 'Robustness',\n",
       "       'value_added': 'Improves robustness to typos and considers keyword semantics.'}},\n",
       "     {'approach_name': 'Candidate generator',\n",
       "      'description': 'Generate diverse candidate SQL queries using LLMs.',\n",
       "      'steps': [{'step_title': 'Diversity in Generation',\n",
       "        'details': 'Increase next token sampling temperature and shuffle order of columns and tables.'},\n",
       "       {'step_title': 'Chain-of-Thought Prompting',\n",
       "        'details': 'Use few-shot examples to guide LLMs on thinking step-by-step.'}],\n",
       "      'improvements': {'metric': 'Diversity',\n",
       "       'value_added': 'Increases likelihood of generating at least one correct answer.'}},\n",
       "     {'approach_name': 'Query fixer',\n",
       "      'description': 'Fix syntactically incorrect SQL queries using LLM-based self-reflection.',\n",
       "      'steps': [{'step_title': 'Iterative Fixing',\n",
       "        'details': 'Reflect on previously generated query using feedback such as syntax error details.'}],\n",
       "      'improvements': {'metric': 'Accuracy',\n",
       "       'value_added': 'Corrects queries that fail to provide correct answers.'}},\n",
       "     {'approach_name': 'Selection agent',\n",
       "      'description': 'Select the correct SQL query from a set of candidates using a trained selection model.',\n",
       "      'steps': [{'step_title': 'Pairwise Comparison',\n",
       "        'details': 'Compare candidates pairwise using a selection model to pick the correct answer.'}],\n",
       "      'improvements': {'metric': 'Selection Accuracy',\n",
       "       'value_added': 'Improves picking of the correct answer among generated candidates.'}}]}]},\n",
       " {'schema_field': {'path': 'dataset[]',\n",
       "   'type': 'array<object>',\n",
       "   'description': 'List of datasets used for training and experimentation',\n",
       "   'node_type': 'trunk'},\n",
       "  'data': [{'dataset': [{'name': 'BIRD',\n",
       "      'source': 'Li et al., 2024c',\n",
       "      'preprocessing': {'steps': '', 'tools_used': ''}},\n",
       "     {'name': 'Spider',\n",
       "      'source': 'Yu et al., 2018',\n",
       "      'preprocessing': {'steps': '', 'tools_used': ''}}]}]},\n",
       " {'schema_field': {'path': 'experiment_results[]',\n",
       "   'type': 'array<object>',\n",
       "   'description': 'Summarized studies and their results, if available',\n",
       "   'node_type': 'trunk'},\n",
       "  'data': [{'experiment_results': [{'experiment_name': 'CHASE-SQL + Gemini 1.5 (Ours)',\n",
       "      'metrics': [{'metric_name': 'EX (Dev)', 'value': '73.01'},\n",
       "       {'metric_name': 'EX (Test)', 'value': '73.0'}]},\n",
       "     {'experiment_name': 'CHASE-SQL + Claude 3.5 Sonnet (Ours)',\n",
       "      'metrics': [{'metric_name': 'EX (Dev)', 'value': '69.53'}]},\n",
       "     {'experiment_name': 'Distillery + GPT-4o (Maamari et al., 2024)',\n",
       "      'metrics': [{'metric_name': 'EX (Dev)', 'value': '67.21'},\n",
       "       {'metric_name': 'EX (Test)', 'value': '71.83'}]},\n",
       "     {'experiment_name': 'CHESS (Talaei et al., 2024)',\n",
       "      'metrics': [{'metric_name': 'EX (Dev)', 'value': '65.00'},\n",
       "       {'metric_name': 'EX (Test)', 'value': '66.69'}]},\n",
       "     {'experiment_name': 'MCS-SQL + GPT-4 (Lee et al., 2024)',\n",
       "      'metrics': [{'metric_name': 'EX (Dev)', 'value': '63.36'},\n",
       "       {'metric_name': 'EX (Test)', 'value': '65.45'}]},\n",
       "     {'experiment_name': 'SuperSQL (Li et al., 2024a)',\n",
       "      'metrics': [{'metric_name': 'EX (Dev)', 'value': '58.5'},\n",
       "       {'metric_name': 'EX (Test)', 'value': '62.66'}]},\n",
       "     {'experiment_name': 'Undisclosed Insights AI',\n",
       "      'metrics': [{'metric_name': 'EX (Dev)', 'value': '72.16'},\n",
       "       {'metric_name': 'EX (Test)', 'value': '70.26'}]},\n",
       "     {'experiment_name': 'AskData + GPT-4o',\n",
       "      'metrics': [{'metric_name': 'EX (Dev)', 'value': '72.03'},\n",
       "       {'metric_name': 'EX (Test)', 'value': '72.39'}]},\n",
       "     {'experiment_name': 'OpenSearch-v2 + GPT-4o',\n",
       "      'metrics': [{'metric_name': 'EX (Dev)', 'value': '69.3'},\n",
       "       {'metric_name': 'EX (Test)', 'value': '72.28'}]},\n",
       "     {'experiment_name': 'PURPLE-RED + GPT-4o',\n",
       "      'metrics': [{'metric_name': 'EX (Dev)', 'value': '68.12'},\n",
       "       {'metric_name': 'EX (Test)', 'value': '70.21'}]},\n",
       "     {'experiment_name': 'Arcwise + GPT-4o',\n",
       "      'metrics': [{'metric_name': 'EX (Dev)', 'value': '67.99'},\n",
       "       {'metric_name': 'EX (Test)', 'value': '66.21'}]},\n",
       "     {'experiment_name': 'ExSL + granite-34b-code',\n",
       "      'metrics': [{'metric_name': 'EX (Dev)', 'value': '67.47'},\n",
       "       {'metric_name': 'EX (Test)', 'value': '67.76'}]}]}]}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from t2j.extractor import FieldExtractor\n",
    "from t2j.prompts import Prompts\n",
    "from t2j.prem_sdk import PremSDK\n",
    "\n",
    "promptsClass = Prompts()\n",
    "model = PremSDK()\n",
    "\n",
    "e = FieldExtractor(model, promptsClass)\n",
    "e.extract(chunks, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': [{'approach': [{'approach_name': 'Value retrieval',\n",
       "     'description': 'Retrieving relevant database values for use in SQL clauses.',\n",
       "     'steps': [{'step_title': 'Keyword extraction',\n",
       "       'details': 'Extract keywords from the given question using an LLM prompted with few-shot examples.'},\n",
       "      {'step_title': 'LSH retrieval',\n",
       "       'details': 'Use locality-sensitive hashing to retrieve the most syntactically-similar words.'},\n",
       "      {'step_title': 'Re-ranking',\n",
       "       'details': 'Re-rank retrieved words based on embedding-based similarity and edit distance.'}],\n",
       "     'improvements': {'metric': 'Robustness',\n",
       "      'value_added': 'Improved handling of typos and keyword semantics during retrieval.'}},\n",
       "    {'approach_name': 'Candidate generator',\n",
       "     'description': 'Generate diverse candidate queries using LLMs.',\n",
       "     'steps': [{'step_title': 'Diversity in generation',\n",
       "       'details': 'Increase the next token sampling temperature and shuffle the order of columns and tables in the prompt.'}],\n",
       "     'improvements': {'metric': 'Diversity',\n",
       "      'value_added': 'Increased likelihood of generating at least one correct answer.'}},\n",
       "    {'approach_name': 'Query fixer',\n",
       "     'description': 'Fix syntactically incorrect queries using self-reflection.',\n",
       "     'steps': [{'step_title': 'Iterative fixing',\n",
       "       'details': 'Reflect on the previously generated query using feedback to guide the correction process.'}],\n",
       "     'improvements': {'metric': 'Accuracy',\n",
       "      'value_added': 'Corrects queries that fail to provide the correct answers.'}},\n",
       "    {'approach_name': 'Selection agent',\n",
       "     'description': 'Select the final response using a trained selection agent.',\n",
       "     'steps': [{'step_title': 'Pairwise comparison',\n",
       "       'details': 'Compare candidates pairwise using a selection model to pick the correct answer.'}],\n",
       "     'improvements': {'metric': 'Selection accuracy',\n",
       "      'value_added': 'More accurate decisions by learning the nuances between similar candidates.'}}]}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'output': [{'approach': [{'approach_name': 'Value retrieval', 'description': 'Retrieving relevant database values for use in SQL clauses.', 'steps': [{'step_title': 'Keyword extraction', 'details': 'Extract keywords from the given question using an LLM prompted with few-shot examples.'}, {'step_title': 'LSH retrieval', 'details': 'Use locality-sensitive hashing to retrieve the most syntactically-similar words.'}, {'step_title': 'Re-ranking', 'details': 'Re-rank retrieved words based on embedding-based similarity and edit distance.'}], 'improvements': {'metric': 'Robustness', 'value_added': 'Improved handling of typos and keyword semantics during retrieval.'}}, {'approach_name': 'Candidate generator', 'description': 'Generate diverse candidate queries using LLMs.', 'steps': [{'step_title': 'Diversity in generation', 'details': 'Increase the next token sampling temperature and shuffle the order of columns and tables in the prompt.'}], 'improvements': {'metric': 'Diversity', 'value_added': 'Increased likelihood of generating at least one correct answer.'}}, {'approach_name': 'Query fixer', 'description': 'Fix syntactically incorrect queries using self-reflection.', 'steps': [{'step_title': 'Iterative fixing', 'details': 'Reflect on the previously generated query using feedback to guide the correction process.'}], 'improvements': {'metric': 'Accuracy', 'value_added': 'Corrects queries that fail to provide the correct answers.'}}, {'approach_name': 'Selection agent', 'description': 'Select the final response using a trained selection agent.', 'steps': [{'step_title': 'Pairwise comparison', 'details': 'Compare candidates pairwise using a selection model to pick the correct answer.'}], 'improvements': {'metric': 'Selection accuracy', 'value_added': 'More accurate decisions by learning the nuances between similar candidates.'}}]}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3 Methods -> sub-headings[0] -> 3.1 Overall Framework',\n",
       " '3 Methods -> sub-headings[1] -> 3.2 Value Retrieval',\n",
       " '3 Methods -> sub-headings[2] -> 3.3 Multi-path Candidate Generation',\n",
       " '3 Methods -> sub-headings[2] -> 3.3 Multi-path Candidate Generation -> sub-headings[0] -> Divide and Conquer CoT',\n",
       " '3 Methods -> sub-headings[2] -> 3.3 Multi-path Candidate Generation -> sub-headings[1] -> Query Plan CoT',\n",
       " '3 Methods -> sub-headings[3] -> 3.4 Query Fixer',\n",
       " '3 Methods -> sub-headings[4] -> 3.5 Selection Agent']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['3 Methods -> sub-headings[0] -> 3.1 Overall Framework', '3 Methods -> sub-headings[1] -> 3.2 Value Retrieval', '3 Methods -> sub-headings[2] -> 3.3 Multi-path Candidate Generation', '3 Methods -> sub-headings[2] -> 3.3 Multi-path Candidate Generation -> sub-headings[0] -> Divide and Conquer CoT', '3 Methods -> sub-headings[2] -> 3.3 Multi-path Candidate Generation -> sub-headings[1] -> Query Plan CoT', '3 Methods -> sub-headings[3] -> 3.4 Query Fixer', '3 Methods -> sub-headings[4] -> 3.5 Selection Agent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'path': 'title',\n",
       "  'type': 'string',\n",
       "  'description': 'Title of the paper or document',\n",
       "  'node_type': 'trunk'},\n",
       " {'path': 'related_work[]',\n",
       "  'type': 'array<string>',\n",
       "  'description': 'List of strings, with related works done; keep the points short and precise',\n",
       "  'node_type': 'trunk'},\n",
       " {'path': 'authors[]',\n",
       "  'type': 'array<string>',\n",
       "  'description': 'List of authors of the paper',\n",
       "  'node_type': 'trunk'},\n",
       " {'path': 'abstract',\n",
       "  'type': 'string',\n",
       "  'description': 'Up to 5 bullet points describing what is new in the approach',\n",
       "  'node_type': 'trunk'},\n",
       " {'path': 'approach[]',\n",
       "  'type': 'array<object>',\n",
       "  'description': 'List of methods used in the approach',\n",
       "  'node_type': 'trunk'},\n",
       " {'path': 'approach[].approach_name',\n",
       "  'type': 'string',\n",
       "  'description': 'Name of the method or technique',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'approach[].description',\n",
       "  'type': 'string',\n",
       "  'description': 'Brief summary of what the method is',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'approach[].steps[]',\n",
       "  'type': 'array<object>',\n",
       "  'description': 'Step-by-step breakdown of the approach',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'approach[].steps[].step_title',\n",
       "  'type': 'string',\n",
       "  'description': 'title of the step',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'approach[].steps[].details',\n",
       "  'type': 'string',\n",
       "  'description': 'more details regarding the step',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'approach[].improvements',\n",
       "  'type': 'object',\n",
       "  'description': 'Impact of the approach',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'approach[].improvements.metric',\n",
       "  'type': 'string',\n",
       "  'description': '',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'approach[].improvements.value_added',\n",
       "  'type': 'string',\n",
       "  'description': '',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'dataset[]',\n",
       "  'type': 'array<object>',\n",
       "  'description': 'List of datasets used for training and experimentation',\n",
       "  'node_type': 'trunk'},\n",
       " {'path': 'dataset[].name',\n",
       "  'type': 'string',\n",
       "  'description': 'Name of the dataset used here',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'dataset[].source',\n",
       "  'type': 'string',\n",
       "  'description': 'source of the dataset, if known',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'dataset[].preprocessing',\n",
       "  'type': 'object',\n",
       "  'description': 'preprocessing steps if any done in the paper',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'dataset[].preprocessing.steps',\n",
       "  'type': 'string',\n",
       "  'description': 'Steps done for preprocessing in numbered format',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'dataset[].preprocessing.tools_used',\n",
       "  'type': 'string',\n",
       "  'description': 'any specific tools used for preprocessing',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'experiment_results[]',\n",
       "  'type': 'array<object>',\n",
       "  'description': 'Summarized studies and their results, if available',\n",
       "  'node_type': 'trunk'},\n",
       " {'path': 'experiment_results[].experiment_name',\n",
       "  'type': 'string',\n",
       "  'description': 'name of the experiment done',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'experiment_results[].metrics[]',\n",
       "  'type': 'array<object>',\n",
       "  'description': 'what all metrics were checked during the process',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'experiment_results[].metrics[].metric_name',\n",
       "  'type': 'string',\n",
       "  'description': 'name of the metric checked',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'experiment_results[].metrics[].value',\n",
       "  'type': 'string',\n",
       "  'description': 'value of the results, if known',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'references[]',\n",
       "  'type': 'array<string>',\n",
       "  'description': 'List of references used in the paper',\n",
       "  'node_type': 'trunk'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_path = \"approach[]\"\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [r for r in res if r['path'].startswith(field_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'path': 'approach[]',\n",
       "  'type': 'array<object>',\n",
       "  'description': 'List of methods used in the approach',\n",
       "  'node_type': 'trunk'},\n",
       " {'path': 'approach[].approach_name',\n",
       "  'type': 'string',\n",
       "  'description': 'Name of the method or technique',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'approach[].description',\n",
       "  'type': 'string',\n",
       "  'description': 'Brief summary of what the method is',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'approach[].steps[]',\n",
       "  'type': 'array<object>',\n",
       "  'description': 'Step-by-step breakdown of the approach',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'approach[].steps[].step_title',\n",
       "  'type': 'string',\n",
       "  'description': 'title of the step',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'approach[].steps[].details',\n",
       "  'type': 'string',\n",
       "  'description': 'more details regarding the step',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'approach[].improvements',\n",
       "  'type': 'object',\n",
       "  'description': 'Impact of the approach',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'approach[].improvements.metric',\n",
       "  'type': 'string',\n",
       "  'description': '',\n",
       "  'node_type': 'branch'},\n",
       " {'path': 'approach[].improvements.value_added',\n",
       "  'type': 'string',\n",
       "  'description': '',\n",
       "  'node_type': 'branch'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"approach\": [\n",
      "    {\n",
      "      \"approach_name\": \"\",\n",
      "      \"description\": \"\",\n",
      "      \"steps\": [\n",
      "        {\n",
      "          \"step_title\": \"\",\n",
      "          \"details\": \"\"\n",
      "        }\n",
      "      ],\n",
      "      \"improvements\": {\n",
      "        \"metric\": \"\",\n",
      "        \"value_added\": \"\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def create_empty_json_from_skeleton(skeleton):\n",
    "    result = {}\n",
    "    \n",
    "    for item in skeleton:\n",
    "        path = item['path']\n",
    "        parts = path.split('.')\n",
    "        current = result\n",
    "        \n",
    "        for i, part in enumerate(parts):\n",
    "            if part.endswith('[]'):\n",
    "                # Handle array case\n",
    "                key = part[:-2]\n",
    "                if key not in current:\n",
    "                    current[key] = []\n",
    "                    # For arrays of objects, add an empty object if it's the last part\n",
    "                    if i == len(parts) - 1 and item['type'] == 'array<object>':\n",
    "                        current[key].append({})\n",
    "                # Move into the array (or create object if needed)\n",
    "                if i < len(parts) - 1:\n",
    "                    if not current[key]:\n",
    "                        current[key].append({})\n",
    "                    current = current[key][0]\n",
    "                else:\n",
    "                    # For leaf arrays that aren't objects, just leave as empty array\n",
    "                    pass\n",
    "            else:\n",
    "                # Handle object case\n",
    "                if part not in current:\n",
    "                    if i == len(parts) - 1:\n",
    "                        # Leaf node - set appropriate empty value based on type\n",
    "                        if item['type'] == 'string':\n",
    "                            current[part] = \"\"\n",
    "                        elif item['type'] == 'object':\n",
    "                            current[part] = {}\n",
    "                        elif item['type'].startswith('array'):\n",
    "                            current[part] = []\n",
    "                    else:\n",
    "                        current[part] = {}\n",
    "                current = current[part]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Given skeleton\n",
    "skeleton = [\n",
    "    {'path': 'approach[]', 'type': 'array<object>', 'description': 'List of methods used in the approach', 'node_type': 'trunk'},\n",
    "    {'path': 'approach[].approach_name', 'type': 'string', 'description': 'Name of the method or technique', 'node_type': 'branch'},\n",
    "    {'path': 'approach[].description', 'type': 'string', 'description': 'Brief summary of what the method is', 'node_type': 'branch'},\n",
    "    {'path': 'approach[].steps[]', 'type': 'array<object>', 'description': 'Step-by-step breakdown of the approach', 'node_type': 'branch'},\n",
    "    {'path': 'approach[].steps[].step_title', 'type': 'string', 'description': 'title of the step', 'node_type': 'branch'},\n",
    "    {'path': 'approach[].steps[].details', 'type': 'string', 'description': 'more details regarding the step', 'node_type': 'branch'},\n",
    "    {'path': 'approach[].improvements', 'type': 'object', 'description': 'Impact of the approach', 'node_type': 'branch'},\n",
    "    {'path': 'approach[].improvements.metric', 'type': 'string', 'description': '', 'node_type': 'branch'},\n",
    "    {'path': 'approach[].improvements.value_added', 'type': 'string', 'description': '', 'node_type': 'branch'}\n",
    "]\n",
    "\n",
    "# Create the empty JSON\n",
    "empty_json = create_empty_json_from_skeleton(skeleton)\n",
    "\n",
    "# Print the result\n",
    "import json\n",
    "print(json.dumps(empty_json, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': ['2 Related Work']}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.extract_json(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': 2, 'sub-headings': []}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[model.extract_json(output)['output'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_all_content_with_ids(data):\n",
    "    counter = {\"value\": 1}\n",
    "    mapping = {}\n",
    "\n",
    "    def recurse(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            for key, value in obj.items():\n",
    "                if key == \"content\":\n",
    "                    mapping[counter[\"value\"]] = value\n",
    "                    obj[key] = counter[\"value\"]\n",
    "                    counter[\"value\"] += 1\n",
    "                else:\n",
    "                    recurse(value)\n",
    "\n",
    "            if \"sub-headings\" in obj and isinstance(obj[\"sub-headings\"], list):\n",
    "                for item in obj[\"sub-headings\"]:\n",
    "                    for sub_key, sub_value in item.items():\n",
    "                        recurse(sub_value)\n",
    "\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                recurse(item)\n",
    "\n",
    "    recurse(data)\n",
    "    return data, mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "copy_chunks = copy.deepcopy(chunks)\n",
    "data, mapping = replace_all_content_with_ids(copy_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1 Introduction': {'content': 1, 'sub-headings': []},\n",
       " '2 Related Work': {'content': 2, 'sub-headings': []},\n",
       " '3 Methods': {'content': 3,\n",
       "  'sub-headings': [{'3.1 Overall Framework': {'content': 13,\n",
       "     'sub-headings': []}},\n",
       "   {'3.2 Value Retrieval': {'content': 14, 'sub-headings': []}},\n",
       "   {'3.3 Multi-path Candidate Generation': {'content': 15,\n",
       "     'sub-headings': [{'Divide and Conquer CoT': {'content': 18,\n",
       "        'sub-headings': []}},\n",
       "      {'Query Plan CoT': {'content': 19, 'sub-headings': []}}]}},\n",
       "   {'3.4 Query Fixer': {'content': 20, 'sub-headings': []}},\n",
       "   {'3.5 Selection Agent': {'content': 21, 'sub-headings': []}}]},\n",
       " '4 Online Synthetic Example Generation': {'content': 22, 'sub-headings': []},\n",
       " '4 Experiments': {'content': 23,\n",
       "  'sub-headings': [{'4.1 Datasets and Models': {'content': 41,\n",
       "     'sub-headings': []}},\n",
       "   {'4.2 BIRD results': {'content': 42, 'sub-headings': []}},\n",
       "   {'4.3 Spider results': {'content': 43, 'sub-headings': []}},\n",
       "   {'4.4 Generator and selection performance': {'content': 44,\n",
       "     'sub-headings': [{'Selection': {'content': 51, 'sub-headings': []}},\n",
       "      {'Candidate Generation Analysis': {'content': 52, 'sub-headings': []}},\n",
       "      {'Selection Agent Analysis': {'content': 53, 'sub-headings': []}},\n",
       "      {'Query Plan': {'content': 54, 'sub-headings': []}},\n",
       "      {'Synthetic Example': {'content': 55, 'sub-headings': []}},\n",
       "      {'Divide and Conquer': {'content': 56, 'sub-headings': []}}]}},\n",
       "   {'4.5 Ablation Studies': {'content': 57, 'sub-headings': []}}]},\n",
       " '5 Conclusion': {'content': 58, 'sub-headings': []},\n",
       " 'Acknowledgments': {'content': 59, 'sub-headings': []},\n",
       " 'References': {'content': 60, 'sub-headings': []}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Early Text-to-SQL methods predominantly utilized sequence-to-sequence architectures, encoding user queries and database schemas using models such as Graph Neural Networks (GNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and pre-trained transformer encoders (Cai et al., 2021; Cao et al., 2021; Hwang et al., 2019). On the decoding side, these systems employed either slot-filling or auto-regressive modelling approaches to construct the final SQL queries from the encoded inputs (Choi et al., 2021; Wang et al., 2019). Additionally, tabular language models like TaBERT (Yin et al., 2020), 2 TaPas (Herzig et al., 2020), and Grappa (Yu et al., 2020) have been developed to encode both tables and textual data effectively. However, the landscape has evolved with the widespread use of LLMs, which have largely replaced earlier methods with their superior performance (Katsogiannis-Meimarakis & Koutrika, 2023; Quamar et al., 2022). Initially, efforts concentrated on optimizing prompt designs for these LLMs (Dong et al., 2023; Gao et al., 2023; Pourreza & Rafiei, 2024a). Subsequent advancements have introduced more complex methodologies, including schema linking (Li et al., 2024b; Pourreza & Rafiei, 2024a,b; Talaei et al., 2024), self-correction or self-debugging (Chen et al., 2023; Talaei et al., 2024; Wang et al., 2023), and self-consistency techniques (Lee et al., 2024; Maamari et al., 2024; Sun et al., 2023; Talaei et al., 2024), further enhancing the performance by proposing complex LLM-based pipelines. '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
