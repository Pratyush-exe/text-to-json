{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from t2j.prem_sdk import PremSDK  \n",
    "from t2j.prompts import Prompts\n",
    "from t2j.chunker import DocumentChunker\n",
    "from t2j.decomposer import SchemaDecomposer\n",
    "from t2j.utils import *\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size_lines = 100\n",
    "promptsClass = Prompts()\n",
    "model = PremSDK()\n",
    "chunker = DocumentChunker(prompts=promptsClass, model=model)\n",
    "\n",
    "FILE_PATH = r\"C:\\Users\\Pratyush\\Desktop\\text-to-json\\examples\\text2SQL-solutions\\chase-sql.pdf\"\n",
    "SCHEMA_PATH = r\"C:\\Users\\Pratyush\\Desktop\\text-to-json\\examples\\text2SQL-solutions\\schema.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|██████████| 15/15 [00:02<00:00,  5.39it/s]\n",
      "100%|██████████| 28/28 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# step 1\n",
    "chunks = chunker.smart_chunk(FILE_PATH)\n",
    "other_data = chunker.extract_other_info(FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Abstract\": {\n",
      "        \"content\": \" In tackling the challenges of large language model (LLM) performance for Text-to-SQL tasks, we introduce CHASE-SQL, a new framework that employs innovative strategies, using test-time compute in multi-agent modeling to improve candidate generation and selection. CHASE-SQL leverages LLMs intrinsic knowledge to generate diverse and high-quality SQL candidates using different LLM generators with: (1) a divide-and-conquer method that decomposes complex queries into manageable sub-queries in a single LLM call; (2) chain-of-thought reasoning based on query execution plans, reflecting the steps a database engine takes during execution; and (3) a unique instance-aware synthetic example generation technique, which offers specific few-shot demonstrations tailored to test questions. To identify the best candidate, a selection agent is employed to rank the candidates through pairwise comparisons with a fine-tuned binary-candidates selection LLM. This selection approach has been demonstrated to be more robust over alternatives. The proposed generators-selector framework not only enhances the quality and diversity of SQL queries but also outperforms previous methods. Overall, our proposed CHASE-SQL achieves the state-of-the-art execution accuracy of 73.0 % and 73.01% on the test set and development set of the notable BIRD Text-to-SQL dataset benchmark, rendering CHASE-SQL the top submission of the leaderboard (at the time of paper submission). \",\n",
      "        \"sub-headings\": []\n",
      "    },\n",
      "    \"1 Introduction\": {\n",
      "        \"content\": \" Text-to-SQL, as a bridge between human language and machine-readable structured query languages, is crucial for many use cases, converting natural language questions into executable SQL commands (Androutsopoulos et al., 1995; Li & Jagadish, 2014; Li et al., 2024c; Yu et al., 2018; ?). By enabling users to interact with complex database systems without requiring SQL proficiency, Text-to-SQL empowers users to extract valuable insights, perform streamlined data exploration, make informed decisions, generate data-driven reports and mine better features for machine learning (Chen et al., 2023; Prez-Mercado et al., 2023; Pourreza & Rafiei, 2024a; Pourreza et al., 2024; Sun et al., 2023; Wang et al., 2019; Xie et al., 2023). Furthermore, Text-to-SQL systems play a pivotal role in automating data analytics with complex reasoning and powering conversational agents, expanding their applications beyond traditional data retrieval (Sun et al., 2023; Xie et al., 2023). As data continues to grow exponentially, the ability to query databases efficiently without extensive SQL knowledge becomes increasingly vital for a broad range of applications. Text-to-SQL can be considered a specialized form of code generation, with the contextual information potentially including the database schema, its metadata and along with the values. In the broader code generation domain, utilizing LLMs to generate a wide range of diverse candidates and select the best one has proven to be effective (Chen et al., 2021; Li et al., 2022; Ni et al., 2023). However, it is non-obvious what 1 arXiv:2410.01943v1  [cs.LG]  2 Oct 2024 leads to most effective candidate proposal and winner selector mechanisms. A straightforward yet effective approach involves generating candidates using zero-/few-shot or open-ended prompting, followed by selecting the best options utilizing self-consistency (Wang et al., 2022), which entails clustering candidates based on their execution outputs. This approach has demonstrated promising results in several studies (Lee et al., 2024; Maamari et al., 2024; Talaei et al., 2024; Wang et al., 2023). However, a single prompt design might not fully unleash the extensive Text-to-SQL knowledge of LLMs, and self-consistency methods might not be always effective. In fact, as illustrated in Table 1, the most consistent answers would not always be the correct ones, with an upper-bound performance 14% higher than that achieved through self-consistency. This substantial gap highlights the potential for significant improvement by implementing more effective selection methods to identify the best answer from the pool of candidate queries. Table 1: Evaluating single-query gen- eration vs. ensemble methods of self- consistency and the upper bound that can be achieved for Text-to-SQL with Gemini 1.5 Pro on the BIRD dev set. EX stands for execution accuracy. Method EX (%) Single query 63.01 Self-consistency 68.84 (+ 5.84) Upper-bound 82.79 (+ 19.78) Building on the challenges outlined in the previous section, we pro- pose novel approaches to improve LLM performance for Text-to-SQL by leveraging judiciously-designed test-time computations in an agentic framework. As indicated by the upper bound in Table 1, utilizing LLMs intrinsic knowledge offers significant potential for improvement. We propose methods that generate a diverse set of high-quality can- didate responses and apply a selection mechanism to identify the best answer. Achieving both high-quality and diverse candidate responses is critical for the success of scoring-based selection methods. Low diversity limits improvement potential and reduces the difference be- tween self-consistency and scoring-based approaches. While techniques like increasing temperature or reordering prompt contents can boost diversity, they often compromise the quality of the candidates. To address this, we introduce effective candidate generators designed to enhance diversity while maintaining high-quality outputs. Specifically, we propose three distinct candidate generation approaches, each capable of producing high-quality responses. The first is inspired by the divide-and-conquer algorithm, which breaks down complex problems into smaller, manageable parts to handle difficult queries. The second employs a query execution-plan-based chain-of-thought strategy, where the reasoning process mirrors the steps a database engine takes during query execution. Lastly, we introduce a novel online synthetic example generation method, which helps the model better understand the underlying data schema of the test database. These methods, when used independently, can produce highly-accurate SQL outputs. To effectively select the best answer among candidates, we introduce a selection agent, trained with a classification objective, that assigns scores based on pairwise comparisons between candidate queries. With this agent, we construct a comparison matrix for all candidates and select the final response based on the highest cumulative score. By combining these candidate generation methods with the proposed scoring model, we create an ensemble approach that leverages the strengths of each strategy to significantly improve overall performance. We present comprehensive evaluations on the efficacy of proposed methodologies of CHASE-SQL. Our innovative candidate generation approaches demonstrate superior performance compared to traditional generic CoT prompts, illustrating their capability in guiding LLMs through the decomposition of complex problems into manageable intermediate steps. Furthermore, the proposed selection agent significantly outperforms conventional consistency-based methods, contributing to the state-of-the-art results. Specifically, CHASE- SQL reaches an execution accuracy of 73.01% and 73.0% on the development set and test set of the challenging BIRD Text-to-SQL dataset which outperforms all of the published and undisclosed methods on this benchmark, by a large margin. \",\n",
      "        \"sub-headings\": []\n",
      "    },\n",
      "    \"2 Related Work\": {\n",
      "        \"content\": \" Early Text-to-SQL methods predominantly utilized sequence-to-sequence architectures, encoding user queries and database schemas using models such as Graph Neural Networks (GNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and pre-trained transformer encoders (Cai et al., 2021; Cao et al., 2021; Hwang et al., 2019). On the decoding side, these systems employed either slot-filling or auto-regressive modelling approaches to construct the final SQL queries from the encoded inputs (Choi et al., 2021; Wang et al., 2019). Additionally, tabular language models like TaBERT (Yin et al., 2020), 2 TaPas (Herzig et al., 2020), and Grappa (Yu et al., 2020) have been developed to encode both tables and textual data effectively. However, the landscape has evolved with the widespread use of LLMs, which have largely replaced earlier methods with their superior performance (Katsogiannis-Meimarakis & Koutrika, 2023; Quamar et al., 2022). Initially, efforts concentrated on optimizing prompt designs for these LLMs (Dong et al., 2023; Gao et al., 2023; Pourreza & Rafiei, 2024a). Subsequent advancements have introduced more complex methodologies, including schema linking (Li et al., 2024b; Pourreza & Rafiei, 2024a,b; Talaei et al., 2024), self-correction or self-debugging (Chen et al., 2023; Talaei et al., 2024; Wang et al., 2023), and self-consistency techniques (Lee et al., 2024; Maamari et al., 2024; Sun et al., 2023; Talaei et al., 2024), further enhancing the performance by proposing complex LLM-based pipelines. \",\n",
      "        \"sub-headings\": []\n",
      "    },\n",
      "    \"3 Methods\": {\n",
      "        \"content\": \" \",\n",
      "        \"sub-headings\": [\n",
      "            {\n",
      "                \"3.1 Overall Framework\": {\n",
      "                    \"content\": \" This section outlines the proposed CHASE-SQL framework, which consists of four primary components: 1) Value retrieval, 2) Candidate generator, 3) Query fixer, and 4) Selection agent. As illustrated in Fig. 1. The proposed framework begins by retrieving relevant database values. Subsequently, all contextual information, including retrieved values, database metadata, and schema, is provided to an LLM to generate candidate queries. These candidate queries then undergo a fixing loop, and finally, all candidates are compared in a pairwise way using the trained selection agent to pick the correct answer. The following sections delve into the details of each component. Figure 1: Overview of the proposed CHASE-SQL framework for Text-to-SQL, with value retrieval and using a selection agent for improve picking of the answers among the generated candidates along with a fixer to provide feedback for refinement of the outputs. \",\n",
      "                    \"sub-headings\": []\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"3.2 Value Retrieval\": {\n",
      "                    \"content\": \" Databases might contain very high number of rows, with often only a few being relevant to a query. Retrieving relevant values is crucial as they can be used in various SQL clauses like WHERE and HAVING. Similar to the approach in (Talaei et al., 2024), we begin by extracting keywords from the given question using an LLM prompted with few-shot examples. For each keyword, we employ locality-sensitive hashing (LSH) (Datar et al., 2004) to retrieve the most syntactically-similar words, and re-rank them based on embedding-based similarity and edit distance. This approach is robust to typos in the question and considers keyword semantics during retrieval. \",\n",
      "                    \"sub-headings\": []\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"3.3 Multi-path Candidate Generation\": {\n",
      "                    \"content\": \" As shown in Table 1, relying solely on consistency among responses can lead to sub-optimal performance. Therefore, we prioritize diversity in generation of multiple response candidates to increase the likelihood of generating at least one correct answer. Among the diverse responses generated by the candidate generators, we select one as the final response using a selection agent that compares candidates pairwise. To generate 3 diverse responses, we increase the next token sampling temperature, and also shuffle the order of columns and tables in the prompt. Chain-of-Thought (CoT) prompting (Wei et al., 2022) has been proposed to enhance LLMs reasoning abilities by conditioning their final responses on a step-by-step chain of reasoning. Most CoT prompting approaches rely on few-shot examples in the prompt to guide LLMs on thinking step-by-step, following the format M = (qi, ri, si), where qi is the example question, ri is the reasoning path, and si is the ground truth SQL query for qi. We employ two distinct reasoning methods and an online synthetic example generation approach. As shown in Fig. 3a, different generators can yield different outputs, indicating their effectiveness for specific questions and databases. \",\n",
      "                    \"sub-headings\": [\n",
      "                        {\n",
      "                            \"Divide and Conquer CoT\": {\n",
      "                                \"content\": \": Divide-and-conquer perspective brings breaking down complex problems into smaller sub-problems, solving each individually, and then combining the solutions to obtain the final answer. Along these lines, we propose a CoT prompting approach that first decomposes the given question into smaller sub-problems using pseudo-SQL queries. In the conquer step, the solutions to these sub-problems are aggregated to construct the final answer. Finally, an optimization step is applied to the constructed query to remove redundant clauses and conditions. This approach is particularly powerful handling complex scenarios that involve nested queries, e.g. intricate WHERE or HAVING conditions, and queries requiring advanced mathematical operations. In Appendix Fig. 17, we exemplify a question and its corresponding SQL query that was successfully solved using this generator, a scenario the other methods considered in this paper could not address due to the querys complex conditions and SQL clauses. For a more detailed view of the divide-and-conquer prompt, please see Appendix Fig. 16. Additionally, Alg. 1 outlines the step-by-step process of this strategy to generate the final SQL output using a single LLM call. Algorithm 1 Divide and Conquer Chain-of-Thought (CoT) Strategy for Text-to-SQL. Input: Set of human-annotated few-shot examples M, user question Qu, target database D associated with the question, and a large language model (LLM) . Divide: 1: Sq (M, D, Qu) // Decompose the original question Qu into a set of sub-questions Sq 2: Ssql // Initialize an empty set Ssql to store partial SQL queries for each sub-question Conquer: 3: for each sub-question qi in Sq do 4: // Generate a partial SQL query for each sub-question qi 5: Ssql Ssql {(M, D, Qu, q1, ..., qi, sql1, ..., sqli1)} 6: end for Assemble: 7: Sf (M, D, Qu, Sq, Ssql) // Assemble the final SQL query Sf from all sub-queries in Ssql 8: return Sf \",\n",
      "                                \"sub-headings\": []\n",
      "                            }\n",
      "                        },\n",
      "                        {\n",
      "                            \"Query Plan CoT\": {\n",
      "                                \"content\": \": A query (execution) plan is a sequence of steps that the database engine follows to access or modify the data described by a SQL command. When a SQL query is executed, the database management systems query optimizers translate the SQL text into a query plan that the database engine can execute. This plan outlines how tables are accessed, how they are joined, and the specific operations performed on the data (see Appendix Fig. 19 as an example). Inspired by the step-by-step process that database engines use to execute SQL queries, we propose a reasoning strategy to construct the final SQL output. Query plans for any given SQL query can be obtained using the EXPLAIN\\\" command, which provides a detailed breakdown of execution steps. However, this output is often presented in a format that is difficult to interpret by LLMs (e.g. in SQLite). To address this, we convert the output of EXPLAIN\\\" command into a human-readable text format that aligns more closely with the pretraining data of LLMs. The human-readable version of query plans consists of three key steps: (1) identifying and locating the relevant tables for the question, (2) performing operations such as counting, filtering, or matching between tables, and (3) delivering the final result by selecting the appropriate columns to return. This reasoning method complements the divide-and-conquer CoT strategy. While the divide-and-conquer approach is better suited for decomposing complex questions, the query plan approach excels when questions require more reasoning over the relationships between different parts of the question and the database schema. It systematically explains which tables to scan, how to match columns, and how to apply filters. Appendix Fig. 20 shows an example of a question that was answered correctly only by this method. Appendix Fig. 18 provides the prompt used for this reasoning strategy. \",\n",
      "                                \"sub-headings\": []\n",
      "                            }\n",
      "                        }\n",
      "                    ]\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"3.4 Query Fixer\": {\n",
      "                    \"content\": \" In some cases, LLMs might generate queries that are syntactically incorrect. These queries are clear candidates for correction, as they fail to provide the correct answers. To address this, we apply an LLM-based query fixer that leverages the self-reflection (Shinn et al., 2024) method. The fixer reflects on the previously generated query, using feedback such as syntax error details or empty result sets to guide the correction process. We continue this iterative fixing approach up to a specified number of attempts,  (set to three in this paper). Appendix Fig. 21 demonstrates the prompt used for this query fixing step. \",\n",
      "                    \"sub-headings\": []\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"3.5 Selection Agent\": {\n",
      "                    \"content\": \" With three different methods for generating SQL queries, we can generate a set of candidate queries for any given question. The key challenge in this step is selecting the correct SQL query from this pool of candidates. A naive approach would be to measure consistency among the candidates by executing them, grouping them based on their execution results, and selecting a query from the largest group as the most likely correct answer. However, this would assume that the most consistent answer is always the best one, which is not always the case. Instead, we propose a more refined picking strategy, Algorithm 3, that relies on a selection agent. Given a set of candidates SQL queries C = {c1, c2, ..., cn}, the final responses are selected by finding 5 the candidate that has the highest score assigned by the selection model. This model p can take k candidates and rank them based on how accurately each of them answers the given question. Concretely, we formulate the selection of the final response as: cf = arg max cC    (n k) X i=1 p(ci1, . . . , cik | Qu, Hu, D)   , (1) where Qu refers to the users question, Hu is the provided hint, and D is the target database from which the question is being asked. In Eq. 1, we pass k candidates to the selection model to be ranked, with k being between 1 and n. In the extreme case of k = 1, the model is unable to make comparisons between candidates, which complicates the evaluation process for the model. As k increases, comparing more candidates makes the process more challenging for the model, as it needs to consider different aspects simultaneously. Consequently, we set k = 2 and train a model with a classification objective to compare only two candidates at a time. Having a set of high-quality and diverse candidates, the most straightforward solution is to employ off-the-shelf LLMs to make pairwise selections. However, experiments with Gemini-1.5-pro showed that using the LLM without fine-tuning resulted in only 58.01% binary classification accuracy. This is primarily due to the candidates being very similar to one another, requiring a fine-tuned model to learn the nuances and make more accurate decisions. To train the selection agent, we first generate candidate SQL queries on the training set (of Text-to-SQL benchmarks), and group them into clusters based on their execution results. For cases where at least one cluster contains correct queries and others contains incorrect ones, we create training examples in the form of tuples (Qu, Ci, Cj, Dij, yij), where Qu is the users question, Ci and Cj are the two candidate queries being compared, Dij is the database schema used by both candidates, and yij 0, 1 is the label indicating whether Ci or Cj is the correct query. To avoid order bias during training, we randomly shuffle the order of correct and incorrect queries in each pair. Since the number of cases with both correct and incorrect candidates is limited, for instances where no correct candidate exists, we include the ground truth SQL query in the prompt as a hint to guide the model in generating correct candidates. Algorithm 3 Picking the final SQL query from a pool of candidates. Input: Set of candidate SQL queries C = {c1, c2, ..., cn}, user question Qu, hint Hu, target database D, and a selection model p, er(ci, D) as the execution result of ci on D 1: ri 0 for all ci C // Initialize the score ri for each candidate query to zero 2: for each distinct pair (ci, cj) where i = j do 3: if er(ci, D) = er(cj, D) then 4: w i // ci is the winner if the execution results match 5: else 6: Si,j schema_union(ci, cj, D) // Construct union of schemas used in ci and cj 7: w p(Si,j, Qu, Hu, ci, cj)w {i, j} // Use binary classifier p to select the winner, w {i, j} 8: end if 9: rw rw + 1 // Increase the score of the winner cw by 1 10: end for 11: cf arg maxciC ri // Select the candidate with the highest score as the final SQL query cf 12: return cf In the pseudo-code for Algorithm 3, we begin by initializing a score of zero for each candidate query. Then, for every distinct pair of queries (ci, cj), we compare both (ci, cj) and (cj, ci) to mitigate any order bias, ensuring that both candidates in a pair are fairly evaluated. If both queries produce the same execution result on the database, we mark one as the winner and increment its score, as these results suggest consistency. If the execution results differ, we generate a union of the schema used by both queries and use the binary classifier to determine which query is more likely to be correct. The classifier takes into account the question, the two candidate queries, and the combined schema to make its decision. The winners score is then updated accordingly. After all comparisons, the candidate with the highest score is selected as the final query. In the rare case of a tie in the final scores, we break the tie by selecting one of the candidates arbitrarily. 6 \",\n",
      "                    \"sub-headings\": []\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"4 Online Synthetic Example Generation\": {\n",
      "        \"content\": \": Using M demonstrations for few-shot in-context learning has shown promising results on various related tasks (Pourreza & Rafiei, 2024a). Besides helping with specifying the task and illustrate the step-by-step process deriving the output, demonstrations constructed using relevant tables and columns can also help the model understand the underlying data schema. Based on this insight, we propose a synthetic demonstration generation strategy for Text-to-SQL  given the user question Qu, the target database D, and the selected columns ti (using a column selection approach similar to (Talaei et al., 2024)). Algorithm 2 Online Synthetic example generation strategy for Text-to-SQL. Input: User question Qu, additional user hint Hu, target database D and filtered relevant table columns t associated with the question, LLM , guidelines Rf for generating examples by SQL features, guidelines Rt for generating examples with filtered schema, and the numbers of examples to generate nf, nt respectively 1: P // {(qi, si) | qi, si }, where qi is input question , si is output SQL for the i-th example 2: P P {(D, Rf, nf)} // Generate n examples with entire database by common SQL features 3: P P {(t, Rt, nt)} // Generate examples with filtered columns to highlight correct schema usage 4: return P Algorithm 2 outlines the online synthetic example generation approach with two LLM generation steps. The first step focuses on generating illustrative examples with common SQL features described in the guideline Rf. The SQL features include equality and non-equality predicates, single table and multi-table JOIN, nested JOIN, ORDER BY and LIMIT, GROUP BY and HAVING, various aggregation functions. These are widely applicable SQL clauses and functions  the generated example SQL queries, incorporating these features, follow the BIRD SQL feature distribution (Appendix Fig 23a). The second step focuses on generating examples highlighting correct interpretation of the underlying data schema  the model  is asked to generate examples using ti and that are similar to the examples outlined in Rt. Appendix A.10 provides the prompts used for the example generation). While a relevant example (e.g. showing a nested JOIN query with multiple tables) can be helpful for questions that require complex JOIN queries, it might also mislead the LLM for overuse (e.g. when a simple single table query is sufficient). This and the inherent ambiguity of natural language query qi, for which we draw the examples by relevance, make the example selection challenging. Thus, we generate and inject the examples to the prompt online per qi. We ask the LLM to generate many input-output pairs for in-context learning. The final set of synthetic examples for qi contains examples generated with both Rf and Rt. This ensures that the example set is diverse both in SQL features/clauses and the choice of relevant tables/columns used. The diversity of the example set is desirable to avoid over-fitting the output to certain patterns (e.g., the model always writes a SQL with JOIN if shown mostly JOIN examples). Mixing various examples for various SQL features and database tables with and without column filtering is observed to result in better generation quality overall (please see Appendix Table 8). \",\n",
      "        \"sub-headings\": []\n",
      "    },\n",
      "    \"4 Experiments\": {\n",
      "        \"content\": \" \",\n",
      "        \"sub-headings\": [\n",
      "            {\n",
      "                \"4.1 Datasets and Models\": {\n",
      "                    \"content\": \" We evaluate the performance of the proposed CHASE-SQL framework on two widely-recognized cross-domain datasets: BIRD (Li et al., 2024c) and Spider (Yu et al., 2018). BIRD contains over 12,751 unique question-SQL pairs from 95 large databases, spanning more than 37 professional domains, with databases designed to resemble real-world scenarios, featuring messy data rows and complex schemas. Spider consists of 10,181 questions and 5,693 unique complex SQL queries across 200 databases, covering 138 domains. The Spider dataset is divided into non-overlapping training, development, and test sets similar to BIRD. For both, we use execution accuracy (EX), the official metric for their respective leaderboard, as the primary evaluation metric to compare methods. Details of the models and their hyperparameters are provided in Appendix section A.2. \",\n",
      "                    \"sub-headings\": []\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"4.2 BIRD results\": {\n",
      "                    \"content\": \" We present the end-to-end Text-to-SQL performance of the proposed CHASE-SQL framework using Claude- 3.5-sonnet and Gemini 1.5 pro on the BIRD development set, and Gemini 1.5 pro on the BIRD test set. We compare with both published methods (either with an available codebase and/or paper) and undisclosed methods. For a fair comparison with Gemini 1.5 pro, all LLM calls in the Claude-3.5-sonnet setting, except for the selection model, are made using Claude-3.5-sonnet (previously-trained selection model is reused). As shown in Table 2, CHASE-SQL with Gemini 1.5 pro achieves 73.01% accuracy on the BIRD development set and 73.0% on the BIRD holdout test set, outperforming all previous works and setting a new state-of-the-art performance. Table 2: Performance Comparison of different Text-to-SQL methods on BIRD benchmark. Method EX (Dev) EX (Test) Published CHASE-SQL + Gemini 1.5 (Ours) 73.01 73.0 CHASE-SQL + Claude 3.5 Sonnet (Ours) 69.53  Distillery + GPT-4o (Maamari et al., 2024) 67.21 71.83 CHESS (Talaei et al., 2024) 65.00 66.69 MCS-SQL + GPT-4 (Lee et al., 2024) 63.36 65.45 SuperSQL (Li et al., 2024a) 58.5 62.66 Undisclosed Insights AI 72.16 70.26 AskData + GPT-4o 72.03 72.39 OpenSearch-v2 + GPT-4o 69.3 72.28 PURPLE-RED + GPT-4o 68.12 70.21 Arcwise + GPT-4o 67.99 66.21 ExSL + granite-34b-code 67.47 67.76 Table 3: Performance Comparison of different Text-to-SQL methods on Spider test set. Method EX Training with Spider MCS-SQL + GPT-4 (Lee et al., 2024) 89.6  CHASE-SQL + Gemini 1.5 (Ours) 87.6  CHESS (Talaei et al., 2024) 87.2  DAIL-SQL + GPT-4 (Gao et al., 2023) 86.6  DIN-SQL + GPT-4 (Pourreza & Rafiei, 2024a) 85.3  C3 + ChatGPT (Dong et al., 2023) 82.3  RESDSQL 3B (Li et al., 2023a) 79.9  DIN-SQL + CodeX (Pourreza & Rafiei, 2024a) 78.2  T5-3B+NatSQL (Rai et al., 2023) 78.0  Graphix-3B+PICARD (Li et al., 2023b) 77.6  \",\n",
      "                    \"sub-headings\": []\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"4.3 Spider results\": {\n",
      "                    \"content\": \" We assess the generalizability of the proposed CHASE-SQL by evaluating it in an end-to-end way on the Spider test set without modifying the few-shot samples in the prompts or training a new selection model, i.e. without using and data from the target distribution. This approach allows us to test the performance of CHASE-SQL on different unseen query and database distributions compared to the data from training distributions. Table 3 demonstrates that CHASE-SQL achieves an execution accuracy of 87.6% on the Spider test set, placing it second among methods that have undergone specific training or prompt optimization for the Spider dataset. This highlights the strong generalizability of CHASE-SQL and its potential for generating high quality Text-to-SQL for unseen samples coming from very different distributions and unique challenges. 7 \",\n",
      "                    \"sub-headings\": []\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"4.4 Generator and selection performance\": {\n",
      "                    \"content\": \" Table 4: Ablation studies on single candidate gen- eration performance of the candidate generators compared with original BIRD prompt + zero-shot CoT with Gemini 1.5 pro on the BIRD dev set. Selector is our final performance applying the se- lection agent to 21 candidates generated from all generators. Method Execution Accuracy (%) (%) Baseline 57.75 - QP CoT 63.62 +5.87 DC CoT 63.92 +6.17 OS CoT 67.09 +9.34 Baseline w Query Fixer 61.58 +3.83 QP CoT w Query Fixer 65.51 +7.76 DC CoT w Query Fixer 65.77 +8.02 OS CoT w Query Fixer 68.02 +10.27 Generator + Fixer: To reveal performance of genera- tors, we conducted an ablation study to evaluate the per- formance of each candidate generation method before and after applying the query fixer. We compare the performance of the proposed generators in producing a single candidate query against the original BIRD prompt (Li et al., 2024c), augmented with zero-shot CoT reasoning (Kojima et al., 2022), which serves as the baseline for assessing the quality of prompts. The results, shown in Table 4, indicate that the proposed methods significantly improve SQL generation performance, compared to the naive baseline, towards the goal of producing high-quality candidates while maintain- ing diversity. Among the candidate generators, the online synthetic data generation approach produced an impressive performance of 68.02%, demonstrating its effectiveness in leveraging test-time compute to improve LLM performance by generating high-quality synthetic examples. Furthermore, the query fixer proved crucial, enhancing the quality of the candidate pool and increasing performance by nearly 2% across all candidate generators. Table 5: Evaluating the binary selection accuracy of the different selection models. \",\n",
      "                    \"sub-headings\": [\n",
      "                        {\n",
      "                            \"Selection\": {\n",
      "                                \"content\": \" Model Binary Acc. (%) Claude-3.5-sonnet 60.21 Gemini-1.5-pro 63.98 Tuned Gemma 2 9B 64.28 Tuned Gemini-1.5-flash 71.01 Selection: We conducted an analysis on the binary selection accu- racy of the selection agent for cases where, in a pairwise comparison, one candidate is correct and the other is incorrect. We exclude cases where both candidates are either correct or incorrect, as the selection would not affect the outcome since both candidates have the same label. We compare the performance of Claude-3.5-sonnet and Gemini-1.5-pro (both out-of-the-box without fine-tuning) with two fine-tuned models: 1) Gemma 2 9B and 2) Gemini-1.5-flash. As shown in Table 5, both fine-tuned models achieve higher accuracy than the untuned counterparts, demonstrating the importance of fine-tuning to teach the model about the specific preferences. \",\n",
      "                                \"sub-headings\": []\n",
      "                            }\n",
      "                        },\n",
      "                        {\n",
      "                            \"Candidate Generation Analysis\": {\n",
      "                                \"content\": \": We analyze the performance of each candidate generator method individually. To better understand the performance potential when effectively selecting the correct SQL query from the candidate pool, we generate seven candidate SQL queries from each generator method (21 candidates in total) for all samples in the BIRD development set. We determine this number of candidates based on the observation that increasing the candidate pool beyond 20 did not yield significant improvements, as illustrated in Fig. 2d. By assuming access to an oracle selection model that always selects the correct SQL query from the seven candidates, we calculate the upper-bound performance achievable for each generator. Conversely, by assuming an adversarial selection model that always selects the wrong SQL query, we determine the lower-bound performance. Fig. 2 illustrates the upper-bound and lower-bound performance for all three methods together with the performance of our selection agent. As shown, the upper-bound performance of the two different CoT methods is generally higher than that of the synthetic example generation method for different number of candidates. However, their lower-bound performance is also lower than the synthetic method. Lower-bound accuracy reflects cases where all candidates are correct, reducing the noise in the selection process since it doesnt matter which candidate is chosen, so a higher lower-bound is preferred. This is evident in the selection agents performance, where a drop in the lower bound leads to diminishing returns from increasing the upper bound, causing the selection agents performance to plateau. Additionally, the upper-bound performance of combining all three methods reaches 82.79%, highlighting the significant room for improvement through better candidate picking methods. This demonstrates that the LLMs parametric knowledge already contains the information needed to solve most questions, highlighting the need for ensemble approaches to effectively extract and utilize this knowledge. 8 (a) Upper-bound and lower-bound Accuracy for Divide and Conquer CoT (b) Upper-bound and lower-bound Accuracy for Online Synthetic Example (c) Upper-bound and lower-bound performance for Query Plan CoT. (d) Upper-bound performance of all three can- didate generators across different difficulty cat- egories. Figure 2: Comparison of the upper- and lower-bound performance of different candidate generators. Additionally, we evaluate the upper-bound performance by combining all candidates from three candidate generation methods across the simple, moderate, and challenging difficulty levels for the BIRD development set. These difficulty categories are assigned by human experts during the creation of the BIRD development set. Fig. 2d shows that, as expected, the upper-bound performance increases with the number of candidates across all difficulty levels. However, for the challenging and moderate classes, the improvement plateaus earlier than in the simple class, suggesting that generating more samples does not further improve the upper-bound performance for these two difficulty levels. Fig. 2 presents a Venn diagram showcasing the performance of three generation methods: Query Plan, Divide and Conquer, and with Synthetic Examples. The numbers within the intersecting regions represent the instances where multiple methods generated at least one correct candidate. This diagram visually highlights the unique contributions of each method, which indicates the necessity of using all three generators. Additionally, in Fig. 3b, we compare the number of correct queries generated by each SQL generation method that are not correct by the other generators. The divide-and-conquer approach outperforms the others on challenging questions, while the query plan method excels on moderately difficult queries. To further analyze the performance of the generators across different domains and varying numbers of columns and tables, we compare the number of correct queries generated for each database, as shown in Appendix Fig. 4. As illustrated, both CoT methods generally perform similarly across databases, while the online synthetic example generation method significantly increases diversity, resulting in more correct answers overall across different databases. \",\n",
      "                                \"sub-headings\": []\n",
      "                            }\n",
      "                        },\n",
      "                        {\n",
      "                            \"Selection Agent Analysis\": {\n",
      "                                \"content\": \": We evaluate the query-picking performance by comparing the Text-to-SQL execution accuracy of the selection agent with the self-consistency method (using majority voting) Wang et al. (2022), an oracle model (upper bound), and an adversarial model (lower bound). To conduct the evaluation, we generate 10 samples from each candidate generation method using two different sampling temperatures: 0.5 and 1.8. The results, shown in Table 6, demonstrate that the selection agent significantly outperforms the self-consistency method with a large margin, roughly 6%. As expected, increasing the sampling temperature 9 35 30 33 38 72 23 1045 \",\n",
      "                                \"sub-headings\": []\n",
      "                            }\n",
      "                        },\n",
      "                        {\n",
      "                            \"Query Plan\": {\n",
      "                                \"content\": \" \",\n",
      "                                \"sub-headings\": []\n",
      "                            }\n",
      "                        },\n",
      "                        {\n",
      "                            \"Synthetic Example\": {\n",
      "                                \"content\": \" \",\n",
      "                                \"sub-headings\": []\n",
      "                            }\n",
      "                        },\n",
      "                        {\n",
      "                            \"Divide and Conquer\": {\n",
      "                                \"content\": \" \",\n",
      "                                \"sub-headings\": []\n",
      "                            }\n",
      "                        }\n",
      "                    ]\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"Unsolved Questions: 258\": {\n",
      "                    \"content\": \" (a) Venn diagram illustrating the num- ber of instances for which each method: Query Plan, Synthetic Example, Divide and Conquer, produces at least one cor- rect candidate. The overlap regions repre- sent multiple methods generating correct candidates. (b) Number of correct queries across different complexity levels that were answered by each method. Figure 3: Comparison of SQL generation methods: Venn diagram showing unique and overlapping correct answers (left) and the performance across different complexity levels (right). raises the upper bound but also lowers the lower bound. This effect is more pronounced for the synthetic data generation method compared to the two CoT methods, mainly because LLMs generate reasoning steps before producing the final SQL query, which helps mitigate the randomness introduced by high-temperature sampling. The performance with self-consistency method generally decreases as temperature increases, since the majority cluster becomes smaller with more random queries. However, the proposed trained selection agent is less affected by temperature scaling and, in two cases, even improved its performance with a more diverse pool of samples. Table 6: Performance comparison of different picking methods on the candidates generated by the candidate generators on BIRD development set with two different temperatures. QP refers to query plan COT, DC refers to divide and conquer COT, and OS is the online synthetic example generation method. Picking Method QP (T=0.5) QP (T=1.8) DC (T=0.5) DC (T=1.8) OS (T=0.5) OS (T=1.8) Lower Bound 50.46 48.63 51.37 47.39 60.43 50.98 Upper Bound 78.55 80.44 78.42 79.34 74.77 79.66 Self-consistency 65.78 65.51 66.43 64.41 67.34 66.88 Our Selection Agent 71.7 71.73 71.31 70.53 70.4 71.38 \",\n",
      "                    \"sub-headings\": []\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"4.5 Ablation Studies\": {\n",
      "                    \"content\": \" In the previous sections, we evaluate the importance of the selection agent and each candidate generation method. Next, we focus on the analysis of the remaining components of CHASE-SQL: LSH for value retrieval, the query fixer, and three reasoning strategies (QP, OS, and DC). Table 7 shows the performance of CHASE- SQL without each of these steps, highlighting their significance in achieving higher-quality performance. The results demonstrate the contribution of each component, where removing LSH, the query fixer, or any of the candidate generators leads to a reduction in execution accuracy, further validating the importance of these components of CHASE-SQL. Moreover, the table compares the performance of our binary selection agent with two other selection methods: self-consistency (Wang et al., 2022) and a ranker agent. The ranker agent receives all candidates generated by our three candidate generators in a single prompt, compares them, and produce a ranking for each. For the ranker agent, we select the query with the lowest rank as the best answer. The binary selection agent significantly outperforms both the self-consistency and ranker agents, demonstrating the effectiveness of the proposed method. 10 \",\n",
      "                    \"sub-headings\": []\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"5 Conclusion\": {\n",
      "        \"content\": \" Table 7: Ablation studies on the performance of CHASE- SQL after removing the query fixer, LSH for value re- trieval, and reasoning strategies, i.e., QP, OS, and DC. Method Execution Accuracy (%) (%) CHASE-SQL All 73.01 - CHASE-SQL w self-consistency 68.84 -4.17 CHASE-SQL w ranker agent 65.51 -7.5 CHASE-SQL w/o LSH 70.09 -2.92 CHASE-SQL w/o Query Fixer 69.23 -3.78 CHASE-SQL w/o QP 72.36 -0.65 CHASE-SQL w/o OS 72.16 -0.85 CHASE-SQL w/o DC 71.77 -1.24 We introduce a novel agentic framework, CHASE-SQL, to leverage test-time compute for generating diverse, high-quality SQL queries and accurately selecting the correct one. We propose multiple chain-of-thought prompting methods and an online synthetic example generation technique, along with a query selection mechanism that scores candidates based on pairwise comparisons. Our framework, CHASE-SQL, sets a new state-of-the-art in the notable public Text-to-SQL leaderboard (at the time of the submission), demon- strating the effectiveness of test-time computation for both generating diverse queries and selecting the most accurate response. CHASE-SQL addresses key issues like query diversity and selection optimization, paving the way for further improvements in complex reasoning tasks encountered at real-world Text-to-SQL challenges. \",\n",
      "        \"sub-headings\": []\n",
      "    },\n",
      "    \"Acknowledgments\": {\n",
      "        \"content\": \" We would like to thank Per Jacobsson, Raj Sinha, Zeke Miller, Reza Sherkat, James Su, Zhixian Yan, David Culler, and Xiance Si, for their valuable comments and feedbacks on our paper. We would also like to thank the BIRD team for their invaluable assistance with the evaluation of the BIRD test set. \",\n",
      "        \"sub-headings\": []\n",
      "    },\n",
      "    \"References\": {\n",
      "        \"content\": \" Ion Androutsopoulos, Graeme D Ritchie, and Peter Thanisch. Natural language interfaces to databasesan introduction. Natural language engineering, 1(1):2981, 1995. Ruichu Cai, Jinjie Yuan, Boyan Xu, and Zhifeng Hao. Sadga: Structure-aware dual graph aggregation network for text-to-sql. Advances in Neural Information Processing Systems, 34:76647676, 2021. Ruisheng Cao, Lu Chen, Zhi Chen, Yanbin Zhao, Su Zhu, and Kai Yu. Lgesql: line graph enhanced text-to-sql model with mixed local and non-local relations. arXiv preprint arXiv:2106.01093, 2021. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Xinyun Chen, Maxwell Lin, Nathanael Schrli, and Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023. DongHyun Choi, Myeong Cheol Shin, EungGyun Kim, and Dong Ryeol Shin. Ryansql: Recursively applying sketch-based slot fillings for complex text-to-sql in cross-domain databases. Computational Linguistics, 47 (2):309332, 2021. Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S Mirrokni. Locality-sensitive hashing scheme based on p-stable distributions. In Proceedings of the twentieth annual symposium on Computational geometry, pp. 253262, 2004. Xuemei Dong, Chao Zhang, Yuhang Ge, Yuren Mao, Yunjun Gao, Jinshu Lin, Dongfang Lou, et al. C3: Zero-shot text-to-sql with chatgpt. arXiv preprint arXiv:2307.07306, 2023. Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, and Jingren Zhou. Text-to-sql empowered by large language models: A benchmark evaluation. arXiv preprint arXiv:2308.15363, 2023. Jonathan Herzig, Pawe Krzysztof Nowak, Thomas Mller, Francesco Piccinno, and Julian Martin Eisenschlos. Tapas: Weakly supervised table parsing via pre-training. arXiv preprint arXiv:2004.02349, 2020. 11 Wonseok Hwang, Jinyeong Yim, Seunghyun Park, and Minjoon Seo. A comprehensive exploration on wikisql with table-aware word contextualization. arXiv preprint arXiv:1902.01069, 2019. George Katsogiannis-Meimarakis and Georgia Koutrika. A survey on deep learning approaches for text-to-sql. The VLDB Journal, 32(4):905936, 2023. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. Dongjun Lee, Choongwon Park, Jaehyuk Kim, and Heesoo Park. Mcs-sql: Leveraging multiple prompts and multiple-choice selection for text-to-sql generation. arXiv preprint arXiv:2405.07467, 2024. Boyan Li, Yuyu Luo, Chengliang Chai, Guoliang Li, and Nan Tang. The dawn of natural language to sql: Are we fully ready? arXiv preprint arXiv:2406.01265, 2024a. Fei Li and Hosagrahar V Jagadish. Constructing an interactive natural language interface for relational databases. Proceedings of the VLDB Endowment, 8(1):7384, 2014. Haoyang Li, Jing Zhang, Cuiping Li, and Hong Chen. Resdsql: Decoupling schema linking and skeleton parsing for text-to-sql. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 1306713075, 2023a. Haoyang Li, Jing Zhang, Hanbing Liu, Ju Fan, Xiaokang Zhang, Jun Zhu, Renjie Wei, Hongyan Pan, Cuiping Li, and Hong Chen. Codes: Towards building open-source language models for text-to-sql. Proceedings of the ACM on Management of Data, 2(3):128, 2024b. Jinyang Li, Binyuan Hui, Reynold Cheng, Bowen Qin, Chenhao Ma, Nan Huo, Fei Huang, Wenyu Du, Luo Si, and Yongbin Li. Graphix-t5: Mixing pre-trained transformers with graph-aware layers for text-to-sql parsing. arXiv preprint arXiv:2301.07507, 2023b. Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, et al. Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls. Advances in Neural Information Processing Systems, 36, 2024c. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rmi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):10921097, 2022. Karime Maamari, Fadhil Abubaker, Daniel Jaroslawicz, and Amine Mhedhbi. The death of schema linking? text-to-sql in the age of well-reasoned language models. arXiv preprint arXiv:2408.07702, 2024. Ansong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-tau Yih, Sida Wang, and Xi Victoria Lin. Lever: Learning to verify language-to-code generation with execution. In International Conference on Machine Learning, pp. 2610626128. PMLR, 2023. Rubn Prez-Mercado, Antonio Balderas, Andrs Muoz, Juan Francisco Cabrera, Manuel Palomo-Duarte, and Juan Manuel Dodero. Chatbotsql: Conversational agent to support relational database query language learning. SoftwareX, 22:101346, 2023. Mohammadreza Pourreza and Davood Rafiei. Din-sql: Decomposed in-context learning of text-to-sql with self-correction. Advances in Neural Information Processing Systems, 36, 2024a. Mohammadreza Pourreza and Davood Rafiei. Dts-sql: Decomposed text-to-sql with small large language models. arXiv preprint arXiv:2402.01117, 2024b. Mohammadreza Pourreza, Ruoxi Sun, Hailong Li, Lesly Miculicich, Tomas Pfister, and Sercan O Arik. Sql-gen: Bridging the dialect gap for text-to-sql via synthetic data and model merging. arXiv preprint arXiv:2408.12733, 2024. 12 Abdul Quamar, Vasilis Efthymiou, Chuan Lei, and Fatma zcan. Natural language interfaces to data. Found. Trends Databases, 11(4):319414, 2022. doi: 10.1561/1900000078. URL https://doi.org/10. 1561/1900000078.\",\n",
      "        \"sub-headings\": []\n",
      "    },\n",
      "    \"abstract\": {\n",
      "        \"content\": \"In tackling the challenges of large language model (LLM) performance for Text-to-SQL tasks, we introduce CHASE-SQL, a new framework that employs innovative strategies, using test-time compute in multi-agent modeling to improve candidate generation and selection. CHASE-SQL leverages LLMs intrinsic knowledge to generate diverse and high-quality SQL candidates using different LLM generators with: (1) a divide-and-conquer method that decomposes complex queries into manageable sub-queries in a single LLM call; (2) chain-of-thought reasoning based on query execution plans, reflecting the steps a database engine takes during execution; and (3) a unique instance-aware synthetic example generation technique, which offers specific few-shot demonstrations tailored to test questions. To identify the best candidate, a selection agent is employed to rank the candidates through pairwise comparisons with a fine-tuned binary-candidates selection LLM. This selection approach has been demonstrated to be more robust over alternatives. The proposed generators-selector framework not only enhances the quality and diversity of SQL queries but also outperforms previous methods. Overall, our proposed CHASE-SQL achieves the state-of-the-art execution accuracy of 73.0 % and 73.01% on the test set and development set of the notable BIRD Text-to-SQL dataset benchmark, rendering CHASE-SQL the top submission of the leaderboard (at the time of paper submission).\",\n",
      "        \"sub-headings\": []\n",
      "    },\n",
      "    \"authors\": {\n",
      "        \"content\": \"Mohammadreza Pourreza1, Hailong Li1, Ruoxi Sun1, Yeounoh Chung1, Shayan Talaei2, Gaurav Tarlok Kakkar1, Yu Gan1, Amin Saberi2, Fatma zcan1, Sercan . Ark1\\n1Google Cloud, Sunnyvale, CA, USA\\n2Stanford University, Stanford, CA, USA\\n{pourreza, hailongli, ruoxis, yeounoh}@google.com\\n{gkakkar, gany, fozcan, soarik}@google.com\\n{stalaei, saberi}@stanford.edu\",\n",
      "        \"sub-headings\": []\n",
      "    },\n",
      "    \"date_written\": {\n",
      "        \"content\": \"October 4, 2024\",\n",
      "        \"sub-headings\": []\n",
      "    },\n",
      "    \"title\": {\n",
      "        \"content\": \"CHASE-SQL: Multi-Path Reasoning and Preference Optimized Candidate Selection in Text-to-SQL\",\n",
      "        \"sub-headings\": []\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for d in other_data:\n",
    "    chunks[d['heading']] = {\n",
    "        'content': d['content'] if type(d['content']) == str else \"\\n\".join(d['content']),\n",
    "        \"sub-headings\": []\n",
    "    }\n",
    "\n",
    "print(json.dumps(chunks, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'path': 'title', 'type': 'string', 'description': 'Title of the paper or document', 'node_type': 'trunk'}\n",
      "{'path': 'authors[]', 'type': 'array<string>', 'description': 'List of authors of the paper', 'node_type': 'trunk'}\n",
      "{'path': 'abstract', 'type': 'string', 'description': 'Up to 5 bullet points describing what is new in the approach', 'node_type': 'trunk'}\n",
      "{'path': 'date_written', 'type': 'string', 'description': 'Date when paper was written', 'node_type': 'trunk'}\n",
      "{'path': 'related_work[]', 'type': 'array<string>', 'description': 'List of strings, with related works done; keep the points short and precise', 'node_type': 'trunk'}\n",
      "{'path': 'approach[]', 'type': 'array<object>', 'description': 'List of methods used in the approach', 'node_type': 'trunk'}\n",
      "{'path': 'approach[].approach_name', 'type': 'string', 'description': 'Name of the method or technique', 'node_type': 'branch'}\n",
      "{'path': 'approach[].description', 'type': 'string', 'description': 'Brief summary of what the method is', 'node_type': 'branch'}\n",
      "{'path': 'approach[].steps[]', 'type': 'array<object>', 'description': 'Step-by-step breakdown of the approach', 'node_type': 'branch'}\n",
      "{'path': 'approach[].steps[].step_title', 'type': 'string', 'description': 'title of the step', 'node_type': 'branch'}\n",
      "{'path': 'approach[].steps[].details', 'type': 'string', 'description': 'more details regarding the step', 'node_type': 'branch'}\n",
      "{'path': 'approach[].improvements', 'type': 'object', 'description': 'Impact of the approach', 'node_type': 'branch'}\n",
      "{'path': 'approach[].improvements.metric', 'type': 'string', 'description': '', 'node_type': 'branch'}\n",
      "{'path': 'approach[].improvements.value_added', 'type': 'string', 'description': '', 'node_type': 'branch'}\n",
      "{'path': 'dataset[]', 'type': 'array<object>', 'description': 'List of datasets used for training and experimentation', 'node_type': 'trunk'}\n",
      "{'path': 'dataset[].name', 'type': 'string', 'description': 'Name of the dataset used here', 'node_type': 'branch'}\n",
      "{'path': 'dataset[].source', 'type': 'string', 'description': 'source of the dataset, if known', 'node_type': 'branch'}\n",
      "{'path': 'dataset[].preprocessing', 'type': 'object', 'description': 'preprocessing steps if any done in the paper', 'node_type': 'branch'}\n",
      "{'path': 'dataset[].preprocessing.steps', 'type': 'string', 'description': 'Steps done for preprocessing in numbered format', 'node_type': 'branch'}\n",
      "{'path': 'dataset[].preprocessing.tools_used', 'type': 'string', 'description': 'any specific tools used for preprocessing', 'node_type': 'branch'}\n",
      "{'path': 'experiment_results[]', 'type': 'array<object>', 'description': 'Summarized studies and their results, if available', 'node_type': 'trunk'}\n",
      "{'path': 'experiment_results[].experiment_name', 'type': 'string', 'description': 'name of the experiment done', 'node_type': 'branch'}\n",
      "{'path': 'experiment_results[].metrics[]', 'type': 'array<object>', 'description': 'what all metrics were checked during the process', 'node_type': 'branch'}\n",
      "{'path': 'experiment_results[].metrics[].metric_name', 'type': 'string', 'description': 'name of the metric checked', 'node_type': 'branch'}\n",
      "{'path': 'experiment_results[].metrics[].value', 'type': 'string', 'description': 'value of the results, if known', 'node_type': 'branch'}\n",
      "{'path': 'references[]', 'type': 'array<string>', 'description': 'List of references used in the paper', 'node_type': 'trunk'}\n"
     ]
    }
   ],
   "source": [
    "# step 2\n",
    "with open(SCHEMA_PATH, 'r') as f:\n",
    "    schema_dict = json.load(f)\n",
    "\n",
    "decomposer = SchemaDecomposer(schema_dict, traversal_limit=1)\n",
    "res = decomposer.decompose()\n",
    "\n",
    "for r in res:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " trunk 0  title\n",
      " trunk 0  authors[]\n",
      " trunk 0  abstract\n",
      " trunk 0  date_written\n",
      " trunk 0  related_work[]\n",
      " trunk 0  approach[]\n",
      "branch 1 ──── approach_name\n",
      "branch 1 ──── description\n",
      "branch 1 ──── steps[]\n",
      "branch 2 ──────── step_title\n",
      "branch 2 ──────── details\n",
      "branch 1 ──── improvements\n",
      "branch 2 ──────── metric\n",
      "branch 2 ──────── value_added\n",
      " trunk 0  dataset[]\n",
      "branch 1 ──── name\n",
      "branch 1 ──── source\n",
      "branch 1 ──── preprocessing\n",
      "branch 2 ──────── steps\n",
      "branch 2 ──────── tools_used\n",
      " trunk 0  experiment_results[]\n",
      "branch 1 ──── experiment_name\n",
      "branch 1 ──── metrics[]\n",
      "branch 2 ──────── metric_name\n",
      "branch 2 ──────── value\n",
      " trunk 0  references[]\n"
     ]
    }
   ],
   "source": [
    "# Step 3\n",
    "decomposer.print_schema_tree(res, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Abstract': {'content': ' In tackling the challenges of large language model (LLM) performance for Text-to-SQL tasks, we introduce CHASE-SQL, a new framework that employs innovative strategies, using test-time compute in multi-agent modeling to improve candidate generation and selection. CHASE-SQL leverages LLMs intrinsic knowledge to generate diverse and high-quality SQL candidates using different LLM generators with: (1) a divide-and-conquer method that decomposes complex queries into manageable sub-queries in a single LLM call; (2) chain-of-thought reasoning based on query execution plans, reflecting the steps a database engine takes during execution; and (3) a unique instance-aware synthetic example generation technique, which offers specific few-shot demonstrations tailored to test questions. To identify the best candidate, a selection agent is employed to rank the candidates through pairwise comparisons with a fine-tuned binary-candidates selection LLM. This selection approach has been demonstrated to be more robust over alternatives. The proposed generators-selector framework not only enhances the quality and diversity of SQL queries but also outperforms previous methods. Overall, our proposed CHASE-SQL achieves the state-of-the-art execution accuracy of 73.0 % and 73.01% on the test set and development set of the notable BIRD Text-to-SQL dataset benchmark, rendering CHASE-SQL the top submission of the leaderboard (at the time of paper submission). ',\n",
       "  'sub-headings': []},\n",
       " '1 Introduction': {'content': ' Text-to-SQL, as a bridge between human language and machine-readable structured query languages, is crucial for many use cases, converting natural language questions into executable SQL commands (Androutsopoulos et al., 1995; Li & Jagadish, 2014; Li et al., 2024c; Yu et al., 2018; ?). By enabling users to interact with complex database systems without requiring SQL proficiency, Text-to-SQL empowers users to extract valuable insights, perform streamlined data exploration, make informed decisions, generate data-driven reports and mine better features for machine learning (Chen et al., 2023; Prez-Mercado et al., 2023; Pourreza & Rafiei, 2024a; Pourreza et al., 2024; Sun et al., 2023; Wang et al., 2019; Xie et al., 2023). Furthermore, Text-to-SQL systems play a pivotal role in automating data analytics with complex reasoning and powering conversational agents, expanding their applications beyond traditional data retrieval (Sun et al., 2023; Xie et al., 2023). As data continues to grow exponentially, the ability to query databases efficiently without extensive SQL knowledge becomes increasingly vital for a broad range of applications. Text-to-SQL can be considered a specialized form of code generation, with the contextual information potentially including the database schema, its metadata and along with the values. In the broader code generation domain, utilizing LLMs to generate a wide range of diverse candidates and select the best one has proven to be effective (Chen et al., 2021; Li et al., 2022; Ni et al., 2023). However, it is non-obvious what 1 arXiv:2410.01943v1  [cs.LG]  2 Oct 2024 leads to most effective candidate proposal and winner selector mechanisms. A straightforward yet effective approach involves generating candidates using zero-/few-shot or open-ended prompting, followed by selecting the best options utilizing self-consistency (Wang et al., 2022), which entails clustering candidates based on their execution outputs. This approach has demonstrated promising results in several studies (Lee et al., 2024; Maamari et al., 2024; Talaei et al., 2024; Wang et al., 2023). However, a single prompt design might not fully unleash the extensive Text-to-SQL knowledge of LLMs, and self-consistency methods might not be always effective. In fact, as illustrated in Table 1, the most consistent answers would not always be the correct ones, with an upper-bound performance 14% higher than that achieved through self-consistency. This substantial gap highlights the potential for significant improvement by implementing more effective selection methods to identify the best answer from the pool of candidate queries. Table 1: Evaluating single-query gen- eration vs. ensemble methods of self- consistency and the upper bound that can be achieved for Text-to-SQL with Gemini 1.5 Pro on the BIRD dev set. EX stands for execution accuracy. Method EX (%) Single query 63.01 Self-consistency 68.84 (+ 5.84) Upper-bound 82.79 (+ 19.78) Building on the challenges outlined in the previous section, we pro- pose novel approaches to improve LLM performance for Text-to-SQL by leveraging judiciously-designed test-time computations in an agentic framework. As indicated by the upper bound in Table 1, utilizing LLMs intrinsic knowledge offers significant potential for improvement. We propose methods that generate a diverse set of high-quality can- didate responses and apply a selection mechanism to identify the best answer. Achieving both high-quality and diverse candidate responses is critical for the success of scoring-based selection methods. Low diversity limits improvement potential and reduces the difference be- tween self-consistency and scoring-based approaches. While techniques like increasing temperature or reordering prompt contents can boost diversity, they often compromise the quality of the candidates. To address this, we introduce effective candidate generators designed to enhance diversity while maintaining high-quality outputs. Specifically, we propose three distinct candidate generation approaches, each capable of producing high-quality responses. The first is inspired by the divide-and-conquer algorithm, which breaks down complex problems into smaller, manageable parts to handle difficult queries. The second employs a query execution-plan-based chain-of-thought strategy, where the reasoning process mirrors the steps a database engine takes during query execution. Lastly, we introduce a novel online synthetic example generation method, which helps the model better understand the underlying data schema of the test database. These methods, when used independently, can produce highly-accurate SQL outputs. To effectively select the best answer among candidates, we introduce a selection agent, trained with a classification objective, that assigns scores based on pairwise comparisons between candidate queries. With this agent, we construct a comparison matrix for all candidates and select the final response based on the highest cumulative score. By combining these candidate generation methods with the proposed scoring model, we create an ensemble approach that leverages the strengths of each strategy to significantly improve overall performance. We present comprehensive evaluations on the efficacy of proposed methodologies of CHASE-SQL. Our innovative candidate generation approaches demonstrate superior performance compared to traditional generic CoT prompts, illustrating their capability in guiding LLMs through the decomposition of complex problems into manageable intermediate steps. Furthermore, the proposed selection agent significantly outperforms conventional consistency-based methods, contributing to the state-of-the-art results. Specifically, CHASE- SQL reaches an execution accuracy of 73.01% and 73.0% on the development set and test set of the challenging BIRD Text-to-SQL dataset which outperforms all of the published and undisclosed methods on this benchmark, by a large margin. ',\n",
       "  'sub-headings': []},\n",
       " '2 Related Work': {'content': ' Early Text-to-SQL methods predominantly utilized sequence-to-sequence architectures, encoding user queries and database schemas using models such as Graph Neural Networks (GNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and pre-trained transformer encoders (Cai et al., 2021; Cao et al., 2021; Hwang et al., 2019). On the decoding side, these systems employed either slot-filling or auto-regressive modelling approaches to construct the final SQL queries from the encoded inputs (Choi et al., 2021; Wang et al., 2019). Additionally, tabular language models like TaBERT (Yin et al., 2020), 2 TaPas (Herzig et al., 2020), and Grappa (Yu et al., 2020) have been developed to encode both tables and textual data effectively. However, the landscape has evolved with the widespread use of LLMs, which have largely replaced earlier methods with their superior performance (Katsogiannis-Meimarakis & Koutrika, 2023; Quamar et al., 2022). Initially, efforts concentrated on optimizing prompt designs for these LLMs (Dong et al., 2023; Gao et al., 2023; Pourreza & Rafiei, 2024a). Subsequent advancements have introduced more complex methodologies, including schema linking (Li et al., 2024b; Pourreza & Rafiei, 2024a,b; Talaei et al., 2024), self-correction or self-debugging (Chen et al., 2023; Talaei et al., 2024; Wang et al., 2023), and self-consistency techniques (Lee et al., 2024; Maamari et al., 2024; Sun et al., 2023; Talaei et al., 2024), further enhancing the performance by proposing complex LLM-based pipelines. ',\n",
       "  'sub-headings': []},\n",
       " '3 Methods': {'content': ' ',\n",
       "  'sub-headings': [{'3.1 Overall Framework': {'content': ' This section outlines the proposed CHASE-SQL framework, which consists of four primary components: 1) Value retrieval, 2) Candidate generator, 3) Query fixer, and 4) Selection agent. As illustrated in Fig. 1. The proposed framework begins by retrieving relevant database values. Subsequently, all contextual information, including retrieved values, database metadata, and schema, is provided to an LLM to generate candidate queries. These candidate queries then undergo a fixing loop, and finally, all candidates are compared in a pairwise way using the trained selection agent to pick the correct answer. The following sections delve into the details of each component. Figure 1: Overview of the proposed CHASE-SQL framework for Text-to-SQL, with value retrieval and using a selection agent for improve picking of the answers among the generated candidates along with a fixer to provide feedback for refinement of the outputs. ',\n",
       "     'sub-headings': []}},\n",
       "   {'3.2 Value Retrieval': {'content': ' Databases might contain very high number of rows, with often only a few being relevant to a query. Retrieving relevant values is crucial as they can be used in various SQL clauses like WHERE and HAVING. Similar to the approach in (Talaei et al., 2024), we begin by extracting keywords from the given question using an LLM prompted with few-shot examples. For each keyword, we employ locality-sensitive hashing (LSH) (Datar et al., 2004) to retrieve the most syntactically-similar words, and re-rank them based on embedding-based similarity and edit distance. This approach is robust to typos in the question and considers keyword semantics during retrieval. ',\n",
       "     'sub-headings': []}},\n",
       "   {'3.3 Multi-path Candidate Generation': {'content': ' As shown in Table 1, relying solely on consistency among responses can lead to sub-optimal performance. Therefore, we prioritize diversity in generation of multiple response candidates to increase the likelihood of generating at least one correct answer. Among the diverse responses generated by the candidate generators, we select one as the final response using a selection agent that compares candidates pairwise. To generate 3 diverse responses, we increase the next token sampling temperature, and also shuffle the order of columns and tables in the prompt. Chain-of-Thought (CoT) prompting (Wei et al., 2022) has been proposed to enhance LLMs reasoning abilities by conditioning their final responses on a step-by-step chain of reasoning. Most CoT prompting approaches rely on few-shot examples in the prompt to guide LLMs on thinking step-by-step, following the format M = (qi, ri, si), where qi is the example question, ri is the reasoning path, and si is the ground truth SQL query for qi. We employ two distinct reasoning methods and an online synthetic example generation approach. As shown in Fig. 3a, different generators can yield different outputs, indicating their effectiveness for specific questions and databases. ',\n",
       "     'sub-headings': [{'Divide and Conquer CoT': {'content': ': Divide-and-conquer perspective brings breaking down complex problems into smaller sub-problems, solving each individually, and then combining the solutions to obtain the final answer. Along these lines, we propose a CoT prompting approach that first decomposes the given question into smaller sub-problems using pseudo-SQL queries. In the conquer step, the solutions to these sub-problems are aggregated to construct the final answer. Finally, an optimization step is applied to the constructed query to remove redundant clauses and conditions. This approach is particularly powerful handling complex scenarios that involve nested queries, e.g. intricate WHERE or HAVING conditions, and queries requiring advanced mathematical operations. In Appendix Fig. 17, we exemplify a question and its corresponding SQL query that was successfully solved using this generator, a scenario the other methods considered in this paper could not address due to the querys complex conditions and SQL clauses. For a more detailed view of the divide-and-conquer prompt, please see Appendix Fig. 16. Additionally, Alg. 1 outlines the step-by-step process of this strategy to generate the final SQL output using a single LLM call. Algorithm 1 Divide and Conquer Chain-of-Thought (CoT) Strategy for Text-to-SQL. Input: Set of human-annotated few-shot examples M, user question Qu, target database D associated with the question, and a large language model (LLM) . Divide: 1: Sq (M, D, Qu) // Decompose the original question Qu into a set of sub-questions Sq 2: Ssql // Initialize an empty set Ssql to store partial SQL queries for each sub-question Conquer: 3: for each sub-question qi in Sq do 4: // Generate a partial SQL query for each sub-question qi 5: Ssql Ssql {(M, D, Qu, q1, ..., qi, sql1, ..., sqli1)} 6: end for Assemble: 7: Sf (M, D, Qu, Sq, Ssql) // Assemble the final SQL query Sf from all sub-queries in Ssql 8: return Sf ',\n",
       "        'sub-headings': []}},\n",
       "      {'Query Plan CoT': {'content': ': A query (execution) plan is a sequence of steps that the database engine follows to access or modify the data described by a SQL command. When a SQL query is executed, the database management systems query optimizers translate the SQL text into a query plan that the database engine can execute. This plan outlines how tables are accessed, how they are joined, and the specific operations performed on the data (see Appendix Fig. 19 as an example). Inspired by the step-by-step process that database engines use to execute SQL queries, we propose a reasoning strategy to construct the final SQL output. Query plans for any given SQL query can be obtained using the EXPLAIN\" command, which provides a detailed breakdown of execution steps. However, this output is often presented in a format that is difficult to interpret by LLMs (e.g. in SQLite). To address this, we convert the output of EXPLAIN\" command into a human-readable text format that aligns more closely with the pretraining data of LLMs. The human-readable version of query plans consists of three key steps: (1) identifying and locating the relevant tables for the question, (2) performing operations such as counting, filtering, or matching between tables, and (3) delivering the final result by selecting the appropriate columns to return. This reasoning method complements the divide-and-conquer CoT strategy. While the divide-and-conquer approach is better suited for decomposing complex questions, the query plan approach excels when questions require more reasoning over the relationships between different parts of the question and the database schema. It systematically explains which tables to scan, how to match columns, and how to apply filters. Appendix Fig. 20 shows an example of a question that was answered correctly only by this method. Appendix Fig. 18 provides the prompt used for this reasoning strategy. ',\n",
       "        'sub-headings': []}}]}},\n",
       "   {'3.4 Query Fixer': {'content': ' In some cases, LLMs might generate queries that are syntactically incorrect. These queries are clear candidates for correction, as they fail to provide the correct answers. To address this, we apply an LLM-based query fixer that leverages the self-reflection (Shinn et al., 2024) method. The fixer reflects on the previously generated query, using feedback such as syntax error details or empty result sets to guide the correction process. We continue this iterative fixing approach up to a specified number of attempts,  (set to three in this paper). Appendix Fig. 21 demonstrates the prompt used for this query fixing step. ',\n",
       "     'sub-headings': []}},\n",
       "   {'3.5 Selection Agent': {'content': ' With three different methods for generating SQL queries, we can generate a set of candidate queries for any given question. The key challenge in this step is selecting the correct SQL query from this pool of candidates. A naive approach would be to measure consistency among the candidates by executing them, grouping them based on their execution results, and selecting a query from the largest group as the most likely correct answer. However, this would assume that the most consistent answer is always the best one, which is not always the case. Instead, we propose a more refined picking strategy, Algorithm 3, that relies on a selection agent. Given a set of candidates SQL queries C = {c1, c2, ..., cn}, the final responses are selected by finding 5 the candidate that has the highest score assigned by the selection model. This model p can take k candidates and rank them based on how accurately each of them answers the given question. Concretely, we formulate the selection of the final response as: cf = arg max cC    (n k) X i=1 p(ci1, . . . , cik | Qu, Hu, D)   , (1) where Qu refers to the users question, Hu is the provided hint, and D is the target database from which the question is being asked. In Eq. 1, we pass k candidates to the selection model to be ranked, with k being between 1 and n. In the extreme case of k = 1, the model is unable to make comparisons between candidates, which complicates the evaluation process for the model. As k increases, comparing more candidates makes the process more challenging for the model, as it needs to consider different aspects simultaneously. Consequently, we set k = 2 and train a model with a classification objective to compare only two candidates at a time. Having a set of high-quality and diverse candidates, the most straightforward solution is to employ off-the-shelf LLMs to make pairwise selections. However, experiments with Gemini-1.5-pro showed that using the LLM without fine-tuning resulted in only 58.01% binary classification accuracy. This is primarily due to the candidates being very similar to one another, requiring a fine-tuned model to learn the nuances and make more accurate decisions. To train the selection agent, we first generate candidate SQL queries on the training set (of Text-to-SQL benchmarks), and group them into clusters based on their execution results. For cases where at least one cluster contains correct queries and others contains incorrect ones, we create training examples in the form of tuples (Qu, Ci, Cj, Dij, yij), where Qu is the users question, Ci and Cj are the two candidate queries being compared, Dij is the database schema used by both candidates, and yij 0, 1 is the label indicating whether Ci or Cj is the correct query. To avoid order bias during training, we randomly shuffle the order of correct and incorrect queries in each pair. Since the number of cases with both correct and incorrect candidates is limited, for instances where no correct candidate exists, we include the ground truth SQL query in the prompt as a hint to guide the model in generating correct candidates. Algorithm 3 Picking the final SQL query from a pool of candidates. Input: Set of candidate SQL queries C = {c1, c2, ..., cn}, user question Qu, hint Hu, target database D, and a selection model p, er(ci, D) as the execution result of ci on D 1: ri 0 for all ci C // Initialize the score ri for each candidate query to zero 2: for each distinct pair (ci, cj) where i = j do 3: if er(ci, D) = er(cj, D) then 4: w i // ci is the winner if the execution results match 5: else 6: Si,j schema_union(ci, cj, D) // Construct union of schemas used in ci and cj 7: w p(Si,j, Qu, Hu, ci, cj)w {i, j} // Use binary classifier p to select the winner, w {i, j} 8: end if 9: rw rw + 1 // Increase the score of the winner cw by 1 10: end for 11: cf arg maxciC ri // Select the candidate with the highest score as the final SQL query cf 12: return cf In the pseudo-code for Algorithm 3, we begin by initializing a score of zero for each candidate query. Then, for every distinct pair of queries (ci, cj), we compare both (ci, cj) and (cj, ci) to mitigate any order bias, ensuring that both candidates in a pair are fairly evaluated. If both queries produce the same execution result on the database, we mark one as the winner and increment its score, as these results suggest consistency. If the execution results differ, we generate a union of the schema used by both queries and use the binary classifier to determine which query is more likely to be correct. The classifier takes into account the question, the two candidate queries, and the combined schema to make its decision. The winners score is then updated accordingly. After all comparisons, the candidate with the highest score is selected as the final query. In the rare case of a tie in the final scores, we break the tie by selecting one of the candidates arbitrarily. 6 ',\n",
       "     'sub-headings': []}}]},\n",
       " '4 Online Synthetic Example Generation': {'content': ': Using M demonstrations for few-shot in-context learning has shown promising results on various related tasks (Pourreza & Rafiei, 2024a). Besides helping with specifying the task and illustrate the step-by-step process deriving the output, demonstrations constructed using relevant tables and columns can also help the model understand the underlying data schema. Based on this insight, we propose a synthetic demonstration generation strategy for Text-to-SQL  given the user question Qu, the target database D, and the selected columns ti (using a column selection approach similar to (Talaei et al., 2024)). Algorithm 2 Online Synthetic example generation strategy for Text-to-SQL. Input: User question Qu, additional user hint Hu, target database D and filtered relevant table columns t associated with the question, LLM , guidelines Rf for generating examples by SQL features, guidelines Rt for generating examples with filtered schema, and the numbers of examples to generate nf, nt respectively 1: P // {(qi, si) | qi, si }, where qi is input question , si is output SQL for the i-th example 2: P P {(D, Rf, nf)} // Generate n examples with entire database by common SQL features 3: P P {(t, Rt, nt)} // Generate examples with filtered columns to highlight correct schema usage 4: return P Algorithm 2 outlines the online synthetic example generation approach with two LLM generation steps. The first step focuses on generating illustrative examples with common SQL features described in the guideline Rf. The SQL features include equality and non-equality predicates, single table and multi-table JOIN, nested JOIN, ORDER BY and LIMIT, GROUP BY and HAVING, various aggregation functions. These are widely applicable SQL clauses and functions  the generated example SQL queries, incorporating these features, follow the BIRD SQL feature distribution (Appendix Fig 23a). The second step focuses on generating examples highlighting correct interpretation of the underlying data schema  the model  is asked to generate examples using ti and that are similar to the examples outlined in Rt. Appendix A.10 provides the prompts used for the example generation). While a relevant example (e.g. showing a nested JOIN query with multiple tables) can be helpful for questions that require complex JOIN queries, it might also mislead the LLM for overuse (e.g. when a simple single table query is sufficient). This and the inherent ambiguity of natural language query qi, for which we draw the examples by relevance, make the example selection challenging. Thus, we generate and inject the examples to the prompt online per qi. We ask the LLM to generate many input-output pairs for in-context learning. The final set of synthetic examples for qi contains examples generated with both Rf and Rt. This ensures that the example set is diverse both in SQL features/clauses and the choice of relevant tables/columns used. The diversity of the example set is desirable to avoid over-fitting the output to certain patterns (e.g., the model always writes a SQL with JOIN if shown mostly JOIN examples). Mixing various examples for various SQL features and database tables with and without column filtering is observed to result in better generation quality overall (please see Appendix Table 8). ',\n",
       "  'sub-headings': []},\n",
       " '4 Experiments': {'content': ' ',\n",
       "  'sub-headings': [{'4.1 Datasets and Models': {'content': ' We evaluate the performance of the proposed CHASE-SQL framework on two widely-recognized cross-domain datasets: BIRD (Li et al., 2024c) and Spider (Yu et al., 2018). BIRD contains over 12,751 unique question-SQL pairs from 95 large databases, spanning more than 37 professional domains, with databases designed to resemble real-world scenarios, featuring messy data rows and complex schemas. Spider consists of 10,181 questions and 5,693 unique complex SQL queries across 200 databases, covering 138 domains. The Spider dataset is divided into non-overlapping training, development, and test sets similar to BIRD. For both, we use execution accuracy (EX), the official metric for their respective leaderboard, as the primary evaluation metric to compare methods. Details of the models and their hyperparameters are provided in Appendix section A.2. ',\n",
       "     'sub-headings': []}},\n",
       "   {'4.2 BIRD results': {'content': ' We present the end-to-end Text-to-SQL performance of the proposed CHASE-SQL framework using Claude- 3.5-sonnet and Gemini 1.5 pro on the BIRD development set, and Gemini 1.5 pro on the BIRD test set. We compare with both published methods (either with an available codebase and/or paper) and undisclosed methods. For a fair comparison with Gemini 1.5 pro, all LLM calls in the Claude-3.5-sonnet setting, except for the selection model, are made using Claude-3.5-sonnet (previously-trained selection model is reused). As shown in Table 2, CHASE-SQL with Gemini 1.5 pro achieves 73.01% accuracy on the BIRD development set and 73.0% on the BIRD holdout test set, outperforming all previous works and setting a new state-of-the-art performance. Table 2: Performance Comparison of different Text-to-SQL methods on BIRD benchmark. Method EX (Dev) EX (Test) Published CHASE-SQL + Gemini 1.5 (Ours) 73.01 73.0 CHASE-SQL + Claude 3.5 Sonnet (Ours) 69.53  Distillery + GPT-4o (Maamari et al., 2024) 67.21 71.83 CHESS (Talaei et al., 2024) 65.00 66.69 MCS-SQL + GPT-4 (Lee et al., 2024) 63.36 65.45 SuperSQL (Li et al., 2024a) 58.5 62.66 Undisclosed Insights AI 72.16 70.26 AskData + GPT-4o 72.03 72.39 OpenSearch-v2 + GPT-4o 69.3 72.28 PURPLE-RED + GPT-4o 68.12 70.21 Arcwise + GPT-4o 67.99 66.21 ExSL + granite-34b-code 67.47 67.76 Table 3: Performance Comparison of different Text-to-SQL methods on Spider test set. Method EX Training with Spider MCS-SQL + GPT-4 (Lee et al., 2024) 89.6  CHASE-SQL + Gemini 1.5 (Ours) 87.6  CHESS (Talaei et al., 2024) 87.2  DAIL-SQL + GPT-4 (Gao et al., 2023) 86.6  DIN-SQL + GPT-4 (Pourreza & Rafiei, 2024a) 85.3  C3 + ChatGPT (Dong et al., 2023) 82.3  RESDSQL 3B (Li et al., 2023a) 79.9  DIN-SQL + CodeX (Pourreza & Rafiei, 2024a) 78.2  T5-3B+NatSQL (Rai et al., 2023) 78.0  Graphix-3B+PICARD (Li et al., 2023b) 77.6  ',\n",
       "     'sub-headings': []}},\n",
       "   {'4.3 Spider results': {'content': ' We assess the generalizability of the proposed CHASE-SQL by evaluating it in an end-to-end way on the Spider test set without modifying the few-shot samples in the prompts or training a new selection model, i.e. without using and data from the target distribution. This approach allows us to test the performance of CHASE-SQL on different unseen query and database distributions compared to the data from training distributions. Table 3 demonstrates that CHASE-SQL achieves an execution accuracy of 87.6% on the Spider test set, placing it second among methods that have undergone specific training or prompt optimization for the Spider dataset. This highlights the strong generalizability of CHASE-SQL and its potential for generating high quality Text-to-SQL for unseen samples coming from very different distributions and unique challenges. 7 ',\n",
       "     'sub-headings': []}},\n",
       "   {'4.4 Generator and selection performance': {'content': ' Table 4: Ablation studies on single candidate gen- eration performance of the candidate generators compared with original BIRD prompt + zero-shot CoT with Gemini 1.5 pro on the BIRD dev set. Selector is our final performance applying the se- lection agent to 21 candidates generated from all generators. Method Execution Accuracy (%) (%) Baseline 57.75 - QP CoT 63.62 +5.87 DC CoT 63.92 +6.17 OS CoT 67.09 +9.34 Baseline w Query Fixer 61.58 +3.83 QP CoT w Query Fixer 65.51 +7.76 DC CoT w Query Fixer 65.77 +8.02 OS CoT w Query Fixer 68.02 +10.27 Generator + Fixer: To reveal performance of genera- tors, we conducted an ablation study to evaluate the per- formance of each candidate generation method before and after applying the query fixer. We compare the performance of the proposed generators in producing a single candidate query against the original BIRD prompt (Li et al., 2024c), augmented with zero-shot CoT reasoning (Kojima et al., 2022), which serves as the baseline for assessing the quality of prompts. The results, shown in Table 4, indicate that the proposed methods significantly improve SQL generation performance, compared to the naive baseline, towards the goal of producing high-quality candidates while maintain- ing diversity. Among the candidate generators, the online synthetic data generation approach produced an impressive performance of 68.02%, demonstrating its effectiveness in leveraging test-time compute to improve LLM performance by generating high-quality synthetic examples. Furthermore, the query fixer proved crucial, enhancing the quality of the candidate pool and increasing performance by nearly 2% across all candidate generators. Table 5: Evaluating the binary selection accuracy of the different selection models. ',\n",
       "     'sub-headings': [{'Selection': {'content': ' Model Binary Acc. (%) Claude-3.5-sonnet 60.21 Gemini-1.5-pro 63.98 Tuned Gemma 2 9B 64.28 Tuned Gemini-1.5-flash 71.01 Selection: We conducted an analysis on the binary selection accu- racy of the selection agent for cases where, in a pairwise comparison, one candidate is correct and the other is incorrect. We exclude cases where both candidates are either correct or incorrect, as the selection would not affect the outcome since both candidates have the same label. We compare the performance of Claude-3.5-sonnet and Gemini-1.5-pro (both out-of-the-box without fine-tuning) with two fine-tuned models: 1) Gemma 2 9B and 2) Gemini-1.5-flash. As shown in Table 5, both fine-tuned models achieve higher accuracy than the untuned counterparts, demonstrating the importance of fine-tuning to teach the model about the specific preferences. ',\n",
       "        'sub-headings': []}},\n",
       "      {'Candidate Generation Analysis': {'content': ': We analyze the performance of each candidate generator method individually. To better understand the performance potential when effectively selecting the correct SQL query from the candidate pool, we generate seven candidate SQL queries from each generator method (21 candidates in total) for all samples in the BIRD development set. We determine this number of candidates based on the observation that increasing the candidate pool beyond 20 did not yield significant improvements, as illustrated in Fig. 2d. By assuming access to an oracle selection model that always selects the correct SQL query from the seven candidates, we calculate the upper-bound performance achievable for each generator. Conversely, by assuming an adversarial selection model that always selects the wrong SQL query, we determine the lower-bound performance. Fig. 2 illustrates the upper-bound and lower-bound performance for all three methods together with the performance of our selection agent. As shown, the upper-bound performance of the two different CoT methods is generally higher than that of the synthetic example generation method for different number of candidates. However, their lower-bound performance is also lower than the synthetic method. Lower-bound accuracy reflects cases where all candidates are correct, reducing the noise in the selection process since it doesnt matter which candidate is chosen, so a higher lower-bound is preferred. This is evident in the selection agents performance, where a drop in the lower bound leads to diminishing returns from increasing the upper bound, causing the selection agents performance to plateau. Additionally, the upper-bound performance of combining all three methods reaches 82.79%, highlighting the significant room for improvement through better candidate picking methods. This demonstrates that the LLMs parametric knowledge already contains the information needed to solve most questions, highlighting the need for ensemble approaches to effectively extract and utilize this knowledge. 8 (a) Upper-bound and lower-bound Accuracy for Divide and Conquer CoT (b) Upper-bound and lower-bound Accuracy for Online Synthetic Example (c) Upper-bound and lower-bound performance for Query Plan CoT. (d) Upper-bound performance of all three can- didate generators across different difficulty cat- egories. Figure 2: Comparison of the upper- and lower-bound performance of different candidate generators. Additionally, we evaluate the upper-bound performance by combining all candidates from three candidate generation methods across the simple, moderate, and challenging difficulty levels for the BIRD development set. These difficulty categories are assigned by human experts during the creation of the BIRD development set. Fig. 2d shows that, as expected, the upper-bound performance increases with the number of candidates across all difficulty levels. However, for the challenging and moderate classes, the improvement plateaus earlier than in the simple class, suggesting that generating more samples does not further improve the upper-bound performance for these two difficulty levels. Fig. 2 presents a Venn diagram showcasing the performance of three generation methods: Query Plan, Divide and Conquer, and with Synthetic Examples. The numbers within the intersecting regions represent the instances where multiple methods generated at least one correct candidate. This diagram visually highlights the unique contributions of each method, which indicates the necessity of using all three generators. Additionally, in Fig. 3b, we compare the number of correct queries generated by each SQL generation method that are not correct by the other generators. The divide-and-conquer approach outperforms the others on challenging questions, while the query plan method excels on moderately difficult queries. To further analyze the performance of the generators across different domains and varying numbers of columns and tables, we compare the number of correct queries generated for each database, as shown in Appendix Fig. 4. As illustrated, both CoT methods generally perform similarly across databases, while the online synthetic example generation method significantly increases diversity, resulting in more correct answers overall across different databases. ',\n",
       "        'sub-headings': []}},\n",
       "      {'Selection Agent Analysis': {'content': ': We evaluate the query-picking performance by comparing the Text-to-SQL execution accuracy of the selection agent with the self-consistency method (using majority voting) Wang et al. (2022), an oracle model (upper bound), and an adversarial model (lower bound). To conduct the evaluation, we generate 10 samples from each candidate generation method using two different sampling temperatures: 0.5 and 1.8. The results, shown in Table 6, demonstrate that the selection agent significantly outperforms the self-consistency method with a large margin, roughly 6%. As expected, increasing the sampling temperature 9 35 30 33 38 72 23 1045 ',\n",
       "        'sub-headings': []}},\n",
       "      {'Query Plan': {'content': ' ', 'sub-headings': []}},\n",
       "      {'Synthetic Example': {'content': ' ', 'sub-headings': []}},\n",
       "      {'Divide and Conquer': {'content': ' ', 'sub-headings': []}}]}},\n",
       "   {'Unsolved Questions: 258': {'content': ' (a) Venn diagram illustrating the num- ber of instances for which each method: Query Plan, Synthetic Example, Divide and Conquer, produces at least one cor- rect candidate. The overlap regions repre- sent multiple methods generating correct candidates. (b) Number of correct queries across different complexity levels that were answered by each method. Figure 3: Comparison of SQL generation methods: Venn diagram showing unique and overlapping correct answers (left) and the performance across different complexity levels (right). raises the upper bound but also lowers the lower bound. This effect is more pronounced for the synthetic data generation method compared to the two CoT methods, mainly because LLMs generate reasoning steps before producing the final SQL query, which helps mitigate the randomness introduced by high-temperature sampling. The performance with self-consistency method generally decreases as temperature increases, since the majority cluster becomes smaller with more random queries. However, the proposed trained selection agent is less affected by temperature scaling and, in two cases, even improved its performance with a more diverse pool of samples. Table 6: Performance comparison of different picking methods on the candidates generated by the candidate generators on BIRD development set with two different temperatures. QP refers to query plan COT, DC refers to divide and conquer COT, and OS is the online synthetic example generation method. Picking Method QP (T=0.5) QP (T=1.8) DC (T=0.5) DC (T=1.8) OS (T=0.5) OS (T=1.8) Lower Bound 50.46 48.63 51.37 47.39 60.43 50.98 Upper Bound 78.55 80.44 78.42 79.34 74.77 79.66 Self-consistency 65.78 65.51 66.43 64.41 67.34 66.88 Our Selection Agent 71.7 71.73 71.31 70.53 70.4 71.38 ',\n",
       "     'sub-headings': []}},\n",
       "   {'4.5 Ablation Studies': {'content': ' In the previous sections, we evaluate the importance of the selection agent and each candidate generation method. Next, we focus on the analysis of the remaining components of CHASE-SQL: LSH for value retrieval, the query fixer, and three reasoning strategies (QP, OS, and DC). Table 7 shows the performance of CHASE- SQL without each of these steps, highlighting their significance in achieving higher-quality performance. The results demonstrate the contribution of each component, where removing LSH, the query fixer, or any of the candidate generators leads to a reduction in execution accuracy, further validating the importance of these components of CHASE-SQL. Moreover, the table compares the performance of our binary selection agent with two other selection methods: self-consistency (Wang et al., 2022) and a ranker agent. The ranker agent receives all candidates generated by our three candidate generators in a single prompt, compares them, and produce a ranking for each. For the ranker agent, we select the query with the lowest rank as the best answer. The binary selection agent significantly outperforms both the self-consistency and ranker agents, demonstrating the effectiveness of the proposed method. 10 ',\n",
       "     'sub-headings': []}}]},\n",
       " '5 Conclusion': {'content': ' Table 7: Ablation studies on the performance of CHASE- SQL after removing the query fixer, LSH for value re- trieval, and reasoning strategies, i.e., QP, OS, and DC. Method Execution Accuracy (%) (%) CHASE-SQL All 73.01 - CHASE-SQL w self-consistency 68.84 -4.17 CHASE-SQL w ranker agent 65.51 -7.5 CHASE-SQL w/o LSH 70.09 -2.92 CHASE-SQL w/o Query Fixer 69.23 -3.78 CHASE-SQL w/o QP 72.36 -0.65 CHASE-SQL w/o OS 72.16 -0.85 CHASE-SQL w/o DC 71.77 -1.24 We introduce a novel agentic framework, CHASE-SQL, to leverage test-time compute for generating diverse, high-quality SQL queries and accurately selecting the correct one. We propose multiple chain-of-thought prompting methods and an online synthetic example generation technique, along with a query selection mechanism that scores candidates based on pairwise comparisons. Our framework, CHASE-SQL, sets a new state-of-the-art in the notable public Text-to-SQL leaderboard (at the time of the submission), demon- strating the effectiveness of test-time computation for both generating diverse queries and selecting the most accurate response. CHASE-SQL addresses key issues like query diversity and selection optimization, paving the way for further improvements in complex reasoning tasks encountered at real-world Text-to-SQL challenges. ',\n",
       "  'sub-headings': []},\n",
       " 'Acknowledgments': {'content': ' We would like to thank Per Jacobsson, Raj Sinha, Zeke Miller, Reza Sherkat, James Su, Zhixian Yan, David Culler, and Xiance Si, for their valuable comments and feedbacks on our paper. We would also like to thank the BIRD team for their invaluable assistance with the evaluation of the BIRD test set. ',\n",
       "  'sub-headings': []},\n",
       " 'References': {'content': ' Ion Androutsopoulos, Graeme D Ritchie, and Peter Thanisch. Natural language interfaces to databasesan introduction. Natural language engineering, 1(1):2981, 1995. Ruichu Cai, Jinjie Yuan, Boyan Xu, and Zhifeng Hao. Sadga: Structure-aware dual graph aggregation network for text-to-sql. Advances in Neural Information Processing Systems, 34:76647676, 2021. Ruisheng Cao, Lu Chen, Zhi Chen, Yanbin Zhao, Su Zhu, and Kai Yu. Lgesql: line graph enhanced text-to-sql model with mixed local and non-local relations. arXiv preprint arXiv:2106.01093, 2021. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Xinyun Chen, Maxwell Lin, Nathanael Schrli, and Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023. DongHyun Choi, Myeong Cheol Shin, EungGyun Kim, and Dong Ryeol Shin. Ryansql: Recursively applying sketch-based slot fillings for complex text-to-sql in cross-domain databases. Computational Linguistics, 47 (2):309332, 2021. Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S Mirrokni. Locality-sensitive hashing scheme based on p-stable distributions. In Proceedings of the twentieth annual symposium on Computational geometry, pp. 253262, 2004. Xuemei Dong, Chao Zhang, Yuhang Ge, Yuren Mao, Yunjun Gao, Jinshu Lin, Dongfang Lou, et al. C3: Zero-shot text-to-sql with chatgpt. arXiv preprint arXiv:2307.07306, 2023. Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, and Jingren Zhou. Text-to-sql empowered by large language models: A benchmark evaluation. arXiv preprint arXiv:2308.15363, 2023. Jonathan Herzig, Pawe Krzysztof Nowak, Thomas Mller, Francesco Piccinno, and Julian Martin Eisenschlos. Tapas: Weakly supervised table parsing via pre-training. arXiv preprint arXiv:2004.02349, 2020. 11 Wonseok Hwang, Jinyeong Yim, Seunghyun Park, and Minjoon Seo. A comprehensive exploration on wikisql with table-aware word contextualization. arXiv preprint arXiv:1902.01069, 2019. George Katsogiannis-Meimarakis and Georgia Koutrika. A survey on deep learning approaches for text-to-sql. The VLDB Journal, 32(4):905936, 2023. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. Dongjun Lee, Choongwon Park, Jaehyuk Kim, and Heesoo Park. Mcs-sql: Leveraging multiple prompts and multiple-choice selection for text-to-sql generation. arXiv preprint arXiv:2405.07467, 2024. Boyan Li, Yuyu Luo, Chengliang Chai, Guoliang Li, and Nan Tang. The dawn of natural language to sql: Are we fully ready? arXiv preprint arXiv:2406.01265, 2024a. Fei Li and Hosagrahar V Jagadish. Constructing an interactive natural language interface for relational databases. Proceedings of the VLDB Endowment, 8(1):7384, 2014. Haoyang Li, Jing Zhang, Cuiping Li, and Hong Chen. Resdsql: Decoupling schema linking and skeleton parsing for text-to-sql. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 1306713075, 2023a. Haoyang Li, Jing Zhang, Hanbing Liu, Ju Fan, Xiaokang Zhang, Jun Zhu, Renjie Wei, Hongyan Pan, Cuiping Li, and Hong Chen. Codes: Towards building open-source language models for text-to-sql. Proceedings of the ACM on Management of Data, 2(3):128, 2024b. Jinyang Li, Binyuan Hui, Reynold Cheng, Bowen Qin, Chenhao Ma, Nan Huo, Fei Huang, Wenyu Du, Luo Si, and Yongbin Li. Graphix-t5: Mixing pre-trained transformers with graph-aware layers for text-to-sql parsing. arXiv preprint arXiv:2301.07507, 2023b. Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, et al. Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls. Advances in Neural Information Processing Systems, 36, 2024c. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rmi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):10921097, 2022. Karime Maamari, Fadhil Abubaker, Daniel Jaroslawicz, and Amine Mhedhbi. The death of schema linking? text-to-sql in the age of well-reasoned language models. arXiv preprint arXiv:2408.07702, 2024. Ansong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-tau Yih, Sida Wang, and Xi Victoria Lin. Lever: Learning to verify language-to-code generation with execution. In International Conference on Machine Learning, pp. 2610626128. PMLR, 2023. Rubn Prez-Mercado, Antonio Balderas, Andrs Muoz, Juan Francisco Cabrera, Manuel Palomo-Duarte, and Juan Manuel Dodero. Chatbotsql: Conversational agent to support relational database query language learning. SoftwareX, 22:101346, 2023. Mohammadreza Pourreza and Davood Rafiei. Din-sql: Decomposed in-context learning of text-to-sql with self-correction. Advances in Neural Information Processing Systems, 36, 2024a. Mohammadreza Pourreza and Davood Rafiei. Dts-sql: Decomposed text-to-sql with small large language models. arXiv preprint arXiv:2402.01117, 2024b. Mohammadreza Pourreza, Ruoxi Sun, Hailong Li, Lesly Miculicich, Tomas Pfister, and Sercan O Arik. Sql-gen: Bridging the dialect gap for text-to-sql via synthetic data and model merging. arXiv preprint arXiv:2408.12733, 2024. 12 Abdul Quamar, Vasilis Efthymiou, Chuan Lei, and Fatma zcan. Natural language interfaces to data. Found. Trends Databases, 11(4):319414, 2022. doi: 10.1561/1900000078. URL https://doi.org/10. 1561/1900000078.',\n",
       "  'sub-headings': []},\n",
       " 'abstract': {'content': 'In tackling the challenges of large language model (LLM) performance for Text-to-SQL tasks, we introduce CHASE-SQL, a new framework that employs innovative strategies, using test-time compute in multi-agent modeling to improve candidate generation and selection. CHASE-SQL leverages LLMs intrinsic knowledge to generate diverse and high-quality SQL candidates using different LLM generators with: (1) a divide-and-conquer method that decomposes complex queries into manageable sub-queries in a single LLM call; (2) chain-of-thought reasoning based on query execution plans, reflecting the steps a database engine takes during execution; and (3) a unique instance-aware synthetic example generation technique, which offers specific few-shot demonstrations tailored to test questions. To identify the best candidate, a selection agent is employed to rank the candidates through pairwise comparisons with a fine-tuned binary-candidates selection LLM. This selection approach has been demonstrated to be more robust over alternatives. The proposed generators-selector framework not only enhances the quality and diversity of SQL queries but also outperforms previous methods. Overall, our proposed CHASE-SQL achieves the state-of-the-art execution accuracy of 73.0 % and 73.01% on the test set and development set of the notable BIRD Text-to-SQL dataset benchmark, rendering CHASE-SQL the top submission of the leaderboard (at the time of paper submission).',\n",
       "  'sub-headings': []},\n",
       " 'authors': {'content': 'Mohammadreza Pourreza1, Hailong Li1, Ruoxi Sun1, Yeounoh Chung1, Shayan Talaei2, Gaurav Tarlok Kakkar1, Yu Gan1, Amin Saberi2, Fatma zcan1, Sercan . Ark1\\n1Google Cloud, Sunnyvale, CA, USA\\n2Stanford University, Stanford, CA, USA\\n{pourreza, hailongli, ruoxis, yeounoh}@google.com\\n{gkakkar, gany, fozcan, soarik}@google.com\\n{stalaei, saberi}@stanford.edu',\n",
       "  'sub-headings': []},\n",
       " 'date_written': {'content': 'October 4, 2024', 'sub-headings': []},\n",
       " 'title': {'content': 'CHASE-SQL: Multi-Path Reasoning and Preference Optimized Candidate Selection in Text-to-SQL',\n",
       "  'sub-headings': []}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4\n",
    "from t2j.extractor import FieldExtractor\n",
    "\n",
    "e = FieldExtractor(model, promptsClass)\n",
    "extracted_data = e.extract(chunks, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'schema_field': {'path': 'title',\n",
       "   'type': 'string',\n",
       "   'description': 'Title of the paper or document',\n",
       "   'node_type': 'trunk'},\n",
       "  'data': 'CHASE-SQL: Multi-Path Reasoning and Preference Optimized Candidate Selection in Text-to-SQL'},\n",
       " {'schema_field': {'path': 'authors[]',\n",
       "   'type': 'array<string>',\n",
       "   'description': 'List of authors of the paper',\n",
       "   'node_type': 'trunk'},\n",
       "  'data': ['Mohammadreza Pourreza',\n",
       "   'Hailong Li',\n",
       "   'Ruoxi Sun',\n",
       "   'Yeounoh Chung',\n",
       "   'Shayan Talaei',\n",
       "   'Gaurav Tarlok Kakkar',\n",
       "   'Yu Gan',\n",
       "   'Amin Saberi',\n",
       "   'Fatma zcan',\n",
       "   'Sercan . Ark']},\n",
       " {'schema_field': {'path': 'abstract',\n",
       "   'type': 'string',\n",
       "   'description': 'Up to 5 bullet points describing what is new in the approach',\n",
       "   'node_type': 'trunk'},\n",
       "  'data': '- Introduction of CHASE-SQL, a new framework for Text-to-SQL tasks.\\n- Utilizes test-time compute in multi-agent modeling for improved candidate generation and selection.\\n- Employs a divide-and-conquer method and chain-of-thought reasoning for SQL generation.\\n- Features a unique instance-aware synthetic example generation technique for few-shot demonstrations.\\n- Achieves state-of-the-art execution accuracy on the BIRD Text-to-SQL dataset benchmark.'},\n",
       " {'schema_field': {'path': 'date_written',\n",
       "   'type': 'string',\n",
       "   'description': 'Date when paper was written',\n",
       "   'node_type': 'trunk'},\n",
       "  'data': 'October 4, 2024'},\n",
       " {'schema_field': {'path': 'related_work[]',\n",
       "   'type': 'array<string>',\n",
       "   'description': 'List of strings, with related works done; keep the points short and precise',\n",
       "   'node_type': 'trunk'},\n",
       "  'data': []},\n",
       " {'schema_field': {'path': 'approach[]',\n",
       "   'type': 'array<object>',\n",
       "   'description': 'List of methods used in the approach',\n",
       "   'node_type': 'trunk'},\n",
       "  'data': [{'approach': [{'approach_name': 'Divide-and-Conquer Chain-of-Thought (CoT) Strategy',\n",
       "      'description': 'A CoT prompting approach that decomposes the given question into smaller sub-problems using pseudo-SQL queries and aggregates solutions to construct the final answer.',\n",
       "      'steps': [{'step_title': 'Divide',\n",
       "        'details': 'Decompose the original question into a set of sub-questions.'},\n",
       "       {'step_title': 'Conquer',\n",
       "        'details': 'Generate a partial SQL query for each sub-question.'},\n",
       "       {'step_title': 'Assemble',\n",
       "        'details': 'Assemble the final SQL query from all sub-queries.'}],\n",
       "      'improvements': {'metric': '',\n",
       "       'value_added': 'Particularly powerful in handling complex scenarios involving nested queries and advanced mathematical operations.'}},\n",
       "     {'approach_name': 'Query Plan Strategy',\n",
       "      'description': \"A reasoning strategy inspired by database engines' step-by-step process to execute SQL queries, converting EXPLAIN command output into a human-readable format.\",\n",
       "      'steps': [{'step_title': 'Identify Tables',\n",
       "        'details': 'Identify and locate the relevant tables for the question.'},\n",
       "       {'step_title': 'Perform Operations',\n",
       "        'details': 'Perform operations such as counting, filtering, or matching between tables.'},\n",
       "       {'step_title': 'Deliver Result',\n",
       "        'details': 'Deliver the final result by selecting the appropriate columns to return.'}],\n",
       "      'improvements': {'metric': '',\n",
       "       'value_added': 'Excels when questions require more reasoning over the relationships between different parts of the question and the database schema.'}},\n",
       "     {'approach_name': 'LLM-based Query Fixer',\n",
       "      'description': 'An LLM-based query fixer that leverages the self-reflection method to correct syntactically incorrect queries.',\n",
       "      'steps': [{'step_title': 'Reflect on Query',\n",
       "        'details': 'Reflect on the previously generated query using feedback such as syntax error details or empty result sets.'},\n",
       "       {'step_title': 'Iterative Fixing',\n",
       "        'details': 'Continue the iterative fixing approach up to a specified number of attempts.'}],\n",
       "      'improvements': {'metric': '',\n",
       "       'value_added': 'Addresses syntactically incorrect queries to provide correct answers.'}},\n",
       "     {'approach_name': 'Selection Agent',\n",
       "      'description': 'A selection agent that compares candidate queries pairwise to select the correct SQL query from a pool of candidates.',\n",
       "      'steps': [{'step_title': 'Initialize Scores',\n",
       "        'details': 'Initialize the score for each candidate query to zero.'},\n",
       "       {'step_title': 'Compare Candidates',\n",
       "        'details': 'Use a binary classifier to determine which query is more likely to be correct.'},\n",
       "       {'step_title': 'Select Final Query',\n",
       "        'details': 'Select the candidate with the highest score as the final SQL query.'}],\n",
       "      'improvements': {'metric': '',\n",
       "       'value_added': 'Provides a refined picking strategy to select the correct SQL query from a pool of candidates.'}}]}]},\n",
       " {'schema_field': {'path': 'dataset[]',\n",
       "   'type': 'array<object>',\n",
       "   'description': 'List of datasets used for training and experimentation',\n",
       "   'node_type': 'trunk'},\n",
       "  'data': [{'dataset': [{'name': 'BIRD',\n",
       "      'source': 'Li et al., 2024c',\n",
       "      'preprocessing': {'steps': '', 'tools_used': ''}},\n",
       "     {'name': 'Spider',\n",
       "      'source': 'Yu et al., 2018',\n",
       "      'preprocessing': {'steps': '', 'tools_used': ''}}]}]},\n",
       " {'schema_field': {'path': 'experiment_results[]',\n",
       "   'type': 'array<object>',\n",
       "   'description': 'Summarized studies and their results, if available',\n",
       "   'node_type': 'trunk'},\n",
       "  'data': [{'experiment_results': [{'experiment_name': 'CHASE-SQL + Gemini 1.5 (Ours) on BIRD',\n",
       "      'metrics': [{'metric_name': 'EX (Dev)', 'value': '73.01'},\n",
       "       {'metric_name': 'EX (Test)', 'value': '73.0'}]},\n",
       "     {'experiment_name': 'CHASE-SQL + Claude 3.5 Sonnet (Ours) on BIRD',\n",
       "      'metrics': [{'metric_name': 'EX (Dev)', 'value': '69.53'}]},\n",
       "     {'experiment_name': 'CHASE-SQL + Gemini 1.5 (Ours) on Spider',\n",
       "      'metrics': [{'metric_name': 'EX', 'value': '87.6'}]}]}]},\n",
       " {'schema_field': {'path': 'references[]',\n",
       "   'type': 'array<string>',\n",
       "   'description': 'List of references used in the paper',\n",
       "   'node_type': 'trunk'},\n",
       "  'data': ['Ion Androutsopoulos, Graeme D Ritchie, and Peter Thanisch. Natural language interfaces to databasesan introduction. Natural language engineering, 1(1):2981, 1995.',\n",
       "   'Ruichu Cai, Jinjie Yuan, Boyan Xu, and Zhifeng Hao. Sadga: Structure-aware dual graph aggregation network for text-to-sql. Advances in Neural Information Processing Systems, 34:76647676, 2021.',\n",
       "   'Ruisheng Cao, Lu Chen, Zhi Chen, Yanbin Zhao, Su Zhu, and Kai Yu. Lgesql: line graph enhanced text-to-sql model with mixed local and non-local relations. arXiv preprint arXiv:2106.01093, 2021.',\n",
       "   'Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.',\n",
       "   'Xinyun Chen, Maxwell Lin, Nathanael Schrli, and Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023.',\n",
       "   'DongHyun Choi, Myeong Cheol Shin, EungGyun Kim, and Dong Ryeol Shin. Ryansql: Recursively applying sketch-based slot fillings for complex text-to-sql in cross-domain databases. Computational Linguistics, 47 (2):309332, 2021.',\n",
       "   'Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S Mirrokni. Locality-sensitive hashing scheme based on p-stable distributions. In Proceedings of the twentieth annual symposium on Computational geometry, pp. 253262, 2004.',\n",
       "   'Xuemei Dong, Chao Zhang, Yuhang Ge, Yuren Mao, Yunjun Gao, Jinshu Lin, Dongfang Lou, et al. C3: Zero-shot text-to-sql with chatgpt. arXiv preprint arXiv:2307.07306, 2023.',\n",
       "   'Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, and Jingren Zhou. Text-to-sql empowered by large language models: A benchmark evaluation. arXiv preprint arXiv:2308.15363, 2023.',\n",
       "   'Jonathan Herzig, Pawe Krzysztof Nowak, Thomas Mller, Francesco Piccinno, and Julian Martin Eisenschlos. Tapas: Weakly supervised table parsing via pre-training. arXiv preprint arXiv:2004.02349, 2020.',\n",
       "   'Wonseok Hwang, Jinyeong Yim, Seunghyun Park, and Minjoon Seo. A comprehensive exploration on wikisql with table-aware word contextualization. arXiv preprint arXiv:1902.01069, 2019.',\n",
       "   'George Katsogiannis-Meimarakis and Georgia Koutrika. A survey on deep learning approaches for text-to-sql. The VLDB Journal, 32(4):905936, 2023.',\n",
       "   'Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022.',\n",
       "   'Dongjun Lee, Choongwon Park, Jaehyuk Kim, and Heesoo Park. Mcs-sql: Leveraging multiple prompts and multiple-choice selection for text-to-sql generation. arXiv preprint arXiv:2405.07467, 2024.',\n",
       "   'Boyan Li, Yuyu Luo, Chengliang Chai, Guoliang Li, and Nan Tang. The dawn of natural language to sql: Are we fully ready? arXiv preprint arXiv:2406.01265, 2024a.',\n",
       "   'Fei Li and Hosagrahar V Jagadish. Constructing an interactive natural language interface for relational databases. Proceedings of the VLDB Endowment, 8(1):7384, 2014.',\n",
       "   'Haoyang Li, Jing Zhang, Cuiping Li, and Hong Chen. Resdsql: Decoupling schema linking and skeleton parsing for text-to-sql. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 1306713075, 2023a.',\n",
       "   'Haoyang Li, Jing Zhang, Hanbing Liu, Ju Fan, Xiaokang Zhang, Jun Zhu, Renjie Wei, Hongyan Pan, Cuiping Li, and Hong Chen. Codes: Towards building open-source language models for text-to-sql. Proceedings of the ACM on Management of Data, 2(3):128, 2024b.',\n",
       "   'Jinyang Li, Binyuan Hui, Reynold Cheng, Bowen Qin, Chenhao Ma, Nan Huo, Fei Huang, Wenyu Du, Luo Si, and Yongbin Li. Graphix-t5: Mixing pre-trained transformers with graph-aware layers for text-to-sql parsing. arXiv preprint arXiv:2301.07507, 2023b.',\n",
       "   'Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, et al. Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls. Advances in Neural Information Processing Systems, 36, 2024c.',\n",
       "   'Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rmi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):10921097, 2022.',\n",
       "   'Karime Maamari, Fadhil Abubaker, Daniel Jaroslawicz, and Amine Mhedhbi. The death of schema linking? text-to-sql in the age of well-reasoned language models. arXiv preprint arXiv:2408.07702, 2024.',\n",
       "   'Ansong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-tau Yih, Sida Wang, and Xi Victoria Lin. Lever: Learning to verify language-to-code generation with execution. In International Conference on Machine Learning, pp. 2610626128. PMLR, 2023.',\n",
       "   'Rubn Prez-Mercado, Antonio Balderas, Andrs Muoz, Juan Francisco Cabrera, Manuel Palomo-Duarte, and Juan Manuel Dodero. Chatbotsql: Conversational agent to support relational database query language learning. SoftwareX, 22:101346, 2023.',\n",
       "   'Mohammadreza Pourreza and Davood Rafiei. Din-sql: Decomposed in-context learning of text-to-sql with self-correction. Advances in Neural Information Processing Systems, 36, 2024a.',\n",
       "   'Mohammadreza Pourreza and Davood Rafiei. Dts-sql: Decomposed text-to-sql with small large language models. arXiv preprint arXiv:2402.01117, 2024b.',\n",
       "   'Mohammadreza Pourreza, Ruoxi Sun, Hailong Li, Lesly Miculicich, Tomas Pfister, and Sercan O Arik. Sql-gen: Bridging the dialect gap for text-to-sql via synthetic data and model merging. arXiv preprint arXiv:2408.12733, 2024.',\n",
       "   'Abdul Quamar, Vasilis Efthymiou, Chuan Lei, and Fatma zcan. Natural language interfaces to data. Found. Trends Databases, 11(4):319414, 2022. doi: 10.1561/1900000078. URL https://doi.org/10.1561/1900000078.']}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 5: aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = merge(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"title\": \"CHASE-SQL: Multi-Path Reasoning and Preference Optimized Candidate Selection in Text-to-SQL\",\n",
      "    \"authors\": [\n",
      "        \"Mohammadreza Pourreza\",\n",
      "        \"Hailong Li\",\n",
      "        \"Ruoxi Sun\",\n",
      "        \"Yeounoh Chung\",\n",
      "        \"Shayan Talaei\",\n",
      "        \"Gaurav Tarlok Kakkar\",\n",
      "        \"Yu Gan\",\n",
      "        \"Amin Saberi\",\n",
      "        \"Fatma zcan\",\n",
      "        \"Sercan . Ark\"\n",
      "    ],\n",
      "    \"abstract\": \"- Introduction of CHASE-SQL, a new framework for Text-to-SQL tasks.\\n- Utilizes test-time compute in multi-agent modeling for improved candidate generation and selection.\\n- Employs a divide-and-conquer method and chain-of-thought reasoning for SQL generation.\\n- Features a unique instance-aware synthetic example generation technique for few-shot demonstrations.\\n- Achieves state-of-the-art execution accuracy on the BIRD Text-to-SQL dataset benchmark.\",\n",
      "    \"date_written\": \"October 4, 2024\",\n",
      "    \"related_work\": [],\n",
      "    \"approach\": [\n",
      "        {\n",
      "            \"approach\": [\n",
      "                {\n",
      "                    \"approach_name\": \"Divide-and-Conquer Chain-of-Thought (CoT) Strategy\",\n",
      "                    \"description\": \"A CoT prompting approach that decomposes the given question into smaller sub-problems using pseudo-SQL queries and aggregates solutions to construct the final answer.\",\n",
      "                    \"steps\": [\n",
      "                        {\n",
      "                            \"step_title\": \"Divide\",\n",
      "                            \"details\": \"Decompose the original question into a set of sub-questions.\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"step_title\": \"Conquer\",\n",
      "                            \"details\": \"Generate a partial SQL query for each sub-question.\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"step_title\": \"Assemble\",\n",
      "                            \"details\": \"Assemble the final SQL query from all sub-queries.\"\n",
      "                        }\n",
      "                    ],\n",
      "                    \"improvements\": {\n",
      "                        \"metric\": \"\",\n",
      "                        \"value_added\": \"Particularly powerful in handling complex scenarios involving nested queries and advanced mathematical operations.\"\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"approach_name\": \"Query Plan Strategy\",\n",
      "                    \"description\": \"A reasoning strategy inspired by database engines' step-by-step process to execute SQL queries, converting EXPLAIN command output into a human-readable format.\",\n",
      "                    \"steps\": [\n",
      "                        {\n",
      "                            \"step_title\": \"Identify Tables\",\n",
      "                            \"details\": \"Identify and locate the relevant tables for the question.\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"step_title\": \"Perform Operations\",\n",
      "                            \"details\": \"Perform operations such as counting, filtering, or matching between tables.\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"step_title\": \"Deliver Result\",\n",
      "                            \"details\": \"Deliver the final result by selecting the appropriate columns to return.\"\n",
      "                        }\n",
      "                    ],\n",
      "                    \"improvements\": {\n",
      "                        \"metric\": \"\",\n",
      "                        \"value_added\": \"Excels when questions require more reasoning over the relationships between different parts of the question and the database schema.\"\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"approach_name\": \"LLM-based Query Fixer\",\n",
      "                    \"description\": \"An LLM-based query fixer that leverages the self-reflection method to correct syntactically incorrect queries.\",\n",
      "                    \"steps\": [\n",
      "                        {\n",
      "                            \"step_title\": \"Reflect on Query\",\n",
      "                            \"details\": \"Reflect on the previously generated query using feedback such as syntax error details or empty result sets.\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"step_title\": \"Iterative Fixing\",\n",
      "                            \"details\": \"Continue the iterative fixing approach up to a specified number of attempts.\"\n",
      "                        }\n",
      "                    ],\n",
      "                    \"improvements\": {\n",
      "                        \"metric\": \"\",\n",
      "                        \"value_added\": \"Addresses syntactically incorrect queries to provide correct answers.\"\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"approach_name\": \"Selection Agent\",\n",
      "                    \"description\": \"A selection agent that compares candidate queries pairwise to select the correct SQL query from a pool of candidates.\",\n",
      "                    \"steps\": [\n",
      "                        {\n",
      "                            \"step_title\": \"Initialize Scores\",\n",
      "                            \"details\": \"Initialize the score for each candidate query to zero.\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"step_title\": \"Compare Candidates\",\n",
      "                            \"details\": \"Use a binary classifier to determine which query is more likely to be correct.\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"step_title\": \"Select Final Query\",\n",
      "                            \"details\": \"Select the candidate with the highest score as the final SQL query.\"\n",
      "                        }\n",
      "                    ],\n",
      "                    \"improvements\": {\n",
      "                        \"metric\": \"\",\n",
      "                        \"value_added\": \"Provides a refined picking strategy to select the correct SQL query from a pool of candidates.\"\n",
      "                    }\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ],\n",
      "    \"dataset\": [\n",
      "        {\n",
      "            \"dataset\": [\n",
      "                {\n",
      "                    \"name\": \"BIRD\",\n",
      "                    \"source\": \"Li et al., 2024c\",\n",
      "                    \"preprocessing\": {\n",
      "                        \"steps\": \"\",\n",
      "                        \"tools_used\": \"\"\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"name\": \"Spider\",\n",
      "                    \"source\": \"Yu et al., 2018\",\n",
      "                    \"preprocessing\": {\n",
      "                        \"steps\": \"\",\n",
      "                        \"tools_used\": \"\"\n",
      "                    }\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ],\n",
      "    \"experiment_results\": [\n",
      "        {\n",
      "            \"experiment_results\": [\n",
      "                {\n",
      "                    \"experiment_name\": \"CHASE-SQL + Gemini 1.5 (Ours) on BIRD\",\n",
      "                    \"metrics\": [\n",
      "                        {\n",
      "                            \"metric_name\": \"EX (Dev)\",\n",
      "                            \"value\": \"73.01\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"metric_name\": \"EX (Test)\",\n",
      "                            \"value\": \"73.0\"\n",
      "                        }\n",
      "                    ]\n",
      "                },\n",
      "                {\n",
      "                    \"experiment_name\": \"CHASE-SQL + Claude 3.5 Sonnet (Ours) on BIRD\",\n",
      "                    \"metrics\": [\n",
      "                        {\n",
      "                            \"metric_name\": \"EX (Dev)\",\n",
      "                            \"value\": \"69.53\"\n",
      "                        }\n",
      "                    ]\n",
      "                },\n",
      "                {\n",
      "                    \"experiment_name\": \"CHASE-SQL + Gemini 1.5 (Ours) on Spider\",\n",
      "                    \"metrics\": [\n",
      "                        {\n",
      "                            \"metric_name\": \"EX\",\n",
      "                            \"value\": \"87.6\"\n",
      "                        }\n",
      "                    ]\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ],\n",
      "    \"references\": [\n",
      "        \"Ion Androutsopoulos, Graeme D Ritchie, and Peter Thanisch. Natural language interfaces to databasesan introduction. Natural language engineering, 1(1):2981, 1995.\",\n",
      "        \"Ruichu Cai, Jinjie Yuan, Boyan Xu, and Zhifeng Hao. Sadga: Structure-aware dual graph aggregation network for text-to-sql. Advances in Neural Information Processing Systems, 34:76647676, 2021.\",\n",
      "        \"Ruisheng Cao, Lu Chen, Zhi Chen, Yanbin Zhao, Su Zhu, and Kai Yu. Lgesql: line graph enhanced text-to-sql model with mixed local and non-local relations. arXiv preprint arXiv:2106.01093, 2021.\",\n",
      "        \"Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.\",\n",
      "        \"Xinyun Chen, Maxwell Lin, Nathanael Schrli, and Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023.\",\n",
      "        \"DongHyun Choi, Myeong Cheol Shin, EungGyun Kim, and Dong Ryeol Shin. Ryansql: Recursively applying sketch-based slot fillings for complex text-to-sql in cross-domain databases. Computational Linguistics, 47 (2):309332, 2021.\",\n",
      "        \"Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S Mirrokni. Locality-sensitive hashing scheme based on p-stable distributions. In Proceedings of the twentieth annual symposium on Computational geometry, pp. 253262, 2004.\",\n",
      "        \"Xuemei Dong, Chao Zhang, Yuhang Ge, Yuren Mao, Yunjun Gao, Jinshu Lin, Dongfang Lou, et al. C3: Zero-shot text-to-sql with chatgpt. arXiv preprint arXiv:2307.07306, 2023.\",\n",
      "        \"Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, and Jingren Zhou. Text-to-sql empowered by large language models: A benchmark evaluation. arXiv preprint arXiv:2308.15363, 2023.\",\n",
      "        \"Jonathan Herzig, Pawe Krzysztof Nowak, Thomas Mller, Francesco Piccinno, and Julian Martin Eisenschlos. Tapas: Weakly supervised table parsing via pre-training. arXiv preprint arXiv:2004.02349, 2020.\",\n",
      "        \"Wonseok Hwang, Jinyeong Yim, Seunghyun Park, and Minjoon Seo. A comprehensive exploration on wikisql with table-aware word contextualization. arXiv preprint arXiv:1902.01069, 2019.\",\n",
      "        \"George Katsogiannis-Meimarakis and Georgia Koutrika. A survey on deep learning approaches for text-to-sql. The VLDB Journal, 32(4):905936, 2023.\",\n",
      "        \"Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022.\",\n",
      "        \"Dongjun Lee, Choongwon Park, Jaehyuk Kim, and Heesoo Park. Mcs-sql: Leveraging multiple prompts and multiple-choice selection for text-to-sql generation. arXiv preprint arXiv:2405.07467, 2024.\",\n",
      "        \"Boyan Li, Yuyu Luo, Chengliang Chai, Guoliang Li, and Nan Tang. The dawn of natural language to sql: Are we fully ready? arXiv preprint arXiv:2406.01265, 2024a.\",\n",
      "        \"Fei Li and Hosagrahar V Jagadish. Constructing an interactive natural language interface for relational databases. Proceedings of the VLDB Endowment, 8(1):7384, 2014.\",\n",
      "        \"Haoyang Li, Jing Zhang, Cuiping Li, and Hong Chen. Resdsql: Decoupling schema linking and skeleton parsing for text-to-sql. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 1306713075, 2023a.\",\n",
      "        \"Haoyang Li, Jing Zhang, Hanbing Liu, Ju Fan, Xiaokang Zhang, Jun Zhu, Renjie Wei, Hongyan Pan, Cuiping Li, and Hong Chen. Codes: Towards building open-source language models for text-to-sql. Proceedings of the ACM on Management of Data, 2(3):128, 2024b.\",\n",
      "        \"Jinyang Li, Binyuan Hui, Reynold Cheng, Bowen Qin, Chenhao Ma, Nan Huo, Fei Huang, Wenyu Du, Luo Si, and Yongbin Li. Graphix-t5: Mixing pre-trained transformers with graph-aware layers for text-to-sql parsing. arXiv preprint arXiv:2301.07507, 2023b.\",\n",
      "        \"Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, et al. Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls. Advances in Neural Information Processing Systems, 36, 2024c.\",\n",
      "        \"Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rmi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):10921097, 2022.\",\n",
      "        \"Karime Maamari, Fadhil Abubaker, Daniel Jaroslawicz, and Amine Mhedhbi. The death of schema linking? text-to-sql in the age of well-reasoned language models. arXiv preprint arXiv:2408.07702, 2024.\",\n",
      "        \"Ansong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-tau Yih, Sida Wang, and Xi Victoria Lin. Lever: Learning to verify language-to-code generation with execution. In International Conference on Machine Learning, pp. 2610626128. PMLR, 2023.\",\n",
      "        \"Rubn Prez-Mercado, Antonio Balderas, Andrs Muoz, Juan Francisco Cabrera, Manuel Palomo-Duarte, and Juan Manuel Dodero. Chatbotsql: Conversational agent to support relational database query language learning. SoftwareX, 22:101346, 2023.\",\n",
      "        \"Mohammadreza Pourreza and Davood Rafiei. Din-sql: Decomposed in-context learning of text-to-sql with self-correction. Advances in Neural Information Processing Systems, 36, 2024a.\",\n",
      "        \"Mohammadreza Pourreza and Davood Rafiei. Dts-sql: Decomposed text-to-sql with small large language models. arXiv preprint arXiv:2402.01117, 2024b.\",\n",
      "        \"Mohammadreza Pourreza, Ruoxi Sun, Hailong Li, Lesly Miculicich, Tomas Pfister, and Sercan O Arik. Sql-gen: Bridging the dialect gap for text-to-sql via synthetic data and model merging. arXiv preprint arXiv:2408.12733, 2024.\",\n",
      "        \"Abdul Quamar, Vasilis Efthymiou, Chuan Lei, and Fatma zcan. Natural language interfaces to data. Found. Trends Databases, 11(4):319414, 2022. doi: 10.1561/1900000078. URL https://doi.org/10.1561/1900000078.\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps(final_output, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from t2j.workflow import Workflow\n",
    "\n",
    "w = Workflow(\"trace-id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = r\"C:\\Users\\Pratyush\\Desktop\\text-to-json\\examples\\text2SQL-solutions\\chase-sql.pdf\"\n",
    "SCHEMA_PATH = r\"C:\\Users\\Pratyush\\Desktop\\text-to-json\\examples\\text2SQL-solutions\\schema.json\"\n",
    "\n",
    "w.run(FILE_PATH, SCHEMA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOTAL_TIME = 1.04 min\n",
    "# TOTAL PRICE = approx 15rs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
